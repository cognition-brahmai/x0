[
    {
        "role": "user",
        "content": "Implement a C++ program that reads a text file containing a list of URLs, downloads the content of each URL using multithreading, and saves the downloaded content to separate files.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Implementing a C++ program that reads URLs from a text file, downloads their content using multithreading, and saves each content to separate files involves several steps. Below is a step-by-step guide and a sample implementation to achieve this.\n\n### Approach\n\n1. **Reading URLs from a File**: Read the list of URLs from a specified text file.\n2. **Multithreading for Downloads**: Use multiple threads to download the content of each URL concurrently.\n3. **Saving Downloaded Content**: Save the content of each URL to a separate file.\n\n### Solution Code\n\n```cpp\n#include <iostream>\n#include <fstream>\n#include <vector>\n#include <thread>\n#include <mutex>\n#include <curl/curl.h>\n\nstd::mutex cout_mutex;\n\nsize_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) {\n    ((std::string*)userp)->append((char*)contents, size * nmemb);\n    return size * nmemb;\n}\n\nvoid downloadURL(const std::string& url, const std::string& filename) {\n    CURL* curl;\n    CURLcode res;\n    std::string readBuffer;\n\n    curl = curl_easy_init();\n    if(curl) {\n        curl_easy_setopt(curl, CURLOPT_URL, url.c_str());\n        curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback);\n        curl_easy_setopt(curl, CURLOPT_WRITEDATA, &readBuffer);\n        res = curl_easy_perform(curl);\n        curl_easy_cleanup(curl);\n\n        if(res != CURLE_OK) {\n            std::lock_guard<std::mutex> lock(cout_mutex);\n            std::cerr << \"Failed to download \" << url << \": \" << curl_easy_strerror(res) << std::endl;\n            return;\n        }\n\n        std::ofstream outfile(filename);\n        if(outfile.is_open()) {\n            outfile << readBuffer;\n            outfile.close();\n            std::lock_guard<std::mutex> lock(cout_mutex);\n            std::cout << \"Downloaded \" << url << \" to \" << filename << std::endl;\n        } else {\n            std::lock_guard<std::mutex> lock(cout_mutex);\n            std::cerr << \"Unable to open file: \" << filename << std::endl;\n        }\n    }\n}\n\nint main(int argc, char* argv[]) {\n    if(argc != 2) {\n        std::cerr << \"Usage: \" << argv[0] << \" <file containing URLs>\" << std::endl;\n        return 1;\n    }\n\n    std::ifstream infile(argv[1]);\n    if(!infile.is_open()) {\n        std::cerr << \"Unable to open file: \" << argv[1] << std::endl;\n        return 1;\n    }\n\n    std::vector<std::string> urls;\n    std::string url;\n    while(std::getline(infile, url)) {\n        if(!url.empty()) {\n            urls.push_back(url);\n        }\n    }\n    infile.close();\n\n    std::vector<std::thread> threads;\n    for(size_t i = 0; i < urls.size(); ++i) {\n        std::string filename = \"download_\" + std::to_string(i) + \".html\";\n        threads.emplace_back(downloadURL, urls[i], filename);\n    }\n\n    for(auto& thread : threads) {\n        thread.join();\n    }\n\n    return 0;\n}\n```\n\n### Explanation\n\n1. **Reading URLs from a File**:\n   - The program reads URLs from a specified text file, one URL per line.\n   \n2. **Multithreading for Downloads**:\n   - Each URL is processed in a separate thread to download its content concurrently.\n   - The `std::thread` library is used to manage threads.\n   \n3. **Downloading Content**:\n   - The `libcurl` library is used to handle HTTP requests and download the content of each URL.\n   - A callback function `WriteCallback` is used to store the downloaded content in a string.\n   \n4. **Saving Downloaded Content**:\n   - The downloaded content is saved to a file named `download_<index>.html`, where `<index>` is the position of the URL in the list.\n   - Error handling is implemented to manage failures in downloading or file operations.\n\nThis approach ensures efficient utilization of resources by downloading multiple URLs simultaneously, thereby reducing the total download time."
    }
]