[
    {
        "role": "user",
        "content": "Implement a Python script that uses the Boto3 library to upload files to an AWS S3 bucket. The script should:\n   a. Accept command line arguments for the local file path, target S3 bucket name, and object key.\n   b. Calculate the MD5 hash of the file content before uploading.\n   c. Set the Content-MD5 header with the calculated hash when uploading the file to S3.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that uploads files to an AWS S3 bucket using the Boto3 library. Let's break it down step by step.\n\nFirst, I need to make sure I have Boto3 installed. I can install it using pip if I don't have it already. So, in the script, I should include a check for Boto3 and maybe instruct the user to install it if it's not present.\n\nNext, the script needs to accept command line arguments for the local file path, the target S3 bucket name, and the object key. Python's argparse module should be perfect for handling command line arguments. I'll need to define these arguments and make sure they are required so that the user has to provide them.\n\nNow, calculating the MD5 hash of the file content before uploading. I need to read the file, compute its MD5 hash, and then set this hash in the Content-MD5 header when uploading to S3. I remember that S3 expects the Content-MD5 header to be the Base64 encoded 128-bit MD5 digest of the object.\n\nSo, steps for calculating the MD5:\n\n1. Open the file in binary mode.\n\n2. Read the file content.\n\n3. Compute the MD5 hash of the content.\n\n4. Encode the hash in Base64.\n\nFor uploading the file to S3 with the Content-MD5 header, I'll need to use the Boto3 S3 client's upload_file method or put_object method. I think put_object might give me more control over headers.\n\nI should also handle any potential errors, like if the file doesn't exist, if there are permissions issues with S3, or if there are problems with calculating the MD5 hash.\n\nLet me think about the structure of the script:\n\n- Import necessary modules: argparse for command line arguments, boto3 for S3 operations, os for file path checks, and hashlib for MD5 calculation.\n\n- Set up argparse to require three arguments: local file path, bucket name, and object key.\n\n- Check if the local file exists.\n\n- Calculate the MD5 hash of the file.\n\n- Use Boto3 to upload the file to S3, setting the Content-MD5 header.\n\n- Print a success message or an error message depending on the outcome.\n\nI should also consider whether to enable logging or provide more verbose output, but for now, keeping it simple might be best.\n\nAlright, let's start writing some pseudocode to outline the steps.\n\nPseudocode:\n\nimport argparse\n\nimport boto3\n\nimport os\n\nimport hashlib\n\nimport base64\n\n# Set up argument parser\n\nparser = argparse.ArgumentParser(description='Upload a file to an S3 bucket with Content-MD5 header.')\n\nparser.add_argument('file_path', help='Local path to the file to upload')\n\nparser.add_argument('bucket_name', help='Name of the S3 bucket')\n\nparser.add_argument('object_key', help='Object key in the S3 bucket')\n\nargs = parser.parse_args()\n\n# Check if file exists\n\nif not os.path.isfile(args.file_path):\n\n    print(f\"Error: The file {args.file_path} does not exist.\")\n\n    exit(1)\n\n# Calculate MD5 hash\n\nwith open(args.file_path, 'rb') as f:\n\n    file_data = f.read()\n\n    md5_hash = hashlib.md5(file_data).digest()\n\n    md5_base64 = base64.b64encode(md5_hash).decode('utf-8')\n\n# Upload to S3 with Content-MD5 header\n\ns3_client = boto3.client('s3')\n\ntry:\n\n    s3_client.put_object(\n\n        Bucket=args.bucket_name,\n\n        Key=args.object_key,\n\n        Body=file_data,\n\n        ContentMD5=md5_base64\n\n    )\n\n    print(f\"Successfully uploaded {args.file_path} to {args.bucket_name}/{args.object_key}\")\n\nexcept Exception as e:\n\n    print(f\"Error uploading file: {e}\")\n\n    exit(1)\n\nThat seems straightforward. But I should consider if the file is large, reading the entire file into memory might not be efficient. Maybe for large files, I should read the file in chunks to calculate the MD5 hash without loading the entire file into memory.\n\nLet me look into that.\n\nYes, hashlib supports updating the hash incrementally, so I can read the file in chunks, update the hash, and then compute the final digest.\n\nHere's how I can do that:\n\nchunk_size = 8192  # 8KB chunks\n\nmd5 = hashlib.md5()\n\nwith open(args.file_path, 'rb') as f:\n\n    while True:\n\n        data = f.read(chunk_size)\n\n        if not data:\n\n            break\n\n        md5.update(data)\n\nmd5_base64 = base64.b64encode(md5.digest()).decode('utf-8')\n\nThis way, even for large files, I'm only reading 8KB at a time into memory.\n\nNow, for the upload, I can use the upload_file method, which is optimized for large files, but I need to set the Content-MD5 header. However, upload_file doesn't directly support custom headers.\n\nAlternatively, I can use the upload_fileobj method with a file object and specify extra arguments including ContentMD5.\n\nLet me check the Boto3 documentation for that.\n\nOkay, I can use upload_file with ExtraArgs parameter to pass additional parameters, including ContentMD5.\n\nSo, the upload step would look like this:\n\nwith open(args.file_path, 'rb') as f:\n\n    s3_client.upload_fileobj(\n\n        f,\n\n        args.bucket_name,\n\n        args.object_key,\n\n        ExtraArgs={'ContentMD5': md5_base64}\n\n    )\n\nThis should work efficiently for large files.\n\nI should also consider adding error handling for specific cases, like invalid bucket names, permission issues, or network problems.\n\nAdditionally, I might want to check if the bucket exists, but that could be an extra step.\n\nFor now, I'll focus on the main functionality.\n\nLet me also think about whether I need to configure AWS credentials. Boto3 usually looks for credentials in environment variables or in the AWS configuration files, so as long as the user has their AWS credentials set up properly, it should work.\n\nI should include a note in the script's docstring or comments about requiring proper AWS credentials configuration.\n\nAlright, I think I have a plan. Let's proceed to write the actual script.\n\n## Final Solution\nTo create a Python script that uploads files to an AWS S3 bucket using the Boto3 library, follow the steps below. This script accepts command line arguments for the local file path, target S3 bucket name, and object key. It calculates the MD5 hash of the file content before uploading and sets the `Content-MD5` header with the calculated hash when uploading the file to S3.\n\n### Solution Code\n\n```python\nimport argparse\nimport boto3\nimport os\nimport hashlib\nimport base64\n\ndef main():\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description='Upload a file to an S3 bucket with Content-MD5 header.')\n    parser.add_argument('file_path', help='Local path to the file to upload')\n    parser.add_argument('bucket_name', help='Name of the S3 bucket')\n    parser.add_argument('object_key', help='Object key in the S3 bucket')\n    args = parser.parse_args()\n\n    # Check if file exists\n    if not os.path.isfile(args.file_path):\n        print(f\"Error: The file {args.file_path} does not exist.\")\n        exit(1)\n\n    # Calculate MD5 hash\n    md5 = calculate_md5(args.file_path)\n    md5_base64 = base64.b64encode(md5.digest()).decode('utf-8')\n\n    # Upload to S3 with Content-MD5 header\n    s3_client = boto3.client('s3')\n    try:\n        with open(args.file_path, 'rb') as f:\n            s3_client.upload_fileobj(\n                f,\n                args.bucket_name,\n                args.object_key,\n                ExtraArgs={'ContentMD5': md5_base64}\n            )\n        print(f\"Successfully uploaded {args.file_path} to {args.bucket_name}/{args.object_key}\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n        exit(1)\n\ndef calculate_md5(file_path):\n    md5 = hashlib.md5()\n    chunk_size = 8192  # 8KB chunks\n    with open(file_path, 'rb') as f:\n        while True:\n            data = f.read(chunk_size)\n            if not data:\n                break\n            md5.update(data)\n    return md5\n\nif __name__ == '__main__':\n    main()\n```\n\n### Explanation\n\n1. **Argument Parsing**: Use `argparse` to require three command-line arguments: the local file path, the target S3 bucket name, and the object key.\n2. **File Existence Check**: Verify if the specified local file exists.\n3. **MD5 Hash Calculation**: Read the file in chunks to compute its MD5 hash efficiently, especially for large files.\n4. **Upload to S3**: Use Boto3's `upload_fileobj` method to upload the file to S3, setting the `ContentMD5` header with the calculated hash.\n5. **Error Handling**: Handle exceptions that may occur during file operations or S3 uploads, providing meaningful error messages.\n\nEnsure you have Boto3 installed (`pip install boto3`) and that your AWS credentials are configured properly for the script to authenticate with S3."
    }
]