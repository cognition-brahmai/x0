[
    {
        "role": "user",
        "content": "Determine the eigenvalues and eigenvectors of the matrix A = [[2, 1], [1, 2]], showing all calculations."
    },
    {
        "role": "assistant",
        "content": "To determine the eigenvalues and eigenvectors of the matrix \\( A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\), we need to follow a systematic approach. Let's go through this step by step.\n\n### Step 1: Find the Eigenvalues\n\nEigenvalues (\\(\\lambda\\)) are scalars that satisfy the characteristic equation of the matrix \\( A \\), which is given by:\n\n\\[\n\\det(A - \\lambda I) = 0\n\\]\n\nwhere \\( I \\) is the identity matrix of the same size as \\( A \\).\n\nFirst, compute \\( A - \\lambda I \\):\n\n\\[\nA - \\lambda I = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 - \\lambda & 1 \\\\ 1 & 2 - \\lambda \\end{bmatrix}\n\\]\n\nNext, find the determinant of this matrix:\n\n\\[\n\\det(A - \\lambda I) = \\det\\begin{bmatrix} 2 - \\lambda & 1 \\\\ 1 & 2 - \\lambda \\end{bmatrix} = (2 - \\lambda)(2 - \\lambda) - (1)(1) = (2 - \\lambda)^2 - 1\n\\]\n\nSimplify the expression:\n\n\\[\n(2 - \\lambda)^2 - 1 = 4 - 4\\lambda + \\lambda^2 - 1 = \\lambda^2 - 4\\lambda + 3\n\\]\n\nSet the characteristic equation to zero:\n\n\\[\n\\lambda^2 - 4\\lambda + 3 = 0\n\\]\n\nSolve for \\(\\lambda\\):\n\n\\[\n\\lambda = \\frac{4 \\pm \\sqrt{16 - 12}}{2} = \\frac{4 \\pm \\sqrt{4}}{2} = \\frac{4 \\pm 2}{2}\n\\]\n\nThus, the eigenvalues are:\n\n\\[\n\\lambda_1 = \\frac{4 + 2}{2} = 3, \\quad \\lambda_2 = \\frac{4 - 2}{2} = 1\n\\]\n\n### Step 2: Find the Eigenvectors\n\nFor each eigenvalue, we find the eigenvectors by solving the system:\n\n\\[\n(A - \\lambda I) \\mathbf{v} = 0\n\\]\n\nwhere \\(\\mathbf{v}\\) is the eigenvector.\n\n#### Eigenvector for \\(\\lambda_1 = 3\\)\n\nSet up the equation:\n\n\\[\n(A - 3I) \\mathbf{v} = 0 \\implies \\begin{bmatrix} 2 - 3 & 1 \\\\ 1 & 2 - 3 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\n\nThis gives us the system of equations:\n\n\\[\n\\begin{cases}\n- v_1 + v_2 = 0 \\\\\nv_1 - v_2 = 0\n\\end{cases}\n\\]\n\nBoth equations simplify to \\( v_1 = v_2 \\). Therefore, the eigenvector corresponding to \\(\\lambda_1 = 3\\) can be written as:\n\n\\[\n\\mathbf{v}_1 = \\begin{bmatrix} v_1 \\\\ v_1 \\end{bmatrix} = v_1 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\n\nWe can choose \\( v_1 = 1 \\), so:\n\n\\[\n\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\n\n#### Eigenvector for \\(\\lambda_2 = 1\\)\n\nSet up the equation:\n\n\\[\n(A - I) \\mathbf{v} = 0 \\implies \\begin{bmatrix} 2 - 1 & 1 \\\\ 1 & 2 - 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\n\nThis gives us the system of equations:\n\n\\[\n\\begin{cases}\nv_1 + v_2 = 0 \\\\\nv_1 + v_2 = 0\n\\end{cases}\n\\]\n\nBoth equations simplify to \\( v_1 = -v_2 \\). Therefore, the eigenvector corresponding to \\(\\lambda_2 = 1\\) can be written as:\n\n\\[\n\\mathbf{v}_2 = \\begin{bmatrix} v_1 \\\\ -v_1 \\end{bmatrix} = v_1 \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\]\n\nWe can choose \\( v_1 = 1 \\), so:\n\n\\[\n\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\]\n\n### Final Answer\n\nThe eigenvalues and their corresponding eigenvectors of the matrix \\( A \\) are:\n\n\\[\n\\boxed{\n\\begin{aligned}\n\\lambda_1 &= 3, & \\mathbf{v}_1 &= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\\\\n\\lambda_2 &= 1, & \\mathbf{v}_2 &= \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\end{aligned}\n}\n\\]\n\n### Additional Insights\n\n1. **Orthogonality of Eigenvectors**: Notice that the eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are orthogonal because their dot product is zero:\n\n\\[\n\\mathbf{v}_1 \\cdot \\mathbf{v}_2 = (1)(1) + (1)(-1) = 1 - 1 = 0\n\\]\n\nThis is a property of symmetric matrices, which \\( A \\) is, as \\( A = A^T \\).\n\n2. **Diagonalization**: Since \\( A \\) has distinct eigenvalues and is symmetric, it can be diagonalized as:\n\n\\[\nA = P D P^{-1}\n\\]\n\nwhere \\( P \\) is the matrix of eigenvectors and \\( D \\) is the diagonal matrix of eigenvalues:\n\n\\[\nP = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\]\n\nThe inverse of \\( P \\) is:\n\n\\[\nP^{-1} = \\frac{1}{\\det(P)} \\begin{bmatrix} -1 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\frac{1}{-2} \\begin{bmatrix} -1 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & -0.5 \\end{bmatrix}\n\\]\n\nThus,\n\n\\[\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & -0.5 \\end{bmatrix}\n\\]\n\nThis diagonalization can be useful for computing powers of \\( A \\), exponentials, and other matrix functions.\n\n3. **Geometric Interpretation**: The eigenvalues represent the scaling factors along the directions of their corresponding eigenvectors. For \\( \\lambda_1 = 3 \\), vectors along \\( \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\) are stretched by a factor of 3, and for \\( \\lambda_2 = 1 \\), vectors along \\( \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\) remain unchanged in length.\n\n4. **Application in Systems of Differential Equations**: The eigenvalues and eigenvectors can be used to solve systems of linear differential equations where the coefficient matrix is \\( A \\). The eigenvalues determine the stability and type of the equilibrium points, while the eigenvectors indicate the directions of the trajectories.\n\n5. **Principal Component Analysis (PCA)**: In statistics and data analysis, eigenvalues and eigenvectors are used in PCA to identify the directions of maximum variance in data. The eigenvector corresponding to the largest eigenvalue points in the direction of the greatest spread of data.\n\n6. **Quadratic Forms**: The matrix \\( A \\) defines a quadratic form \\( Q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\). The eigenvalues of \\( A \\) determine the nature of this quadratic form. Since both eigenvalues are positive, \\( Q(\\mathbf{x}) \\) is positive definite, indicating that the quadratic form is always positive for non-zero \\( \\mathbf{x} \\).\n\nBy understanding eigenvalues and eigenvectors, we gain insights into the behavior and properties of linear transformations represented by matrices, which is fundamental in many areas of mathematics, physics, engineering, and data science."
    }
]