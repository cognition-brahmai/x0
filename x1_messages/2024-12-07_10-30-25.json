[
    {
        "role": "user",
        "content": "Implement a Python script that reads data from a CSV file and inserts it into a MySQL table. The script should accept command line arguments for the CSV file path, MySQL connection details (host, port, username, password, and database name), and the target table name.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that reads data from a CSV file and inserts it into a MySQL table. It should also accept command line arguments for the CSV file path, MySQL connection details like host, port, username, password, and database name, and the target table name. Okay, let's break this down.\n\nFirst, I need to make sure I have Python set up on my machine. I think I do, but I should check. Yeah, I've got Python 3 installed. Good.\n\nNext, I need to handle command line arguments. I recall that in Python, there's a module called `argparse` that makes it easy to handle command line options and arguments. I should use that.\n\nSo, I'll start by importing `argparse`. Then, I'll define the arguments that the script accepts. That would be the CSV file path, MySQL host, port, username, password, database name, and the target table name.\n\nI should also think about how to make sure that sensitive information like the password is handled securely. Maybe I can look into ways to handle passwords without exposing them in the command line.\n\nBut for now, let's focus on setting up the argument parsing.\n\nHere's a rough outline of how the argument parsing part might look:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description='Import CSV to MySQL.')\nparser.add_argument('csv_path', type=str, help='Path to the CSV file')\nparser.add_argument('host', type=str, help='MySQL host')\nparser.add_argument('port', type=int, help='MySQL port')\nparser.add_argument('username', type=str, help='MySQL username')\nparser.add_argument('password', type=str, help='MySQL password')\nparser.add_argument('database', type=str, help='MySQL database name')\nparser.add_argument('table_name', type=str, help='Target table name')\n\nargs = parser.parse_args()\n```\n\nOkay, that seems straightforward. Now, I need to read the CSV file. Python has a built-in module for that called `csv`. I should import that.\n\nI need to consider the structure of the CSV file. Does it have headers? Are there any specific delimiters used? For now, I'll assume that it's a standard CSV with headers and comma-separated values.\n\nSo, I'll use `csv.reader` to read the file, specifying the delimiter as a comma.\n\nNext, I need to connect to the MySQL database. For that, I'll need to use a MySQL connector library in Python. I think `mysql-connector-python` is a common one. I should make sure it's installed.\n\nI can install it using pip if I don't have it:\n\n```\npip install mysql-connector-python\n```\n\nOnce I have the connector, I can establish a connection to the MySQL server using the provided connection details.\n\nHere's a rough idea of how that might look:\n\n```python\nimport mysql.connector\n\ntry:\n    connection = mysql.connector.connect(\n        host=args.host,\n        port=args.port,\n        user=args.username,\n        password=args.password,\n        database=args.database\n    )\n    if connection.is_connected():\n        print(\"Connected to MySQL database\")\nexcept Exception as e:\n    print(f\"Error connecting to MySQL: {e}\")\n```\n\nAlright, so after connecting, I need to insert the data from the CSV into the specified table.\n\nFirst, I need to read the data from the CSV file. I'll open the file using `open()` and use `csv.reader` to read its contents.\n\nAssuming the CSV has headers, I can read the headers to know the column names, which I'll need for the SQL insert statement.\n\nHere's how I might read the CSV:\n\n```python\nimport csv\n\ndata = []\nwith open(args.csv_path, mode='r', newline='', encoding='utf-8') as file:\n    reader = csv.reader(file)\n    headers = next(reader)  # Assuming first row is header\n    for row in reader:\n        data.append(row)\n```\n\nNow, `data` is a list of lists, where each inner list represents a row in the CSV.\n\nNext, I need to construct an SQL INSERT statement to insert this data into the specified table.\n\nThe INSERT statement would look something like:\n\n```sql\nINSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...);\n```\n\nSo, I need to construct this statement using the headers as the columns and the row data as the values.\n\nI need to make sure that the number of columns in the CSV matches the number of columns in the table. Otherwise, the insert will fail.\n\nTo make this dynamic, I'll create the INSERT statement using the headers.\n\nHere's an example of how to construct the INSERT statement:\n\n```python\ncolumns = ', '.join(headers)\nplaceholders = ', '.join(['%s'] * len(headers))\ninsert_query = f\"INSERT INTO {args.table_name} ({columns}) VALUES ({placeholders})\"\n```\n\nThis way, `%s` is a placeholder for the values, which will be provided when executing the query.\n\nNow, I need to execute this insert query for each row in the data.\n\nI'll use the connection's cursor to execute the queries.\n\nIt's important to handle exceptions and ensure that the transaction is committed or rolled back appropriately.\n\nHere's a rough outline of how that might look:\n\n```python\ncursor = connection.cursor()\ntry:\n    for row in data:\n        cursor.execute(insert_query, row)\n    connection.commit()\n    print(\"Data inserted successfully\")\nexcept Exception as e:\n    connection.rollback()\n    print(f\"Error inserting data: {e}\")\nfinally:\n    cursor.close()\n    connection.close()\n```\n\nThis is a basic structure, but I need to consider a few more things.\n\nFirst, error handling. What if the CSV file doesn't exist or is invalid? I need to handle file reading errors.\n\nSecond, what if the table doesn't exist in the database? The insert will fail. Maybe I should add an option to create the table if it doesn't exist, based on the CSV headers.\n\nThird, performance. If the CSV is large, inserting row by row might be inefficient. I should look into batch inserting or using executemany().\n\nFourth, data types. The CSV data is read as strings, but the database columns might have different types. I need to make sure that the data is converted appropriately.\n\nLet's address these one by one.\n\n1. Error handling for file reading:\n\nI can wrap the file reading code in a try-except block to handle cases where the file doesn't exist or can't be read.\n\n```python\ntry:\n    with open(args.csv_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n        data = [row for row in reader]\nexcept FileNotFoundError:\n    print(f\"Error: The file {args.csv_path} was not found.\")\nexcept Exception as e:\n    print(f\"Error reading CSV file: {e}\")\n```\n\n2. Table creation:\n\nI can add an optional command line argument to specify whether to create the table if it doesn't exist.\n\nFor table creation, I'll need to construct a CREATE TABLE statement using the CSV headers as column names. I'll assume all columns are of type VARCHAR for simplicity, unless specified otherwise.\n\nI can add an option like `--create-table` or `-c` to indicate that the table should be created.\n\nHere's how I might add that to the argument parser:\n\n```python\nparser.add_argument('-c', '--create_table', action='store_true', help='Create table if it does not exist')\n```\n\nThen, if `args.create_table` is True, I'll attempt to create the table.\n\nTo create the table, I need to know the column names and their types. Since I might not know the types, I'll default to VARCHAR(255).\n\nHere's how I might construct the CREATE TABLE statement:\n\n```python\ncreate_table_query = f\"CREATE TABLE IF NOT EXISTS {args.table_name} (\"\ncreate_table_query += \", \".join([f'{header} VARCHAR(255)' for header in headers])\ncreate_table_query += \")\"\n```\n\nThen, execute this query before inserting data.\n\n3. Performance for large CSV files:\n\nTo improve performance, I can use `executemany()` instead of executing individual inserts for each row.\n\n`executemany()` is designed to handle multiple rows at once, which is more efficient.\n\nHere's how I can modify the insert part:\n\n```python\ntry:\n    cursor.executemany(insert_query, data)\n    connection.commit()\n    print(\"Data inserted successfully\")\nexcept Exception as e:\n    connection.rollback()\n    print(f\"Error inserting data: {e}\")\nfinally:\n    cursor.close()\n    connection.close()\n```\n\n4. Data types:\n\nAs mentioned, the CSV data is read as strings, but the database columns might expect different types like integers or dates.\n\nTo handle this, I could add an option for the user to specify the column types when creating the table.\n\nFor now, to keep it simple, I'll assume all columns are VARCHAR and handle data as strings.\n\nBut if time permits, I can add more functionality to infer types or allow the user to specify them.\n\nAnother consideration is handling NULL values. If a cell in the CSV is empty, should it be inserted as an empty string or as NULL?\n\nIn SQL, empty strings and NULL have different meanings.\n\nI can add an option to treat empty cells as NULL.\n\nFor example, add another command line argument like `--nullify_empty`, and if set, replace empty strings in the data with NULL before inserting.\n\nHere's how I might add that to the argument parser:\n\n```python\nparser.add_argument('--nullify_empty', action='store_true', help='Treat empty strings as NULL')\n```\n\nThen, before inserting the data, I can replace empty strings with None (which represents NULL in MySQL connector):\n\n```python\nif args.nullify_empty:\n    data = [[None if cell == '' else cell for cell in row] for row in data]\n```\n\nAlright, I think I've covered the main points.\n\nLet me summarize the steps:\n\n1. Parse command line arguments for CSV path, MySQL connection details, table name, and optional flags for creating the table and nullifying empty strings.\n\n2. Connect to the MySQL database using the provided credentials.\n\n3. Read the CSV file, assuming the first row contains headers.\n\n4. If requested, create the table with columns corresponding to the CSV headers, all as VARCHAR(255).\n\n5. Construct an INSERT SQL query using the headers.\n\n6. Insert the data into the table using executemany() for better performance.\n\n7. Handle exceptions and ensure the connection is closed properly.\n\nI should also think about logging or providing more detailed output for debugging purposes.\n\nMaybe add verbose mode with another command line option, like `--verbose` or `-v`, which prints more information about the process.\n\nHere's how I can add that:\n\n```python\nparser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n```\n\nThen, throughout the script, I can print additional information if `args.verbose` is True.\n\nFor example, after reading the CSV, print the number of rows read.\n\nAfter inserting, print the number of rows inserted.\n\nThis will help in verifying that the script is working correctly.\n\nAlso, I should validate that the provided MySQL connection details are correct.\n\nPerhaps I can test the connection before proceeding further.\n\nSimilarly, check if the table exists (if not creating it), to prevent errors during insertion.\n\nWait, if I'm allowing table creation, then I can proceed with insertion assuming the table exists after creation.\n\nBut if not creating the table, I should check if the table exists before attempting to insert data.\n\nTo check if the table exists, I can execute a query like:\n\n```sql\nSELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'database_name' AND table_name = 'table_name';\n```\n\nIf the count is 0, the table doesn't exist.\n\nBut if I'm allowing table creation, I can skip this check.\n\nAlright, I think I have a plan.\n\nLet me start writing the script step by step.\n\nFirst, import the necessary modules:\n\n- argparse for command line arguments\n\n- csv for reading CSV files\n\n- mysql.connector for connecting to MySQL\n\nI should also import sys for handling system exits if needed.\n\nNext, set up the argument parser and define the required arguments.\n\nThen, connect to the MySQL database using the provided credentials.\n\nAfter connecting, read the CSV file and store its contents.\n\nIf the `--create_table` flag is set, create the table with columns based on the CSV headers.\n\nConstruct the INSERT query using the headers.\n\nIf `--nullify_empty` is set, replace empty strings in the data with None.\n\nExecute the insert query using executemany().\n\nCommit the transaction and close the connection.\n\nHandle exceptions appropriately to provide useful error messages.\n\nAlso, consider adding a main function and using the `if __name__ == '__main__'` guard.\n\nLet me sketch a rough structure of the script:\n\n```python\nimport argparse\nimport csv\nimport mysql.connector\nimport sys\n\ndef main():\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description='Import CSV to MySQL.')\n    # Define arguments\n    # ...\n    args = parser.parse_args()\n    \n    # Connect to MySQL\n    try:\n        connection = mysql.connector.connect(\n            host=args.host,\n            port=args.port,\n            user=args.username,\n            password=args.password,\n            database=args.database\n        )\n        if connection.is_connected():\n            print(\"Connected to MySQL database\")\n    except Exception as e:\n        print(f\"Error connecting to MySQL: {e}\")\n        sys.exit(1)\n    \n    # Read CSV\n    try:\n        with open(args.csv_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            headers = next(reader)\n            data = [row for row in reader]\n    except FileNotFoundError:\n        print(f\"Error: The file {args.csv_path} was not found.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        sys.exit(1)\n    \n    # Create table if requested\n    if args.create_table:\n        create_table_query = f\"CREATE TABLE IF NOT EXISTS {args.table_name} (\"\n        create_table_query += \", \".join([f'{header} VARCHAR(255)' for header in headers])\n        create_table_query += \")\"\n        try:\n            cursor = connection.cursor()\n            cursor.execute(create_table_query)\n            connection.commit()\n            print(\"Table created successfully\")\n        except Exception as e:\n            print(f\"Error creating table: {e}\")\n            connection.rollback()\n            sys.exit(1)\n        finally:\n            cursor.close()\n    \n    # Construct insert query\n    columns = ', '.join(headers)\n    placeholders = ', '.join(['%s'] * len(headers))\n    insert_query = f\"INSERT INTO {args.table_name} ({columns}) VALUES ({placeholders})\"\n    \n    # Nullify empty strings if requested\n    if args.nullify_empty:\n        data = [[None if cell == '' else cell for cell in row] for row in data]\n    \n    # Insert data\n    try:\n        cursor = connection.cursor()\n        cursor.executemany(insert_query, data)\n        connection.commit()\n        print(\"Data inserted successfully\")\n    except Exception as e:\n        print(f\"Error inserting data: {e}\")\n        connection.rollback()\n        sys.exit(1)\n    finally:\n        cursor.close()\n        connection.close()\n\nif __name__ == '__main__':\n    main()\n```\n\nThis is a basic structure. I need to test it with sample data to ensure it works as expected.\n\nI should also think about edge cases, such as:\n\n- Empty CSV files\n\n- CSV files with headers but no data rows\n\n- CSV files with missing values (though I'm handling empty strings)\n\n- Tables that already exist in the database\n\n- Incorrect MySQL credentials\n\n- Network issues preventing connection to the MySQL server\n\nI need to make sure that the script handles these scenarios gracefully, providing informative error messages and avoiding crashes.\n\nAdditionally, for security reasons, I should consider using parameterized queries to prevent SQL injection, although in this case, since the table name and column names are provided via command line arguments, I need to trust that the user provides valid input.\n\nBut to be safe, I can sanitize the input or use escaping methods provided by the MySQL connector.\n\nWait, actually, using `%s` placeholders in the INSERT query already handles escaping for the values, but the table name and column names need to be properly handled.\n\nI should ensure that the table name and column names are valid identifiers and not vulnerable to SQL injection.\n\nOne way to do this is to validate that they only contain allowed characters, like letters, numbers, and underscores.\n\nI can create a function to sanitize identifiers.\n\nHere's an example:\n\n```python\nimport re\n\ndef sanitize_identifier(identifier):\n    return re.sub(r'[^a-zA-Z0-9_]', '', identifier)\n```\n\nThen, apply this function to the table name and headers.\n\nBut actually, to prevent SQL injection, it's better to use the identifier quoting functions provided by the MySQL connector.\n\nFor example, use `connection.converter.escape(identifier)` or similar methods.\n\nI need to check the documentation for the MySQL connector to see the proper way to quote identifiers.\n\nLooking it up, I find that `connection.cursor().connection.escape(identifier)` can be used to escape identifiers.\n\nBut actually, for identifiers, it's better to use the `identifier_quote_char`, which is usually backtick (`) in MySQL.\n\nSo, I can define a function like:\n\n```python\ndef quote_identifier(identifier):\n    return '`{}`'.format(identifier)\n```\n\nThen, use this function when constructing the INSERT query and the CREATE TABLE query.\n\nThis will help prevent SQL injection by properly quoting the identifiers.\n\nSo, for example:\n\n```python\nquoted_table_name = quote_identifier(args.table_name)\nquoted_headers = [quote_identifier(header) for header in headers]\ncolumns = ', '.join(quoted_headers)\nplaceholders = ', '.join(['%s'] * len(headers))\ninsert_query = f\"INSERT INTO {quoted_table_name} ({columns}) VALUES ({placeholders})\"\n```\n\nAnd similarly for the CREATE TABLE query:\n\n```python\ncreate_table_query = f\"CREATE TABLE IF NOT EXISTS {quoted_table_name} (\"\ncreate_table_query += \", \".join([f'{quote_identifier(header)} VARCHAR(255)' for header in headers])\ncreate_table_query += \")\"\n```\n\nThis should help ensure that the identifiers are safely quoted.\n\nAnother consideration is character encoding. I've opened the CSV file with `utf-8` encoding, which should handle most cases, but if the CSV uses a different encoding, I need to allow the user to specify it.\n\nI can add another command line argument for the CSV encoding.\n\nFor example:\n\n```python\nparser.add_argument('--encoding', type=str, default='utf-8', help='CSV file encoding')\n```\n\nThen, use `args.encoding` when opening the CSV file.\n\nSimilarly, I can set the charset for the MySQL connection.\n\nWhen creating the connection, I can specify `charset='utf8mb4'` or whatever charset is appropriate.\n\nThis ensures that the data is handled correctly in both the CSV and the database.\n\nAlright, I think I've covered most of the bases here.\n\nI should also think about how to handle large CSV files without loading the entire file into memory.\n\nCurrently, I'm reading all rows into a list, which might not be suitable for very large files.\n\nTo handle large files, I can read and insert data in batches.\n\nI can add another command line argument for batch size, and insert rows in batches of that size.\n\nThis way, only a batch of rows is held in memory at a time.\n\nHere's how I might implement batching:\n\nAdd a new argument:\n\n```python\nparser.add_argument('--batch_size', type=int, default=1000, help='Number of rows to insert per batch')\n```\n\nThen, instead of inserting all data at once, iterate through the data in batches and execute insertions batch by batch.\n\nHere's an example:\n\n```python\nbatch_size = args.batch_size\nfor i in range(0, len(data), batch_size):\n    batch = data[i:i + batch_size]\n    try:\n        cursor.executemany(insert_query, batch)\n        connection.commit()\n        print(f\"Inserted batch {i//batch_size + 1}\")\n    except Exception as e:\n        connection.rollback()\n        print(f\"Error inserting batch {i//batch_size + 1}: {e}\")\n```\n\nThis way, only a batch of rows is processed at a time, reducing memory usage.\n\nI should also consider the performance impact of committing after each batch. In some cases, committing less frequently might be faster, but it could also lead to longer transaction times.\n\nI can add another option to control how often to commit, perhaps every N batches.\n\nBut for now, committing after each batch seems reasonable.\n\nAnother thing to consider is the time it takes to insert large amounts of data. Maybe I can display a progress bar to give an idea of how far along the insertion is.\n\nPython has libraries like `tqdm` that can be used to create progress bars.\n\nI can add an optional dependency on `tqdm` and use it if available.\n\nHere's how I might do that:\n\nFirst, install tqdm:\n\n```\npip install tqdm\n```\n\nThen, in the script:\n\n```python\ntry:\n    from tqdm import tqdm\n    has_tqdm = True\nexcept ImportError:\n    has_tqdm = False\n```\n\nThen, when inserting batches:\n\n```python\nif has_tqdm and args.verbose:\n    total_batches = len(data) // batch_size + (1 if len(data) % batch_size else 0)\n    for i in tqdm(range(0, len(data), batch_size), total=total_batches):\n        batch = data[i:i + batch_size]\n        try:\n            cursor.executemany(insert_query, batch)\n            connection.commit()\n        except Exception as e:\n            connection.rollback()\n            print(f\"Error inserting batch {i//batch_size + 1}: {e}\")\nelse:\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        try:\n            cursor.executemany(insert_query, batch)\n            connection.commit()\n        except Exception as e:\n            connection.rollback()\n            print(f\"Error inserting batch {i//batch_size + 1}: {e}\")\n```\n\nThis way, if `tqdm` is installed and verbose mode is enabled, a progress bar will be displayed.\n\nIf not, it will fall back to the regular batch insertion without a progress bar.\n\nI should also think about how to handle different CSV dialects, such as different delimiters or quote characters.\n\nI can add more command line arguments to specify these settings.\n\nFor example:\n\n```python\nparser.add_argument('--delimiter', type=str, default=',', help='CSV delimiter')\nparser.add_argument('--quotechar', type=str, default='\"', help='CSV quote character')\n```\n\nThen, use these when reading the CSV:\n\n```python\nwith open(args.csv_path, mode='r', newline='', encoding=args.encoding) as file:\n    reader = csv.reader(file, delimiter=args.delimiter, quotechar=args.quotechar)\n    headers = next(reader)\n    data = [row for row in reader]\n```\n\nThis provides more flexibility for different CSV formats.\n\nAnother feature could be to option to skip rows or select specific columns.\n\nBut that might complicate the script too much for now.\n\nI should also consider the scenario where the CSV has more or fewer columns than expected.\n\nIn such cases, the insert query will fail.\n\nI can add checks to ensure that the number of columns in the data matches the number of columns expected by the table.\n\nBut enforcing this strictly might not always be desirable, as sometimes users might want to insert only some columns.\n\nAlternatively, I can make the script more flexible by adjusting the insert query based on the data provided.\n\nBut to keep it simple, I'll assume that the CSV headers match the table columns in number and order.\n\nIf not, the user will need to preprocess the CSV or the table accordingly.\n\nAlright, I think I've covered enough ground here.\n\nI should proceed to implement the script based on this plan and then test it with sample data to ensure it works correctly.\n\n## Final Solution\nTo accomplish the task of importing data from a CSV file into a MySQL database using Python, we will use the `mysql-connector-python` library to handle the database connection and operations, and the `csv` module to read the CSV file. We will also utilize the `argparse` module to handle command-line arguments for flexibility.\n\n### Approach\n\n1. **Parse Command-Line Arguments:** \n   - Use `argparse` to accept parameters such as CSV file path, MySQL connection details, and target table name.\n   \n2. **Connect to the MySQL Database:**\n   - Establish a connection to the MySQL database using provided credentials.\n   \n3. **Read the CSV File:**\n   - Read the CSV file, assuming the first row contains headers.\n   \n4. **Create Table (Optional):**\n   - If specified, create the table in the database using the CSV headers as column names.\n   \n5. **Insert Data into the Table:**\n   - Construct an SQL INSERT query using the headers.\n   - Use `executemany()` for efficient batch insertion of data.\n   \n6. **Handle Exceptions and Ensure Proper Resource Management:**\n   - Handle exceptions to provide useful error messages.\n   - Ensure the database connection is closed properly after operations.\n\n### Solution Code\n\n```python\nimport argparse\nimport csv\nimport mysql.connector\nimport sys\nimport re\n\ndef main():\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description='Import CSV to MySQL.')\n    parser.add_argument('csv_path', type=str, help='Path to the CSV file')\n    parser.add_argument('host', type=str, help='MySQL host')\n    parser.add_argument('port', type=int, help='MySQL port')\n    parser.add_argument('username', type=str, help='MySQL username')\n    parser.add_argument('password', type=str, help='MySQL password')\n    parser.add_argument('database', type=str, help='MySQL database name')\n    parser.add_argument('table_name', type=str, help='Target table name')\n    parser.add_argument('-c', '--create_table', action='store_true', help='Create table if it does not exist')\n    parser.add_argument('--nullify_empty', action='store_true', help='Treat empty strings as NULL')\n    parser.add_argument('--encoding', type=str, default='utf-8', help='CSV file encoding')\n    parser.add_argument('--delimiter', type=str, default=',', help='CSV delimiter')\n    parser.add_argument('--quotechar', type=str, default='\"', help='CSV quote character')\n    parser.add_argument('--batch_size', type=int, default=1000, help='Number of rows to insert per batch')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n    \n    args = parser.parse_args()\n    \n    # Function to quote identifiers to prevent SQL injection\n    def quote_identifier(identifier):\n        return '`{}`'.format(identifier)\n    \n    # Connect to MySQL\n    try:\n        connection = mysql.connector.connect(\n            host=args.host,\n            port=args.port,\n            user=args.username,\n            password=args.password,\n            database=args.database,\n            charset='utf8mb4'  # Ensure UTF-8 encoding\n        )\n        if connection.is_connected():\n            print(\"Connected to MySQL database\")\n    except Exception as e:\n        print(f\"Error connecting to MySQL: {e}\")\n        sys.exit(1)\n    \n    # Read CSV\n    try:\n        with open(args.csv_path, mode='r', newline='', encoding=args.encoding) as file:\n            reader = csv.reader(file, delimiter=args.delimiter, quotechar=args.quotechar)\n            headers = next(reader)\n            data = [row for row in reader]\n    except FileNotFoundError:\n        print(f\"Error: The file {args.csv_path} was not found.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        sys.exit(1)\n    \n    # Create table if requested\n    if args.create_table:\n        quoted_headers = [quote_identifier(header) for header in headers]\n        create_table_query = f\"CREATE TABLE IF NOT EXISTS {quote_identifier(args.table_name)} (\"\n        create_table_query += \", \".join([f'{header} VARCHAR(255)' for header in quoted_headers])\n        create_table_query += \")\"\n        try:\n            cursor = connection.cursor()\n            cursor.execute(create_table_query)\n            connection.commit()\n            print(\"Table created successfully\")\n        except Exception as e:\n            print(f\"Error creating table: {e}\")\n            connection.rollback()\n            sys.exit(1)\n        finally:\n            cursor.close()\n    \n    # Construct insert query\n    quoted_table_name = quote_identifier(args.table_name)\n    quoted_headers = [quote_identifier(header) for header in headers]\n    columns = ', '.join(quoted_headers)\n    placeholders = ', '.join(['%s'] * len(headers))\n    insert_query = f\"INSERT INTO {quoted_table_name} ({columns}) VALUES ({placeholders})\"\n    \n    # Nullify empty strings if requested\n    if args.nullify_empty:\n        data = [[None if cell == '' else cell for cell in row] for row in data]\n    \n    # Insert data in batches\n    batch_size = args.batch_size\n    cursor = connection.cursor()\n    try:\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i + batch_size]\n            try:\n                cursor.executemany(insert_query, batch)\n                connection.commit()\n                if args.verbose:\n                    print(f\"Inserted batch {i//batch_size + 1}\")\n            except Exception as e:\n                connection.rollback()\n                print(f\"Error inserting batch {i//batch_size + 1}: {e}\")\n    except Exception as e:\n        print(f\"Error inserting data: {e}\")\n        connection.rollback()\n        sys.exit(1)\n    finally:\n        cursor.close()\n        connection.close()\n        print(\"Data insertion completed successfully.\")\n\nif __name__ == '__main__':\n    main()\n```\n\n### Explanation\n\n- **Argument Parsing:** We use `argparse` to handle command-line arguments for flexibility.\n- **Database Connection:** We use `mysql-connector-python` to connect to the MySQL database.\n- **CSV Reading:** The `csv` module is used to read the CSV file, assuming the first row contains headers.\n- **Table Creation:** If specified, the script creates the table in the database using the CSV headers.\n- **Data Insertion:** Data is inserted in batches for efficiency and to manage memory usage effectively.\n- **Error Handling:** The script includes robust error handling to manage exceptions during connection, reading, and insertion processes.\n\nThis script provides a comprehensive solution for importing CSV data into a MySQL database, with options for creating tables and handling large datasets efficiently."
    }
]