[
    {
        "role": "user",
        "content": "Develop a Java web scraper that extracts the titles and URLs of the top 10 search results from Google for a given query. Save the extracted data to a JSON file.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I'm going to try to help you with this task. So, you want to build a Java web scraper that can extract the titles and URLs of the top 10 search results from Google for a specific query, and then save that data into a JSON file. Sounds interesting!\n\nFirst off, I need to make sure I understand what's involved here. Web scraping is about extracting data from websites, right? In this case, it's Google search results. I've heard that scraping Google isn't straightforward because of their robots.txt file and other restrictions. I'll need to look into that.\n\nLet me start by setting up a new Java project. I'm familiar with IntelliJ IDEA, so I'll use that as my IDE. I'll create a new Java class for my scraper.\n\nNow, for web scraping in Java, there are several libraries I can use, like Jsoup, HTMLUnit, or even Selenium. Jsoup seems popular and straightforward for parsing HTML. Maybe I'll go with that.\n\nFirst, I need to add Jsoup to my project. Since I'm using Maven, I'll add the Jsoup dependency to my pom.xml file.\n\n```xml\n<dependency>\n    <groupId>org.jsoup</groupId>\n    <artifactId>jsoup</artifactId>\n    <version>1.14.3</version> <!-- Check for the latest version -->\n</dependency>\n```\n\nOkay, with Jsoup set up, I can start writing code to fetch the Google search results page.\n\nBut wait, I remember that Google doesn't allow scraping through their terms of service. There might be legal and ethical considerations here. Maybe I should look into using the Google Custom Search JSON API instead, which is designed for programmatic access to search results.\n\nLet me check the Google Developers website to see if there's an API that can fulfill this requirement.\n\nAh, yes, there's the Custom Search JSON API. It allows you to search through web content and returns the results in JSON format, which is perfect for what we need.\n\nUsing the API would be better than scraping, as it's against Google's terms to scrape their search results. Plus, it's more reliable and less likely to run into blocking issues.\n\nSo, I'll proceed with using the Google Custom Search JSON API.\n\nFirst, I need to set up a Custom Search Engine (CSE) through the Google CSE console.\n\nI'll go to https://cse.google.com/cse/setup/basic and follow the steps to create a new CSE.\n\nI can choose to search the entire web or specific sites. For this task, since it's to search the web, I'll select \"Search the entire web but emphasize included sites.\"\n\nI'll name my CSE something like \"Top Search Results\" and set the domain to www.google.com.\n\nAfter setting up the CSE, I need to enable the Custom Search JSON API in the Google Cloud Console.\n\nI'll go to https://console.developers.google.com/ and create a new project if I don't have one already.\n\nThen, enable the \"Custom Search JSON API\" for my project.\n\nNext, I need to create API keys for my project. I'll need both a API key and a CSE ID.\n\nIn the Cloud Console, under \"Credentials,\" I'll create a new API key.\n\nI should restrict this key to only access the Custom Search JSON API to avoid any misuse.\n\nAlso, set IP restrictions if necessary for security.\n\nOnce I have the API key, I need to note down the CSE ID from the CSE control panel.\n\nNow, with the API key and CSE ID, I can make requests to the Custom Search JSON API.\n\nThe endpoint is: https://www.googleapis.com/customsearch/v1\n\nI need to pass the following parameters in the query string:\n\n- key: [YOUR_API_KEY]\n\n- cx: [YOUR_CX_ID]\n\n- q: [SEARCH_QUERY]\n\n- num: 10 (to get top 10 results)\n\nFor example:\n\nhttps://www.googleapis.com/customsearch/v1?key=YOUR_API_KEY&cx=YOUR_CX_ID&q=java+programming&num=10\n\nThis should return a JSON response with the top 10 search results for \"java programming.\"\n\nI can use Java's HTTP libraries to make this GET request and parse the JSON response.\n\nIn Java, I can use the built-in HttpURLConnection class to make the HTTP request.\n\nFor parsing JSON, I can use the org.json library or Google's Gson library.\n\nI'll go with Gson because it's easier to work with complex JSON structures.\n\nSo, I'll add the Gson dependency to my pom.xml.\n\n```xml\n<dependency>\n    <groupId>com.google.code.gson</groupId>\n    <artifactId>gson</artifactId>\n    <version>2.8.8</version> <!-- Check for the latest version -->\n</dependency>\n```\n\nNow, I need to structure my Java class to handle the API request and JSON parsing.\n\nFirst, I'll create a method to build the API URL with the given query, API key, and CSE ID.\n\nThen, make the HTTP GET request and read the response.\n\nFinally, parse the JSON response to extract the titles and URLs of the search results.\n\nLet me sketch a rough outline of the class.\n\n```java\nimport java.io.*;\nimport java.net.*;\nimport com.google.gson.*;\n\npublic class GoogleScraper {\n\n    private static final String API_KEY = \"YOUR_API_KEY\";\n    private static final String CSE_ID = \"YOUR_CX_ID\";\n    private static final String BASE_URL = \"https://www.googleapis.com/customsearch/v1\";\n\n    public static void main(String[] args) {\n        String query = \"java programming\"; // Example query\n        try {\n            String json = fetchSearchResults(query);\n            saveToJSON(json, \"results.json\");\n            System.out.println(\"Data saved to results.json\");\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    private static String fetchSearchResults(String query) throws IOException {\n        // Build the URL\n        URL url = new URL(BASE_URL + \"?key=\" + API_KEY + \"&cx=\" + CSE_ID + \"&q=\" + URLEncoder.encode(query, \"UTF-8\") + \"&num=10\");\n        HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n        connection.setRequestMethod(\"GET\");\n\n        // Read the response\n        BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()));\n        String inputLine;\n        StringBuilder content = new StringBuilder();\n        while ((inputLine = in.readLine()) != null) {\n            content.append(inputLine);\n        }\n        in.close();\n        connection.disconnect();\n\n        return content.toString();\n    }\n\n    private static void saveToJSON(String json, String filename) throws FileNotFoundException {\n        try (PrintWriter out = new PrintWriter(filename)) {\n            out.print(json);\n        }\n    }\n}\n```\n\nThis is a basic structure. It fetches the search results for a given query and saves the raw JSON response to a file.\n\nBut, we need to extract only the titles and URLs of the top 10 results and save them in a JSON format.\n\nSo, I need to parse the JSON response to get the relevant data.\n\nLooking at the Custom Search JSON API documentation, the response structure is something like this:\n\n```json\n{\n  \"kind\": \"customsearch#search\",\n  \"url\": {\n    \"type\": \"application/json\",\n    \"template\": \"https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&imgsz={imgsz?}&imgtype={imgtype?}&imgcolor={imgcolor?}&imgDominantColor={imgDominantColor?}&alt=json\"\n  },\n  \"queries\": {\n    \"request\": [\n      {\n        \"title\": \"Google Custom Search - java programming\",\n        \"totalResults\": \"351000000\",\n        \"searchTerms\": \"java programming\",\n        \"count\": 10,\n        \"startIndex\": 1,\n        \"inputEncoding\": \"utf8\",\n        \"outputEncoding\": \"utf8\",\n        \"safe\": \"off\",\n        \"cx\": \"YOUR_CX_ID\"\n      }\n    ]\n  },\n  \"context\": {\n    \"title\": \"Google\"\n  },\n  \"searchInformation\": {\n    \"searchTime\": 0.353144,\n    \"formattedSearchTime\": \"0.35\",\n    \"totalResults\": \"351000000\",\n    \"formattedTotalResults\": \"351,000,000\"\n  },\n  \"items\": [\n    {\n      \"kind\": \"customsearch#result\",\n      \"title\": \"Java Programming Tutorials, Learn Java Online - javatpoint\",\n      \"htmlTitle\": \"Java Programming Tutorials, Learn Java Online - \\u003cb\\u003ejavatpoint\\u003c/b\\u003e\",\n      \"link\": \"https://www.javatpoint.com/java-tutorial\",\n      \"displayLink\": \"www.javatpoint.com\",\n      \"snippet\": \"Java is a high-level, class-based, object-oriented programming language that is ... Java is a general-purpose programming language that is class-based, object ...\",\n      \"htmlSnippet\": \"Java is a high-level, class-based, object-oriented programming language that is ... Java is a general-purpose programming language that is class-based, object ...\",\n      \"cacheId\": \"RxX3T1GZ9bQJ\",\n      \"formattedUrl\": \"https://www.javatpoint.com/java-tutorial\",\n      \"htmlFormattedUrl\": \"https://www.\\u003cb\\u003ejava\\u003c/b\\u003e\\u003cb\\u003etpoint\\u003c/b\\u003e.com/\\u003cb\\u003ejava\\u003c/b\\u003e-tutorial\",\n      \"pagemap\": {\n        \"cse_thumbnail\": [\n          {\n            \"src\": \"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcS9v37RrNw34z0bN58m2P5DG6uQa8WlL2i8X9qf66Zu1gqKi5LZ\",\n            \"width\": \"225\",\n            \"height\": \"225\"\n          }\n        ],\n        \"metatags\": [\n          {\n            \"msvalidate.01\": \"6E0A8B55AD688348100844899D76064B\",\n            \"og:image\": \"https://static.javatpoint.com/images/jtplogo.png\",\n            \"og:site_name\": \"Javatpoint\",\n            \"og:title\": \"Java Tutorial\",\n            \"twitter:card\": \"summary\",\n            \"og:description\": \"Java is a high-level, class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible.\",\n            \"viewport\": \"width=device-width, initial-scale=1.0\",\n            \"google-site-verification\": \"POEYYn2q3VXtdJ3J7IphO308C2YdU8D1P3D3eZ90NqI\",\n            \"og:url\": \"https://www.javatpoint.com/java-tutorial\"\n          }\n        ],\n        \"cse_image\": [\n          {\n            \"src\": \"https://static.javatpoint.com/images/jtplogo.png\"\n          }\n        ]\n      }\n    },\n    // More items...\n  ]\n}\n```\n\nFrom this response, the search results are in the \"items\" array. Each item has \"title\" and \"link\" fields that we need.\n\nSo, I'll need to parse the JSON to extract the \"items\" array and then collect the \"title\" and \"link\" from each item.\n\nUsing Gson, I can create a simple POJO to map the JSON structure.\n\nLet's define a class to hold the search results.\n\n```java\nimport com.google.gson.annotations.SerializedName;\n\nimport java.util.List;\n\npublic class SearchResult {\n    @SerializedName(\"items\")\n    private List<Item> items;\n\n    public List<Item> getItems() {\n        return items;\n    }\n\n    public static class Item {\n        @SerializedName(\"title\")\n        private String title;\n        @SerializedName(\"link\")\n        private String link;\n\n        public String getTitle() {\n            return title;\n        }\n\n        public String getLink() {\n            return link;\n        }\n    }\n}\n```\n\nNow, I can modify the fetchSearchResults method to parse the JSON response into this SearchResult object.\n\nAlso, I need to collect the titles and URLs into a list of maps or a custom object, and then save that list to a JSON file.\n\nLet me adjust the code accordingly.\n\nFirst, modify fetchSearchResults to return a SearchResult object.\n\n```java\nprivate static SearchResult fetchSearchResults(String query) throws IOException {\n    URL url = new URL(BASE_URL + \"?key=\" + API_KEY + \"&cx=\" + CSE_ID + \"&q=\" + URLEncoder.encode(query, \"UTF-8\") + \"&num=10\");\n    HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n    connection.setRequestMethod(\"GET\");\n\n    try (BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()))) {\n        String inputLine;\n        StringBuilder content = new StringBuilder();\n        while ((inputLine = in.readLine()) != null) {\n            content.append(inputLine);\n        }\n        connection.disconnect();\n        Gson gson = new Gson();\n        return gson.fromJson(content.toString(), SearchResult.class);\n    }\n}\n```\n\nThen, create a method to extract the titles and URLs and save them to a JSON file.\n\n```java\nprivate static void saveToJSON(SearchResult searchResult, String filename) throws FileNotFoundException {\n    List<SearchResult.Item> items = searchResult.getItems();\n    if (items != null) {\n        Gson gson = new GsonBuilder().setPrettyPrinting().create();\n        try (PrintWriter out = new PrintWriter(filename)) {\n            String json = gson.toJson(items);\n            out.print(json);\n        }\n    }\n}\n```\n\nThis way, the saveToJSON method will take the SearchResult object, extract the list of items, and save them to a JSON file in a readable format.\n\nLet me test this code with a sample query.\n\nI'll set the query to \"java programming\" and see if it fetches and saves the top 10 results correctly.\n\nAfter running the program, I should have a \"results.json\" file containing the titles and URLs of the top 10 search results.\n\nWait a minute, I need to replace \"YOUR_API_KEY\" and \"YOUR_CX_ID\" with actual values.\n\nI'll have to go back to the Google Cloud Console and Custom Search JSON API to get these values.\n\nOnce I have them, I can plug them into the code.\n\nAlso, I should handle any potential exceptions that might occur during the HTTP request or JSON parsing.\n\nFor example, if the API key is invalid or the CSE ID is incorrect, the API might return an error.\n\nI need to check the response code and handle errors gracefully.\n\nLet me add some error handling to the fetchSearchResults method.\n\n```java\nprivate static SearchResult fetchSearchResults(String query) throws IOException {\n    URL url = new URL(BASE_URL + \"?key=\" + API_KEY + \"&cx=\" + CSE_ID + \"&q=\" + URLEncoder.encode(query, \"UTF-8\") + \"&num=10\");\n    HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n    connection.setRequestMethod(\"GET\");\n\n    int responseCode = connection.getResponseCode();\n    if (responseCode != HttpURLConnection.HTTP_OK) {\n        throw new IOException(\"Failed to fetch search results. Response code: \" + responseCode);\n    }\n\n    try (BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()))) {\n        String inputLine;\n        StringBuilder content = new StringBuilder();\n        while ((inputLine = in.readLine()) != null) {\n            content.append(inputLine);\n        }\n        connection.disconnect();\n        Gson gson = new Gson();\n        return gson.fromJson(content.toString(), SearchResult.class);\n    }\n}\n```\n\nThis way, if the response code is not 200 OK, it throws an IOException with an appropriate message.\n\nAlso, I should consider rate limits and quotas for the Custom Search JSON API to avoid exceeding them.\n\nI can check the quota usage from the Google Cloud Console.\n\nAdditionally, I might want to add logging or print statements to debug any issues during development.\n\nNow, let's think about how to make this more user-friendly.\n\nMaybe I can accept the query as a command-line argument instead of hardcoding it.\n\nSo, in the main method, I can check if args[0] is provided and use it as the query.\n\nHere's how I can modify the main method:\n\n```java\npublic static void main(String[] args) {\n    if (args.length < 1) {\n        System.out.println(\"Please provide a search query.\");\n        return;\n    }\n    String query = args[0];\n    try {\n        SearchResult searchResult = fetchSearchResults(query);\n        saveToJSON(searchResult, \"results.json\");\n        System.out.println(\"Data saved to results.json\");\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n}\n```\n\nThis way, when running the program, the user needs to provide the search query as a command-line argument.\n\nFor example:\n\n```\njava GoogleScraper \"java programming\"\n```\n\nThis will fetch the top 10 search results for \"java programming\" and save them to \"results.json\".\n\nFurthermore, I can enhance the program by allowing the user to specify the output file name as a second argument.\n\nHere's an updated main method:\n\n```java\npublic static void main(String[] args) {\n    if (args.length < 1) {\n        System.out.println(\"Usage: java GoogleScraper \\\"search query\\\" [output_file.json]\");\n        return;\n    }\n    String query = args[0];\n    String outputFile = \"results.json\";\n    if (args.length >= 2) {\n        outputFile = args[1];\n    }\n    try {\n        SearchResult searchResult = fetchSearchResults(query);\n        saveToJSON(searchResult, outputFile);\n        System.out.println(\"Data saved to \" + outputFile);\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n}\n```\n\nNow, the user can specify the output file name, like:\n\n```\njava GoogleScraper \"java programming\" \"java_results.json\"\n```\n\nThis flexibility makes the scraper more useful for different queries and output filenames.\n\nAlso, I should consider sanitizing the input query to prevent any injection attacks or invalid characters in the URL.\n\nUsing URLEncoder.encode should handle most of that, but it's good to be cautious.\n\nMoreover, I might want to handle cases where there are fewer than 10 results or no results at all.\n\nIn such cases, the \"items\" list might be empty or have fewer elements.\n\nI need to make sure that the program handles these scenarios gracefully without throwing exceptions.\n\nIn the saveToJSON method, I've already checked if items is null before processing it, which is good.\n\nBut, to be thorough, I can add more checks.\n\nAdditionally, I can print the number of results fetched to inform the user.\n\nFor example:\n\n```java\nprivate static void saveToJSON(SearchResult searchResult, String filename) throws FileNotFoundException {\n    List<SearchResult.Item> items = searchResult.getItems();\n    if (items != null && !items.isEmpty()) {\n        Gson gson = new GsonBuilder().setPrettyPrinting().create();\n        try (PrintWriter out = new PrintWriter(filename)) {\n            String json = gson.toJson(items);\n            out.print(json);\n            System.out.println(\"Saved \" + items.size() + \" results to \" + filename);\n        }\n    } else {\n        System.out.println(\"No results found.\");\n    }\n}\n```\n\nThis way, the user knows whether any results were saved or not.\n\nFurthermore, I can add more features, like handling pagination to fetch more than 10 results, but for now, sticking to the top 10 should suffice.\n\nAnother consideration is to respect Google's terms of service and usage limits.\n\nI shouldn't make too many requests in a short period, as it might lead to IP blocking or API key restrictions.\n\nIt's always a good practice to throttle the requests or implement exponential backoff in case of errors.\n\nBut for a simple scraper like this, it should be fine as long as it's used responsibly.\n\nLastly, I should ensure that the API key has the necessary restrictions applied in the Google Cloud Console to prevent misuse.\n\nFor example, I can set IP restrictions to only allow requests from specific IP addresses.\n\nThis adds an extra layer of security to protect the API key.\n\nIn summary, here's the complete code for the GoogleScraper class:\n\n```java\nimport java.io.*;\nimport java.net.*;\nimport com.google.gson.*;\n\npublic class GoogleScraper {\n\n    private static final String API_KEY = \"YOUR_API_KEY\";\n    private static final String CSE_ID = \"YOUR_CX_ID\";\n    private static final String BASE_URL = \"https://www.googleapis.com/customsearch/v1\";\n\n    public static void main(String[] args) {\n        if (args.length < 1) {\n            System.out.println(\"Usage: java GoogleScraper \\\"search query\\\" [output_file.json]\");\n            return;\n        }\n        String query = args[0];\n        String outputFile = \"results.json\";\n        if (args.length >= 2) {\n            outputFile = args[1];\n        }\n        try {\n            SearchResult searchResult = fetchSearchResults(query);\n            saveToJSON(searchResult, outputFile);\n            System.out.println(\"Data saved to \" + outputFile);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    private static SearchResult fetchSearchResults(String query) throws IOException {\n        URL url = new URL(BASE_URL + \"?key=\" + API_KEY + \"&cx=\" + CSE_ID + \"&q=\" + URLEncoder.encode(query, \"UTF-8\") + \"&num=10\");\n        HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n        connection.setRequestMethod(\"GET\");\n\n        int responseCode = connection.getResponseCode();\n        if (responseCode != HttpURLConnection.HTTP_OK) {\n            throw new IOException(\"Failed to fetch search results. Response code: \" + responseCode);\n        }\n\n        try (BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()))) {\n            String inputLine;\n            StringBuilder content = new StringBuilder();\n            while ((inputLine = in.readLine()) != null) {\n                content.append(inputLine);\n            }\n            connection.disconnect();\n            Gson gson = new Gson();\n            return gson.fromJson(content.toString(), SearchResult.class);\n        }\n    }\n\n    private static void saveToJSON(SearchResult searchResult, String filename) throws FileNotFoundException {\n        List<SearchResult.Item> items = searchResult.getItems();\n        if (items != null && !items.isEmpty()) {\n            Gson gson = new GsonBuilder().setPrettyPrinting().create();\n            try (PrintWriter out = new PrintWriter(filename)) {\n                String json = gson.toJson(items);\n                out.print(json);\n                System.out.println(\"Saved \" + items.size() + \" results to \" + filename);\n            }\n        } else {\n            System.out.println(\"No results found.\");\n        }\n    }\n}\n\nclass SearchResult {\n    @SerializedName(\"items\")\n    private List<Item> items;\n\n    public List<Item> getItems() {\n        return items;\n    }\n\n    public static class Item {\n        @SerializedName(\"title\")\n        private String title;\n        @SerializedName(\"link\")\n        private String link;\n\n        public String getTitle() {\n            return title;\n        }\n\n        public String getLink() {\n            return link;\n        }\n    }\n}\n```\n\nDon't forget to replace \"YOUR_API_KEY\" and \"YOUR_CX_ID\" with your actual API key and CSE ID.\n\nThis should accomplish the task of fetching the top 10 search results from Google for a given query and saving their titles and URLs to a JSON file.\n\n## Final Solution\nTo achieve the task of extracting the titles and URLs of the top 10 search results from Google for a given query and saving them to a JSON file, we will use the Google Custom Search JSON API along with Java's built-in libraries and Gson for JSON handling.\n\n### Approach\n\n1. **Set Up Google Custom Search JSON API**:\n   - Create a Custom Search Engine (CSE) through the Google CSE console.\n   - Enable the Custom Search JSON API in the Google Cloud Console.\n   - Generate API keys and note down the CSE ID.\n\n2. **Make API Request**:\n   - Use `HttpURLConnection` to make a GET request to the Google Custom Search JSON API.\n   - Pass the API key, CSE ID, and the search query as parameters.\n\n3. **Parse JSON Response**:\n   - Use Gson to parse the JSON response and extract the titles and URLs of the search results.\n\n4. **Save Data to JSON File**:\n   - Collect the extracted data into a list and use Gson to save it into a JSON file.\n\n### Solution Code\n\n```java\nimport java.io.*;\nimport java.net.*;\nimport com.google.gson.*;\n\npublic class GoogleScraper {\n\n    // Replace with your actual API key and CSE ID\n    private static final String API_KEY = \"YOUR_API_KEY\";\n    private static final String CSE_ID = \"YOUR_CX_ID\";\n    private static final String BASE_URL = \"https://www.googleapis.com/customsearch/v1\";\n\n    public static void main(String[] args) {\n        if (args.length < 1) {\n            System.out.println(\"Usage: java GoogleScraper \\\"search query\\\" [output_file.json]\");\n            return;\n        }\n        String query = args[0];\n        String outputFile = \"results.json\";\n        if (args.length >= 2) {\n            outputFile = args[1];\n        }\n        try {\n            SearchResult searchResult = fetchSearchResults(query);\n            saveToJSON(searchResult, outputFile);\n            System.out.println(\"Data saved to \" + outputFile);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    private static SearchResult fetchSearchResults(String query) throws IOException {\n        URL url = new URL(BASE_URL + \"?key=\" + API_KEY + \"&cx=\" + CSE_ID + \"&q=\" + URLEncoder.encode(query, \"UTF-8\") + \"&num=10\");\n        HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n        connection.setRequestMethod(\"GET\");\n\n        int responseCode = connection.getResponseCode();\n        if (responseCode != HttpURLConnection.HTTP_OK) {\n            throw new IOException(\"Failed to fetch search results. Response code: \" + responseCode);\n        }\n\n        try (BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()))) {\n            String inputLine;\n            StringBuilder content = new StringBuilder();\n            while ((inputLine = in.readLine()) != null) {\n                content.append(inputLine);\n            }\n            connection.disconnect();\n            Gson gson = new Gson();\n            return gson.fromJson(content.toString(), SearchResult.class);\n        }\n    }\n\n    private static void saveToJSON(SearchResult searchResult, String filename) throws FileNotFoundException {\n        List<SearchResult.Item> items = searchResult.getItems();\n        if (items != null && !items.isEmpty()) {\n            Gson gson = new GsonBuilder().setPrettyPrinting().create();\n            try (PrintWriter out = new PrintWriter(filename)) {\n                String json = gson.toJson(items);\n                out.print(json);\n                System.out.println(\"Saved \" + items.size() + \" results to \" + filename);\n            }\n        } else {\n            System.out.println(\"No results found.\");\n        }\n    }\n}\n\nclass SearchResult {\n    @SerializedName(\"items\")\n    private List<Item> items;\n\n    public List<Item> getItems() {\n        return items;\n    }\n\n    public static class Item {\n        @SerializedName(\"title\")\n        private String title;\n        @SerializedName(\"link\")\n        private String link;\n\n        public String getTitle() {\n            return title;\n        }\n\n        public String getLink() {\n            return link;\n        }\n    }\n}\n```\n\n### Instructions\n\n1. **Set Up Google Cloud Project**:\n   - Create a project in the [Google Cloud Console](https://console.developers.google.com/).\n   - Enable the Custom Search JSON API.\n   - Generate API keys and note down the API key and CSE ID.\n\n2. **Add Dependencies**:\n   - Ensure you have Gson andJsoup in your project dependencies. For Maven, include:\n     ```xml\n     <dependency>\n         <groupId>com.google.code.gson</groupId>\n         <artifactId>gson</artifactId>\n         <version>2.8.8</version>\n     </dependency>\n     <dependency>\n         <groupId>org.jsoup</groupId>\n         <artifactId>jsoup</artifactId>\n         <version>1.14.3</version>\n     </dependency>\n     ```\n\n3. **Run the Scraper**:\n   - Replace `YOUR_API_KEY` and `YOUR_CX_ID` with your actual API key and CSE ID.\n   - Execute the scraper from the command line, providing the search query and optional output file name:\n     ```\n     java GoogleScraper \"java programming\" \"results.json\"\n     ```\n\nThis approach ensures that you are adhering to Google's terms of service by using their official API, and it provides a reliable way to fetch and save search results."
    }
]