[
    {
        "role": "user",
        "content": "Develop a Python script that uses the Beautiful Soup library to scrape a given Wikipedia page for all its headings (h1, h2, h3, etc.) and prints them out in a hierarchical format, indented according to their heading level."
    },
    {
        "role": "assistant",
        "content": "Sure, I can help you with that! To create a Python script that scrapes headings from a Wikipedia page using Beautiful Soup, we'll need to follow these steps:\n\n1. **Install Required Libraries**: Ensure that you have Beautiful Soup and Requests libraries installed. You can install them using pip if you haven't already.\n\n2. **Fetch the Web Page**: Use the Requests library to fetch the HTML content of the Wikipedia page.\n\n3. **Parse the HTML**: Use Beautiful Soup to parse the HTML content.\n\n4. **Extract Headings**: Find all the heading tags (h1, h2, h3, etc.) in the parsed HTML.\n\n5. **Print Headings Hierarchically**: Print the headings with indentation corresponding to their level.\n\nLet's go through each step in detail.\n\n### Step 1: Install Required Libraries\n\nFirst, make sure you have the necessary libraries installed. You can install them using pip:\n\n```bash\npip install beautifulsoup4 requests\n```\n\n### Step 2: Fetch the Web Page\n\nWe'll use the Requests library to get the HTML content of the Wikipedia page. For this example, let's assume we're scraping the main page.\n\n```python\nimport requests\n\nurl = 'https://en.wikipedia.org/wiki/Main_Page'\nresponse = requests.get(url)\nhtml_content = response.content\n```\n\n### Step 3: Parse the HTML\n\nNow, we'll parse the HTML content using Beautiful Soup.\n\n```python\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(html_content, 'html.parser')\n```\n\n### Step 4: Extract Headings\n\nWe need to find all the heading tags (h1, h2, h3, etc.) in the parsed HTML.\n\n```python\nheadings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n```\n\n### Step 5: Print Headings Hierarchically\n\nTo print the headings with appropriate indentation based on their level, we can determine the heading level from the tag name and indent accordingly.\n\n```python\nfor heading in headings:\n    level = int(heading.name[1])  # Extract the level from the tag name, e.g., 'h1' -> 1\n    indent = '  ' * (level - 1)  # Calculate indentation\n    print(f\"{indent}{heading.text.strip()}\")\n```\n\n### Complete Script\n\nPutting it all together, here's the complete Python script:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef main():\n    url = 'https://en.wikipedia.org/wiki/Main_Page'\n    response = requests.get(url)\n    html_content = response.content\n    \n    soup = BeautifulSoup(html_content, 'html.parser')\n    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n    \n    for heading in headings:\n        level = int(heading.name[1])\n        indent = '  ' * (level - 1)\n        print(f\"{indent}{heading.text.strip()}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n### Explanation\n\n1. **Fetching the Page**: We use `requests.get` to fetch the HTML content of the Wikipedia page.\n2. **Parsing HTML**: `BeautifulSoup` is used to parse the HTML content.\n3. **Extracting Headings**: We find all heading tags using `soup.find_all`.\n4. **Printing Hierarchically**: We determine the heading level, calculate the indentation, and print the heading text.\n\nThis script will print all the headings from the specified Wikipedia page in a hierarchical format, with each level indented appropriately. You can modify the `url` variable to scrape different pages."
    }
]