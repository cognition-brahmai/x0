[
    {
        "role": "user",
        "content": "Create a Python script that uses the Beautiful Soup library to scrape a given Wikipedia page for all its internal links (links pointing to other Wikipedia pages). The script should take the URL of the Wikipedia page as a command-line argument and output the scraped links in a JSON format.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that scrapes internal links from a Wikipedia page using Beautiful Soup, and then outputs those links in JSON format. The script should take the URL of the Wikipedia page as a command-line argument. Sounds straightforward, but I need to make sure I cover all the bases here.\n\nFirst things first, I need to make sure I have Beautiful Soup installed. I think it's called `beautifulsoup4`, and I might need `requests` to fetch the page content as well. Oh, and for parsing the URL to make sure it's a Wikipedia page, maybe I'll need `urllib.parse` or something similar.\n\nSo, the plan is:\n\n1. Check if the correct number of command-line arguments is provided. The script should expect one argument: the URL of the Wikipedia page.\n\n2. Validate that the provided URL is indeed a Wikipedia page. Maybe check if it contains \"wikipedia.org/wiki/\" or something like that.\n\n3. Use requests to fetch the content of the page.\n\n4. Pass the content to Beautiful Soup for parsing.\n\n5. Find all the anchor tags (`<a>`) that have href attributes pointing to other Wikipedia pages.\n\n6. Extract those hrefs, make sure they are internal links, and collect them.\n\n7. Output the list of links in JSON format.\n\nAlright, let's start by setting up the script to handle command-line arguments. I need to check if exactly one argument is provided besides the script name. If not, print an error message and exit.\n\nNext, validate the URL. I can use `urllib.parse` to parse the URL and check if the netloc is 'en.wikipedia.org' or similar. I should also check that the path starts with '/wiki/'.\n\nThen, use requests to get the page content. I should handle possible exceptions, like if the page doesn't exist or there's no internet connection.\n\nOnce I have the content, I can pass it to Beautiful Soup. I should specify the parser to use, maybe 'html.parser' since it's built-in.\n\nNow, to find internal links. Internal links on Wikipedia should be those that are relative or start with '/wiki/' or have absolute URLs containing 'wikipedia.org/wiki/'.\n\nI need to loop through all `<a>` tags, check if they have an href attribute, and then check if the href points to another Wikipedia page.\n\nFor each valid link, I'll collect it, perhaps storing just the page title part after '/wiki/' to make it cleaner.\n\nFinally, I'll dump the list of links into a JSON array and print it out.\n\nLet me think about how to make sure I'm only getting internal links. Maybe I can check if the href starts with '/wiki/' or if it's a relative URL that starts with '../wiki/' or something like that. Also, I should avoid special pages like categories, user pages, etc., but maybe that's going too far.\n\nHmm, perhaps for now, I'll just check if the href starts with '/wiki/' and doesn't contain any special substrings like 'Special:', 'Category:', 'User:', etc. I can maintain a list of such substrings to exclude.\n\nAlso, I should handle URL encoding, in case page titles have special characters.\n\nOh, and I should probably remove any query parameters from the hrefs, as they might not be needed for just getting the page title.\n\nLet me outline the steps in code:\n\n- Import necessary modules: `sys`, `json`, `requests`, `urllib.parse`, and `bs4`.\n\n- Check command-line arguments.\n\n- Validate the provided URL.\n\n- Fetch the page content using requests.\n\n- Parse the content with Beautiful Soup.\n\n- Find all `<a>` tags with href attributes.\n\n- For each href:\n\n- Normalize the URL to get the path.\n\n- Check if it starts with '/wiki/' and doesn't contain excluded substrings.\n\n- Extract the page title from the path.\n\n- Collect the titles in a list.\n\n- Dump the list to JSON and print it.\n\nI need to be careful with relative URLs. Some hrefs might be relative, like starting with '../wiki/', so I need to handle that properly.\n\nMaybe I can use `urllib.parse.urljoin` to get the absolute URL from the href and then parse it to get the path.\n\nLet me think about an example. Suppose the base URL is 'https://en.wikipedia.org/wiki/Web_scraping', and there's a link with href='/wiki/Beautiful_Soup', which is straightforward.\n\nBut what if there's a link like '../wiki/Python_(programming_language)'? `urljoin` should handle that correctly.\n\nAlso, there might be links with fragments, like '/wiki/Web_scraping#Implementation', but I can ignore the fragment part for just getting the page title.\n\nI should also make sure to handle URLs with query parameters, like '/wiki/Web_scraping?foo=bar', and strip them off.\n\nIn Beautiful Soup, I can access the href attribute with `tag['href']`, and then use `urllib.parse.urlparse` to parse it and get the path.\n\nThen, from the path, I can extract the page title by splitting on '/' and taking the last part.\n\nAfter that, decode the title to handle any percent-encoded characters.\n\nFinally, collect all such titles in a list and output them as a JSON array.\n\nI should also consider performance, but since Wikipedia pages aren't too large, and Beautiful Soup is efficient enough for this purpose, I don't think it'll be an issue.\n\nError handling is important. I need to handle cases where the URL is invalid, the page doesn't exist, or there's no internet connection.\n\nI can use try-except blocks around the requests.get call and any potential URL parsing errors.\n\nAlso, I should make sure that if Beautiful Soup doesn't find any `<a>` tags, the script still runs correctly and outputs an empty JSON array.\n\nLet me consider writing some pseudocode to organize my thoughts:\n\n```\n\nimport sys\n\nimport json\n\nimport requests\n\nfrom urllib.parse import urljoin, urlparse, unquote\n\nfrom bs4 import BeautifulSoup\n\nif len(sys.argv) != 2:\n\n    print(\"Usage: python script.py <wikipedia_url>\")\n\n    sys.exit(1)\n\nurl = sys.argv[1]\n\nif not url.startswith('https://en.wikipedia.org/wiki/'):\n\n    print(\"Error: Please provide a valid Wikipedia page URL.\")\n\n    sys.exit(1)\n\ntry:\n\n    response = requests.get(url)\n\n    response.raise_for_status()\n\nexcept requests.RequestException as e:\n\n    print(f\"Error fetching the URL: {e}\")\n\n    sys.exit(1)\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\ninternal_links = []\n\nexcluded_substrings = ['Special:', 'Category:', 'User:', 'File:', 'Help:', 'Template:']\n\nfor a in soup.find_all('a', href=True):\n\n    href = a['href']\n\n    absolute_url = urljoin(url, href)\n\n    parsed_url = urlparse(absolute_url)\n\n    path = parsed_url.path\n\n    if path.startswith('/wiki/'):\n\n        for substr in excluded_substrings:\n\n            if substr in path:\n\n                break\n\n        else:\n\n            title = path.split('/')[-1]\n\n            decoded_title = unquote(title)\n\n            internal_links.append(decoded_title)\n\nprint(json.dumps(internal_links, indent=4))\n\n```\n\nThat seems like a good starting point. I should test this with a few Wikipedia pages to see if it works as expected.\n\nLet me think about potential edge cases:\n\n- Pages with no internal links.\n\n- Pages with lots of links.\n\n- Links with special characters in the title.\n\n- Links with query parameters.\n\n- Links that are redirects.\n\n- Links that are disabled or have other attributes.\n\nI should also consider that some links might be in different languages, but since we're scraping English Wikipedia, I'll assume they are English pages.\n\nAdditionally, I might want to avoid collecting the same link multiple times, so perhaps I can use a set to collect the titles instead of a list.\n\nBut Wikipedia pages can have multiple links to the same page, so maybe it's okay to have duplicates if that's how the page is structured.\n\nWait, but for simplicity, I can collect them in a set to avoid duplicates.\n\nBut then, the order might not be preserved, which might not matter for this purpose.\n\nAlternatively, I can collect them in a list and handle duplicates if needed.\n\nI'll keep it as a list for now.\n\nAlso, I should think about whether to include or exclude links that are commented out or in disabled elements, but I think Beautiful Soup will only pick up links that are actually in the HTML.\n\nI should also consider that some links might be in infoboxes, references, or other elements that might not be part of the main article content, but for the scope of this task, I think collecting all internal links is sufficient.\n\nAnother thing to think about is that Wikipedia pages have a lot of elements like tables, lists, etc., but Beautiful Soup should handle them all when iterating through `<a>` tags.\n\nI should also make sure that the script doesn't follow any redirects automatically, but requests.get should handle redirects by default, so that should be fine.\n\nWait, no, requests follows redirects by default, which is what we want, to get the final page content.\n\nBut I need to make sure that the base URL is correctly set for resolving relative links.\n\nBeautiful Soup should handle that with the `urljoin` function.\n\nLet me think about how to test this script.\n\nI can use a known Wikipedia page, like 'https://en.wikipedia.org/wiki/Web_scraping', and see if it correctly extracts all internal links.\n\nI can manually check a few links to ensure they are indeed internal Wikipedia links.\n\nI should also test with a page that has various types of links, including those with special characters and query parameters.\n\nAdditionally, I can test with a page that has no internal links, like a category page or a disambiguation page, to see how the script handles it.\n\nI should also consider pages in other languages, but since the script is designed for English Wikipedia, I'll assume the input URL is always an English Wikipedia page.\n\nWait, but the URL validation checks for 'en.wikipedia.org', so it should be fine.\n\nI should also think about pages that are protected or have limited access, but requests should still be able to fetch them as long as they are publicly accessible.\n\nAnother thing to consider is that Wikipedia's robots.txt allows scraping, but I should still be respectful and not hammer their servers with too many requests.\n\nBut since this script is for educational purposes and likely to be used sparingly, it should be okay.\n\nI should also include a user-agent string in the requests to identify myself.\n\nI can set a custom User-Agent header with requests.\n\nSomething like \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n\nWait, but it's better to include a custom user-agent that identifies the script and perhaps provides contact information, as per Wikipedia's scraping guidelines.\n\nI can set something like \"MyScraper/1.0 (+https://example.com/my_scraper; myemail@example.com)\"\n\nBut since this is a hypothetical script, I'll just use a generic user-agent.\n\nLet me add that to the requests.get call.\n\nAlso, I should consider adding a delay between requests, but since this script is per-page, and not making multiple requests, it's probably not necessary.\n\nBut if someone were to use this script in a loop to scrape multiple pages, they should be mindful of Wikipedia's rate limits.\n\nI should perhaps include a note in the script about usage guidelines.\n\nBut since this is just the script, I'll assume the user is responsible for adhering to Wikipedia's policies.\n\nLet me also think about how to handle URLs that are already absolute URLs pointing to other Wikipedia pages.\n\nFor example, href=\"https://en.wikipedia.org/wiki/Beautiful_Soup\"\n\nurljoin should handle that correctly and not modify the URL.\n\nThen, urlparse can extract the path from it.\n\nSimilarly, relative URLs like \"/wiki/Python\" should be resolved correctly.\n\nWhat about URLs that have a leading '#' for fragments?\n\nWell, those are internal anchors and should be ignored since they point to the same page.\n\nWait, no, fragments can point to sections within the same page or different pages.\n\nBut in Wikipedia, sections are part of the page, so if the href is like '#See_also', it's pointing to the same page's section.\n\nI should exclude those, as they don't point to other pages.\n\nSimilarly, if the href starts with '#' , it's an anchor on the same page.\n\nSo, I need to make sure to exclude hrefs that start with '#'.\n\nI can add a condition to check if href starts with '#'' and skip those.\n\nAlso, hrefs that start with '//' are protocol-relative URLs, but Wikipedia should not have many of those.\n\nStill, to be safe, I can check if the href starts with '//', and perhaps prepend 'https:' to make them absolute.\n\nWait, but Wikipedia URLs are https, so hrefs starting with '//' would point to http://en.wikipedia.org/, which is unlikely.\n\nI think it's safe to assume that hrefs starting with '//' are not internal links.\n\nAlternatively, I can use urljoin to resolve them correctly.\n\nLet me check the Beautiful Soup documentation for handling URLs.\n\nYes, urljoin takes the base URL and the href, and resolves it correctly.\n\nSo, as long as I pass the base URL, it should handle relative URLs properly.\n\nI should make sure to pass the base URL without any trailing slashes.\n\nWait, no, urljoin handles that.\n\nLet me verify with an example.\n\nSuppose base_url is 'https://en.wikipedia.org/wiki/Web_scraping'\n\nAnd href is '/wiki/Beautiful_Soup'\n\nThen, urljoin(base_url, href) should give 'https://en.wikipedia.org/wiki/Beautiful_Soup'\n\nSimilarly, if href is '../wiki/Python_(programming_language)', it should resolve correctly.\n\nWait, no, '../' would go up one directory, which in URL terms would be 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n\nWait, no, 'https://en.wikipedia.org/wiki/Web_scraping' is already in the wiki directory, so '../wiki/Python_' would still be 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n\nWait, no, '../wiki/Python_' from 'https://en.wikipedia.org/wiki/Web_scraping' would still be 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n\nWait, no, './wiki/Python_' would be 'https://en.wikipedia.org/wiki/wiki/Python_', which is incorrect.\n\nWait, I'm getting confused.\n\nLet me look up how urljoin works.\n\nAccording to the urllib.parse documentation, urljoin(base, url) constructs a full (\"absolute\") URL by combining a \"base URL\" (`base`) with another URL (`url`).\n\nIt supports partial URLs and uses the semantics of URL concatenation.\n\nSo, if base is 'https://en.wikipedia.org/wiki/Web_scraping' and url is '/wiki/Beautiful_Soup', urljoin should give 'https://en.wikipedia.org/wiki/Beautiful_Soup'\n\nIf url is '../wiki/Python_', it should resolve to 'https://en.wikipedia.org/wiki/Python_'\n\nBut in practice, Wikipedia URLs don't use '../' in their internal links; they usually use '/wiki/' followed by the page title.\n\nSo, to keep it simple, I'll assume that all internal links start with '/wiki/'\n\nThen, I can restrict the check to only include hrefs that start with '/wiki/'\n\nAnd exclude any hrefs that start with '#' or other protocols.\n\nWait, but to be thorough, I should still handle hrefs that might start with '../wiki/', but urljoin should take care of that.\n\nLet me confirm with an example.\n\nbase_url = 'https://en.wikipedia.org/wiki/Web_scraping'\n\nhref = '../wiki/Python_'\n\nabsolute_url = urljoin(base_url, href)\n\nprint(absolute_url)\n\nThis should output 'https://en.wikipedia.org/wiki/Python_'\n\nYes, that works.\n\nSo, I can rely on urljoin to resolve the hrefs correctly.\n\nNow, about extracting the page title from the path.\n\nSuppose path is '/wiki/Web_scraping', then title is 'Web_scraping'\n\nIf path is '/wiki/Python_(programming_language)', title is 'Python_(programming_language)'\n\nIf path is '/wiki/Special:Search', title is 'Special:Search', but I'm excluding special pages anyway.\n\nI need to make sure that the path is exactly '/wiki/some_page', without any extra path components.\n\nWait, but some pages might have subpages, like '/wiki/Category:Some_category', but I'm excluding categories already.\n\nWait, no, '/wiki/Category:Some_category' would be excluded because it contains 'Category:'.\n\nSimilarly, '/wiki/User:Some_user' would be excluded due to 'User:'.\n\nSo, as long as I exclude hrefs containing those substrings, I should be fine.\n\nI should make sure that the excluded_substrings list is comprehensive enough to cover all special namespaces.\n\nLooking at Wikipedia's namespace documentation, some common namespaces to exclude might include:\n\n- Special:\n\n- Category:\n\n- File:\n\n- Help:\n\n- Template:\n\n- User:\n\n- Talk:\n\n- Wikipedia:\n\n- Portal:\n\n- Book:\n\n- Draft:\n\n- Education Program:\n\n- TimedText:\n\n- Module:\n\n- MediaWiki:\n\n- Category talk:\n\n- Template talk:\n\n- Help talk:\n\n- File talk:\n\n- MediaWiki talk:\n\n- Template talk:\n\n- User talk:\n\n- Wikipedia talk:\n\n- Book talk:\n\n- Draft talk:\n\n- Education Program talk:\n\n- File talk:\n\n- Help talk:\n\n- Module talk:\n\n- Talk:\n\n- TimedText talk:\n\n- User talk:\n\n- Wikipedia talk:\n\nThis list might be too exhaustive, but to be safe, I can include them all.\n\nAlternatively, I can check if the path contains '/wiki/' followed by any of these namespace prefixes.\n\nBut that might be inefficient.\n\nMaybe a better way is to extract the page title from the path and then check if it starts with any of these namespace names.\n\nWait, but some namespaces might not be directly detectable from the path.\n\nI think maintaining a list of excluded substrings in the path is sufficient for this purpose.\n\nI'll compile a list of substrings to exclude based on common Wikipedia namespaces.\n\nNow, about encoding. Wikipedia page titles can contain special characters that are percent-encoded in the URL.\n\nFor example, '(', ')', spaces, etc.\n\nSo, after extracting the title from the path, I need to decode it to get the actual page title.\n\nI can use urllib.parse.unquote to decode the title.\n\nFor example, 'Python_%28programming_language%29' should be decoded to 'Python_(programming_language)'\n\nThen, I can collect the decoded titles in the list.\n\nI should also make sure that the script doesn't collect external links, like links to other websites.\n\nSince internal links should all start with '/wiki/', and external links have absolute URLs pointing elsewhere, urljoin should help\u533a\u5206\u5b83\u4eec\u3002\n\n\u53e6\u5916\uff0c\u6709\u4e9b\u94fe\u63a5\u53ef\u80fd\u5305\u542b\u67e5\u8be2\u53c2\u6570\uff0c\u6bd4\u5982 `/wiki/Web_scraping?foo=bar`\uff0c\u4f46\u6211\u53ea\u9700\u8981 `/wiki/Web_scraping` \u90e8\u5206\u3002\n\n\u56e0\u6b64\uff0c\u5728\u63d0\u53d6\u8def\u5f84\u540e\uff0c\u6211\u5e94\u8be5\u53bb\u9664\u4efb\u4f55\u67e5\u8be2\u53c2\u6570\u3002\n\n\u6211\u53ef\u4ee5\u4f7f\u7528 `urlparse` \u7684 `path` \u5c5e\u6027\u6765\u83b7\u53d6\u8def\u5f84\uff0c\u800c\u4e0d\u5305\u542b\u67e5\u8be2\u53c2\u6570\u3002\n\n\u518d\u8005\uff0c\u6709\u4e9b\u94fe\u63a5\u53ef\u80fd\u5305\u542b\u7247\u6bb5\u6807\u8bc6\u7b26\uff0c\u6bd4\u5982 `/wiki/Web_scraping#See_also`\uff0c\u6211\u4e5f\u53ea\u9700\u8981 `/wiki/Web_scraping` \u90e8\u5206\u3002\n\n\u6240\u4ee5\uff0c\u540c\u6837\u5730\uff0c\u4f7f\u7528 `path` \u5c5e\u6027\u53ef\u4ee5\u5ffd\u7565\u7247\u6bb5\u3002\n\n\u73b0\u5728\uff0c\u8ba9\u6211\u8003\u8651\u4e00\u4e0b\u5982\u4f55\u5904\u7406\u91cd\u5b9a\u5411\u3002\n\n\u5047\u5982\u67d0\u4e2a\u94fe\u63a5\u6307\u5411\u4e00\u4e2a\u91cd\u5b9a\u5411\u9875\u9762\uff0c\u6bd4\u5982 `/wiki/Some_redirect_page`\uff0cwhich then redirects to another page.\n\n\u4f46\u662f\uff0c\u5728\u6211\u7684\u811a\u672c\u4e2d\uff0c\u6211\u53ea\u6293\u53d6\u9875\u9762\u4e0a\u7684\u94fe\u63a5\uff0c\u4e0d\u8ddf\u968f\u91cd\u5b9a\u5411\u3002\n\n\u6240\u4ee5\uff0c\u6211\u4f1a\u6293\u53d6 `/wiki/Some_redirect_page`\uff0c\u800c\u4e0d\u662f\u5b83\u91cd\u5b9a\u5411\u5230\u7684\u9875\u9762\u3002\n\n\u5982\u679c\u9700\u8981\uff0c\u6211\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5904\u7406\u91cd\u5b9a\u5411\uff0c\u4f46\u90a3\u4f1a\u590d\u6742\u5f97\u591a\uff0c\u53ef\u80fd\u9700\u8981\u67e5\u8be2 Wikipedia \u7684 API \u6765\u89e3\u6790\u91cd\u5b9a\u5411\u3002\n\n\u4f46\u5bf9\u4e8e\u8fd9\u4e2a\u4efb\u52a1\u6765\u8bf4\uff0c\u53ea\u9700\u6293\u53d6\u9875\u9762\u4e0a\u7684\u94fe\u63a5\uff0c\u4e0d\u9700\u8981\u5904\u7406\u91cd\u5b9a\u5411\u3002\n\n\u4e0b\u4e00\u6b65\u662f\u5b9e\u73b0\u4ee3\u7801\u3002\n\n\u6211\u4f1a\u6309\u7167\u4e4b\u524d\u8ba1\u5212\u7684\u6b65\u9aa4\u6765\u7f16\u5199\u4ee3\u7801\u3002\n\n\u9996\u5148\uff0c\u5bfc\u5165\u5fc5\u8981\u7684\u6a21\u5757\uff1a\n\n- `sys` \u7528\u4e8e\u547d\u4ee4\u884c\u53c2\u6570\n\n- `json` \u7528\u4e8e\u8f93\u51fa JSON\n\n- `requests` \u7528\u4e8e\u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n\n- `urllib.parse` \u7528\u4e8e URL \u64cd\u4f5c\n\n- `bs4` \u7528\u4e8e\u89e3\u6790 HTML\n\n\u7136\u540e\uff0c\u68c0\u67e5\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u786e\u4fdd\u63d0\u4f9b\u4e86\u4e14\u4ec5\u63d0\u4f9b\u4e86\u4e00\u4e2a URL\u3002\n\n\u63a5\u7740\uff0c\u9a8c\u8bc1 URL \u662f\u5426\u662f\u6709\u6548\u7684 Wikipedia \u9875\u9762 URL\u3002\n\n\u7136\u540e\uff0c\u4f7f\u7528 `requests.get` \u83b7\u53d6\u9875\u9762\u5185\u5bb9\uff0c\u5e76\u5904\u7406\u53ef\u80fd\u7684\u8bf7\u6c42\u5f02\u5e38\u3002\n\n\u7136\u540e\uff0c\u4f7f\u7528 Beautiful Soup \u89e3\u6790\u9875\u9762\u5185\u5bb9\u3002\n\n\u7136\u540e\uff0c\u627e\u5230\u6240\u6709\u7684 `<a>` \u6807\u7b7e\uff0c\u5e76\u68c0\u67e5\u5b83\u4eec\u7684 `href` \u5c5e\u6027\u3002\n\n\u5bf9\u4e8e\u6bcf\u4e2a `href`\uff1a\n\n- \u4f7f\u7528 `urljoin` \u6784\u5efa\u7edd\u5bf9 URL\n\n- \u4f7f\u7528 `urlparse` \u89e3\u6790 URL\uff0c\u83b7\u53d6\u8def\u5f84\n\n- \u68c0\u67e5\u8def\u5f84\u662f\u5426\u4ee5 `/wiki/` \u5f00\u59cb\n\n- \u68c0\u67e5\u8def\u5f84\u662f\u5426\u4e0d\u5305\u542b\u6392\u9664\u7684\u5b50\u5b57\u7b26\u4e32\n\n- \u63d0\u53d6\u9875\u9762\u6807\u9898\uff0c\u5e76\u89e3\u7801\n\n- \u5c06\u9875\u9762\u6807\u9898\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\n\n\u6700\u540e\uff0c\u5c06\u5217\u8868\u8f6c\u6362\u4e3a JSON \u5e76\u6253\u5370\u3002\n\n\u6211\u9700\u8981\u786e\u4fdd\u4ee3\u7801\u662f\u9ad8\u6548\u7684\uff0c\u4f46\u56e0\u4e3a Wikipedia \u9875\u9762\u901a\u5e38\u4e0d\u4f1a\u592a\u5927\uff0cBeautiful Soup \u5e94\u8be5\u80fd\u591f handle \u5f88\u597d\u3002\n\n\u6211\u8fd8\u9700\u8981\u5904\u7406\u53ef\u80fd\u7684\u7f16\u7801\u95ee\u9898\uff0c\u786e\u4fdd\u7279\u6b8a\u5b57\u7b26\u88ab\u6b63\u786e\u89e3\u7801\u3002\n\n\u6b64\u5916\uff0c\u6211\u5e94\u8be5\u8003\u8651robots.txt\u548cWikipedia\u7684\u722c\u866b\u653f\u7b56\u3002\n\n\u867d\u7136Wikipedia\u5141\u8bb8\u722c\u866b\uff0c\u4f46\u6211\u5e94\u8be5\u5728\u4ee3\u7801\u4e2d\u5305\u542b\u9002\u5f53\u7684\u7528\u6237\u4ee3\u7406\uff0c\u5e76\u4e14\u5c0a\u91cd\u4ed6\u4eec\u7684\u722c\u866b\u5ef6\u8fdf\u6307\u5bfc\u3002\n\n\u4f46\u6211\u5728\u8fd9\u4e2a\u811a\u672c\u4e2d\u4e0d\u9700\u8981 explicit \u5730\u5904\u7406robots.txt\uff0c\u56e0\u4e3arequests\u4e0d\u4f1a\u81ea\u52a8\u9075\u5b88\u5b83\u3002\n\n\u6700\u540e\uff0c\u6211\u5e94\u8be5\u6dfb\u52a0\u6ce8\u91ca\uff0c\u4f7f\u4ee3\u7801\u6613\u4e8e\u7406\u89e3\u3002\n\n\u8ba9\u6211\u5f00\u59cb\u7f16\u5199\u4ee3\u7801\u3002\n\n\u9996\u5148\uff0c\u5bfc\u5165\u6a21\u5757\uff1a\n\n```python\n\nimport sys\n\nimport json\n\nimport requests\n\nfrom urllib.parse import urljoin, urlparse, unquote\n\nfrom bs4 import BeautifulSoup\n\n```\n\n\u7136\u540e\uff0c\u68c0\u67e5\u547d\u4ee4\u884c\u53c2\u6570\uff1a\n\n```python\n\nif len(sys.argv) != 2:\n\n    print(\"Usage: python script.py <wikipedia_url>\")\n\n    sys.exit(1)\n\nurl = sys.argv[1]\n\n```\n\n\u9a8c\u8bc1 URL\uff1a\n\n```python\n\nif not url.startswith('https://en.wikipedia.org/wiki/'):\n\n    print(\"Error: Please provide a valid Wikipedia page URL.\")\n\n    sys.exit(1)\n\n```\n\n\u83b7\u53d6\u9875\u9762\u5185\u5bb9\uff1a\n\n```python\n\ntry:\n\n    response = requests.get(url)\n\n    response.raise_for_status()\n\nexcept requests.RequestException as e:\n\n    print(f\"Error fetching the URL: {e}\")\n\n    sys.exit(1)\n\n```\n\n\u89e3\u6790\u5185\u5bb9\uff1a\n\n```python\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n```\n\n\u627e\u5230\u6240\u6709 `<a>` \u6807\u7b7e\uff1a\n\n```python\n\ninternal_links = []\n\nexcluded_substrings = [\n\n    'Special:', 'Category:', 'User:', 'File:', 'Help:', 'Template:',\n\n    'Wikipedia:', 'Portal:', 'Book:', 'Draft:', 'Education Program:',\n\n    'TimedText:', 'Module:', 'MediaWiki:',\n\n    'Category_talk:', 'Template_talk:', 'Help_talk:', 'File_talk:',\n\n    'MediaWiki_talk:', 'Template_talk:', 'User_talk:', 'Wikipedia_talk:',\n\n    'Book_talk:', 'Draft_talk:', 'Education_Program_talk:', 'File_talk:',\n\n    'Help_talk:', 'Module_talk:', 'Talk:', 'TimedText_talk:', 'User_talk:',\n\n    'Wikipedia_talk:'\n\n]\n\nfor a in soup.find_all('a', href=True):\n\n    href = a['href']\n\n    if href.startswith('#'):\n\n        continue  # Skip anchors within the same page\n\n    absolute_url = urljoin(url, href)\n\n    parsed_url = urlparse(absolute_url)\n\n    path = parsed_url.path\n\n    if path.startswith('/wiki/'):\n\n        if any(substr in path for substr in excluded_substrings):\n\n            continue\n\n        title = path.split('/')[-1]\n\n        decoded_title = unquote(title)\n\n        internal_links.append(decoded_title)\n\n```\n\n\u8f93\u51fa JSON\uff1a\n\n```python\n\nprint(json.dumps(internal_links, indent=4))\n\n```\n\n\u73b0\u5728\uff0c\u6211\u9700\u8981\u8003\u8651\u662f\u5426\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u6216\u6dfb\u52a0\u9519\u8bef\u5904\u7406\u3002\n\n\u4f8b\u5982\uff0c\u5982\u679c\u9875\u9762\u4e0d\u5b58\u5728\uff0crequests.get\u4f1a\u8fd4\u56de404\uff0c\u6211\u5df2\u7ecf\u6709\u4ee3\u7801\u6765\u5904\u7406status_code\u4e0d\u662f200\u7684\u60c5\u51b5\u3002\n\n\u53e6\u5916\uff0c\u5982\u679cURL\u65e0\u6548\uff0curlparse\u4f1a\u5f15\u53d1\u5f02\u5e38\uff0c\u4f46\u6211\u5df2\u7ecf\u4f7f\u7528\u4e86try-except\u6765\u6355\u83b7\u8bf7\u6c42\u5f02\u5e38\u3002\n\n\u6211\u8fd8\u5e94\u8be5\u8003\u8651\u9875\u9762\u52a0\u8f7d\u5b8c\u6210\u540e\uff0cBeautiful Soup\u89e3\u6790\u53ef\u80fd\u5931\u8d25\uff0c\u4f46Beautiful Soup\u901a\u5e38\u5f88 robust\u3002\n\n\u518d\u8005\uff0c\u6709\u4e9b\u9875\u9762\u53ef\u80fd\u6709\u52a8\u6001\u5185\u5bb9\uff0c\u4f46Wikipedia\u9875\u9762\u662f\u9759\u6001\u7684\uff0c\u6240\u4ee5\u8fd9\u4e0d\u9002\u7528\u3002\n\n\u6700\u540e\uff0c\u6211\u5e94\u8be5\u6d4b\u8bd5\u8fd9\u4e2a\u811a\u672c\uff0c\u770b\u770b\u662f\u5426\u6309\u9884\u671f\u5de5\u4f5c\u3002\n\n\u6211\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2aknown\u7684Wikipedia\u9875\u9762\uff0c\u6bd4\u5982'https://en.wikipedia.org/wiki/Web_scraping'\uff0c\u5e76\u68c0\u67e5\u8f93\u51fa\u7684JSON\u662f\u5426\u5305\u542b\u9884\u671f\u7684\u5185\u90e8\u94fe\u63a5\u3002\n\n\u6211\u53ef\u4ee5\u624b\u52a8\u9a8c\u8bc1\u4e00\u4e9b\u94fe\u63a5\uff0c\u786e\u4fdd\u5b83\u4eec\u662f\u6709\u6548\u7684Wikipedia\u9875\u9762\u3002\n\n\u6b64\u5916\uff0c\u6211\u5e94\u8be5\u68c0\u67e5\u662f\u5426\u6392\u9664\u4e86\u7279\u6b8a\u547d\u540d\u7a7a\u95f4\u7684\u9875\u9762\uff0c\u5982Category:\u3001User:\u7b49\u3002\n\n\u603b\u4e4b\uff0c\u8fd9\u4e2a\u811a\u672c\u5e94\u8be5\u80fd\u591f\u5b8c\u6210\u4efb\u52a1\u3002\n\n## \u6700\u7ec8\u89e3\u51b3\u65b9\u6848\n\u4e3a\u4e86\u4ece\u7ed9\u5b9a\u7684\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u522e\u53d6\u6240\u6709\u5185\u90e8\u94fe\u63a5\uff0c\u5e76\u4ee5JSON\u683c\u5f0f\u8f93\u51fa\uff0c\u53ef\u4ee5\u4f7f\u7528Python\u7684Beautiful Soup\u5e93\u3002\u6b64\u811a\u672c\u5c06\u63a5\u53d7\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684URL\u4f5c\u4e3a\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u5e76\u63d0\u53d6\u6240\u6709\u6307\u5411\u5176\u4ed6\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684\u94fe\u63a5\u3002\n\n### \u5b9e\u73b0\u601d\u8def\n\n1. **\u547d\u4ee4\u884c\u53c2\u6570\u68c0\u67e5**\uff1a\u786e\u4fdd\u811a\u672c\u63a5\u6536\u5230\u4e14\u4ec5\u63a5\u6536\u5230\u4e00\u4e2aURL\u53c2\u6570\u3002\n2. **URL\u9a8c\u8bc1**\uff1a\u786e\u8ba4\u63d0\u4f9b\u7684URL\u662f\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684URL\u3002\n3. **\u9875\u9762\u5185\u5bb9\u83b7\u53d6**\uff1a\u4f7f\u7528`requests`\u5e93\u83b7\u53d6\u9875\u9762HTML\u5185\u5bb9\u3002\n4. **HTML\u89e3\u6790**\uff1a\u4f7f\u7528Beautiful Soup\u89e3\u6790HTML\u5185\u5bb9\u3002\n5. **\u94fe\u63a5\u63d0\u53d6**\uff1a\u67e5\u627e\u6240\u6709`<a>`\u6807\u7b7e\uff0c\u5e76\u63d0\u53d6\u5176`href`\u5c5e\u6027\u3002\n6. **\u94fe\u63a5\u8fc7\u6ee4**\uff1a\u7b5b\u9009\u51fa\u6307\u5411\u5176\u4ed6\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684\u5185\u90e8\u94fe\u63a5\uff0c\u6392\u9664\u7279\u6b8a\u9875\u9762\u548c\u5916\u90e8\u94fe\u63a5\u3002\n7. **JSON\u8f93\u51fa**\uff1a\u5c06\u63d0\u53d6\u7684\u9875\u9762\u6807\u9898\u5217\u8868\u8f6c\u6362\u4e3aJSON\u683c\u5f0f\u5e76\u6253\u5370\u3002\n\n### \u89e3\u51b3\u65b9\u6848\u4ee3\u7801\n\n```python\nimport sys\nimport json\nimport requests\nfrom urllib.parse import urljoin, urlparse, unquote\nfrom bs4 import BeautifulSoup\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python script.py <wikipedia_url>\")\n        sys.exit(1)\n    \n    url = sys.argv[1]\n    if not url.startswith('https://en.wikipedia.org/wiki/'):\n        print(\"Error: Please provide a valid Wikipedia page URL.\")\n        sys.exit(1)\n    \n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        sys.exit(1)\n    \n    soup = BeautifulSoup(response.content, 'html.parser')\n    internal_links = []\n    excluded_substrings = [\n        'Special:', 'Category:', 'User:', 'File:', 'Help:', 'Template:',\n        'Wikipedia:', 'Portal:', 'Book:', 'Draft:', 'Education Program:',\n        'TimedText:', 'Module:', 'MediaWiki:',\n        'Category_talk:', 'Template_talk:', 'Help_talk:', 'File_talk:',\n        'MediaWiki_talk:', 'Template_talk:', 'User_talk:', 'Wikipedia_talk:',\n        'Book_talk:', 'Draft_talk:', 'Education_Program_talk:', 'File_talk:',\n        'Help_talk:', 'Module_talk:', 'Talk:', 'TimedText_talk:', 'User_talk:',\n        'Wikipedia_talk:'\n    ]\n    \n    for a in soup.find_all('a', href=True):\n        href = a['href']\n        if href.startswith('#'):\n            continue  # Skip anchors within the same page\n        absolute_url = urljoin(url, href)\n        parsed_url = urlparse(absolute_url)\n        path = parsed_url.path\n        if path.startswith('/wiki/'):\n            if any(substr in path for substr in excluded_substrings):\n                continue\n            title = path.split('/')[-1]\n            decoded_title = unquote(title)\n            internal_links.append(decoded_title)\n    \n    print(json.dumps(internal_links, indent=4))\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### \u8bf4\u660e\n\n1. **\u547d\u4ee4\u884c\u53c2\u6570\u68c0\u67e5**\uff1a\u811a\u672c\u9996\u5148\u68c0\u67e5\u662f\u5426\u63d0\u4f9b\u4e86\u6b63\u786e\u7684\u547d\u4ee4\u884c\u53c2\u6570\u3002\u5982\u679c\u6ca1\u6709\u63d0\u4f9bURL\u6216\u63d0\u4f9b\u4e86\u591a\u4e2aURL\uff0c\u811a\u672c\u5c06\u663e\u793a\u7528\u6cd5\u5e76\u9000\u51fa\u3002\n2. **URL\u9a8c\u8bc1**\uff1a\u786e\u8ba4\u63d0\u4f9b\u7684URL\u662f\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684URL\uff0c\u901a\u8fc7\u68c0\u67e5\u662f\u5426\u4ee5`https://en.wikipedia.org/wiki/`\u5f00\u5934\u3002\n3. **\u9875\u9762\u5185\u5bb9\u83b7\u53d6**\uff1a\u4f7f\u7528`requests.get`\u83b7\u53d6\u9875\u9762\u5185\u5bb9\uff0c\u5e76\u5904\u7406\u53ef\u80fd\u7684\u8bf7\u6c42\u5f02\u5e38\u3002\n4. **HTML\u89e3\u6790**\uff1a\u4f7f\u7528Beautiful Soup\u89e3\u6790\u83b7\u53d6\u7684HTML\u5185\u5bb9\u3002\n5. **\u94fe\u63a5\u63d0\u53d6\u4e0e\u8fc7\u6ee4**\uff1a\u904d\u5386\u6240\u6709`<a>`\u6807\u7b7e\uff0c\u63d0\u53d6`href`\u5c5e\u6027\uff0c\u5e76\u4f7f\u7528`urljoin`\u6784\u5efa\u7edd\u5bf9URL\u3002\u901a\u8fc7\u68c0\u67e5\u8def\u5f84\u662f\u5426\u4ee5`/wiki/`\u5f00\u5934\u548c\u4e0d\u5305\u542b\u6392\u9664\u7684\u5b50\u5b57\u7b26\u4e32\u6765\u7b5b\u9009\u5185\u90e8\u94fe\u63a5\u3002\n6. **JSON\u8f93\u51fa**\uff1a\u5c06\u63d0\u53d6\u7684\u9875\u9762\u6807\u9898\u5217\u8868\u8f6c\u6362\u4e3aJSON\u683c\u5f0f\u5e76\u6253\u5370\u3002\n\n\u6b64\u811a\u672c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u6765\u522e\u53d6\u7ef4\u57fa\u767e\u79d1\u9875\u9762\u7684\u5185\u90e8\u94fe\u63a5\uff0c\u5e76\u4ee5\u7ed3\u6784\u5316\u7684JSON\u683c\u5f0f\u8f93\u51fa\u7ed3\u679c\u3002"
    }
]