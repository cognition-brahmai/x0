[
    {
        "role": "user",
        "content": "Write a bash script that takes a CSV file as input and outputs the total number of rows, columns, and unique values in each column. The script should support command line arguments for specifying the input file and delimiter used in the CSV."
    },
    {
        "role": "assistant",
        "content": "Alright, I'm going to write a bash script that does what you've asked. So, the script needs to take a CSV file as input and output the total number of rows, columns, and unique values in each column. It should also support command line arguments for specifying the input file and the delimiter used in the CSV.\n\nFirst, I need to understand what a CSV file is. CSV stands for Comma-Separated Values, which is a simple file format used to store tabular data, where each line represents a row and each field is separated by a comma or another delimiter.\n\nBut in this case, the user wants to specify the delimiter, so the script needs to accept that as an argument. Also, the user should be able to specify the input file.\n\nSo, I need to handle command-line arguments in bash. I recall that in bash, command-line arguments are accessed using $1, $2, etc., where $1 is the first argument, $2 is the second, and so on.\n\nI should also consider using getopt or getopts for parsing options, but for simplicity, I'll assume that the user will provide the input file and delimiter in a specific order.\n\nLet's say the script is called analyze_csv.sh, and it's invoked like this:\n\nbash analyze_csv.sh input.csv ','\n\nHere, input.csv is the file, and ',' is the delimiter.\n\nFirst, I need to check if the correct number of arguments is provided. If not, I should display an error message and exit.\n\nI can do that by checking the number of arguments using $#.\n\nIf $# is not equal to 2, display an error message and exit.\n\nNext, I need to read the CSV file and process it.\n\nTo count the total number of rows, I can use the wc -l command on the file.\n\nBut wait, the first row might be headers, and depending on whether to include it or not, but for total rows, I'll include it.\n\nSo, total rows = wc -l on the file.\n\nTo count the number of columns, I can take the first line (header) and count the number of fields separated by the delimiter.\n\nSo, columns = head -n 1 input.csv | tr ',' '\\n' | wc -l\n\nAssuming ',' is the delimiter.\n\nBut I need to make it dynamic based on the provided delimiter.\n\nSo, something like:\n\ncolumns=$(head -n 1 \"$input_file\" | tr \"$delimiter\" '\\n' | wc -l)\n\nSimilarly, to count unique values in each column, I need to process each column separately.\n\nI can loop through each column number and extract that column using cut or awk, and then use sort -u | wc -l to count unique values.\n\nLet me think about this.\n\nFirst, get the number of columns.\n\nThen, loop from 1 to the number of columns.\n\nFor each column, extract that column using cut or awk, and then sort -u | wc -l.\n\nBut cut uses fields based on delimiters, and awk can handle delimiters as well.\n\nI think awk might be more suitable here because it can handle different delimiters easily.\n\nSo, something like:\n\nfor i in $(seq 1 $columns); do\n\nunique_counts[$i]=$(awk -F\"$delimiter\" '{print $i}' \"$input_file\" | sort -u | wc -l)\n\ndone\n\nThen, print the results.\n\nWait, but arrays in bash are a bit tricky.\n\nI need to declare an array to hold the unique counts for each column.\n\nAlternatively, I can just print them one by one.\n\nSo, maybe:\n\nfor i in $(seq 1 $columns); do\n\necho \"Column $i unique values: $(awk -F\"$delimiter\" '{print $i}' \"$input_file\" | sort -u | wc -l)\"\n\ndone\n\nBut this might be inefficient because awk is called for each column, which means reading the file multiple times.\n\nIs there a way to read the file once and process all columns at once?\n\nYes, awk is very powerful for this.\n\nI can write an awk script that processes the entire file and prints the unique counts for each column.\n\nSo, perhaps something like:\n\nawk -F\"$delimiter\" '\n\n{ for (i=1; i<=NF; i++) { values[i][$i] = 1 } }\n\nEND {\n\nfor (i=1; i<=NF; i++) {\n\nn=asorti(values[i], tmp)\n\nprint \"Column \" i \" unique values: \" n\n\n}\n\n}\n\n' \"$input_file\"\n\nThis way, awk reads the file once and collects unique values for each column using an associative array.\n\nThen, at the end, it sorts and counts the unique values for each column.\n\nThis seems more efficient.\n\nSo, I'll go with this approach.\n\nAlso, I need to output the total number of rows and columns.\n\nSo, first, print the total rows and columns, and then the unique counts for each column.\n\nI can get the total rows with wc -l.\n\nAnd columns with counting the number of fields in the first line.\n\nSo, in the script:\n\n# Get total rows\n\ntotal_rows=$(wc -l < \"$input_file\")\n\n# Get total columns\n\ntotal_columns=$(head -n 1 \"$input_file\" | tr \"$delimiter\" '\\n' | wc -l)\n\n# Get unique counts for each column using awk\n\necho \"Total rows: $total_rows\"\n\necho \"Total columns: $total_columns\"\n\necho \"Unique values per column:\"\n\nawk -F\"$delimiter\" '\n\n{ for (i=1; i<=NF; i++) { values[i][$i] = 1 } }\n\nEND {\n\nfor (i=1; i<=NF; i++) {\n\nn=asorti(values[i], tmp)\n\nprint \"Column \" i \" unique values: \" n\n\n}\n\n}\n\n' \"$input_file\"\n\nThis should work.\n\nBut I need to make sure that the delimiter is handled correctly.\n\nAlso, need to handle cases where the delimiter is a special character.\n\nMaybe I should quote it properly.\n\nAlso, consider that the delimiter could be a space or tab, so need to handle that.\n\nAdditionally, need to handle files with no header, files with empty lines, etc.\n\nAlso, need to handle cases where columns have missing values.\n\nBut for simplicity, I'll assume that the CSV is well-formed.\n\nI should also add some error checking, like checking if the input file exists.\n\nSo, at the beginning of the script:\n\nif [ ! -f \"$input_file\" ]; then\n\necho \"Error: Input file not found.\"\n\nexit 1\n\nfi\n\nAlso, maybe check if the delimiter is provided.\n\nAlright, let's structure the script.\n\nFirst, check the number of arguments.\n\nif [ $# -ne 2 ]; then\n\necho \"Usage: $0 input_file delimiter\"\n\nexit 1\n\nfi\n\nThen, assign the arguments to variables.\n\ninput_file=$1\n\ndelimiter=$2\n\nThen, check if the file exists.\n\nif [ ! -f \"$input_file\" ]; then\n\necho \"Error: Input file not found.\"\n\nexit 1\n\nfi\n\nThen, get total rows.\n\ntotal_rows=$(wc -l < \"$input_file\")\n\nGet total columns.\n\ntotal_columns=$(head -n 1 \"$input_file\" | tr \"$delimiter\" '\\n' | wc -l\")\n\nThen, print the results.\n\necho \"Total rows: $total_rows\"\n\necho \"Total columns: $total_columns\"\n\nThen, get unique counts for each column using awk.\n\necho \"Unique values per column:\"\n\nawk -v FS=\"$delimiter\" '\n\n{ for (i=1; i<=NF; i++) { values[i][$i] = 1 } }\n\nEND {\n\nfor (i=1; i<=NF; i++) {\n\nn=asorti(values[i], tmp)\n\nprint \"Column \" i \" unique values: \" n\n\n}\n\n}\n\n' \"$input_file\"\n\nI think this should work.\n\nLet me test this with a sample CSV.\n\nSuppose I have a file called data.csv:\n\nName,Age,City\n\nAlice,30,New York\n\nBob,25,California\n\nCharlie,35,New York\n\nTotal rows: 3\n\nTotal columns: 3\n\nUnique values per column:\n\nColumn 1 unique values: 3 (Alice, Bob, Charlie)\n\nColumn 2 unique values: 3 (30, 25, 35)\n\nColumn 3 unique values: 2 (New York, California)\n\nYes, that makes sense.\n\nAnother thing to consider is that the first line might be headers, but for counting unique values, it's included as part of the data.\n\nIf needed, the user can exclude the header separately, but for now, I'll include it.\n\nAlso, need to handle cases where there are empty fields or missing values.\n\nIn such cases, empty strings will be considered as unique values.\n\nSo, if a column has some empty fields, they will be counted as one unique value.\n\nThat should be fine, as it's reflecting the data accurately.\n\nAlright, I think this script should meet the requirements.\n\n## Final Solution\nTo create a bash script that analyzes a CSV file and provides the total number of rows, columns, and unique values in each column, follow these steps:\n\n### Approach\n1. **Input Handling**: Accept the input file and delimiter via command-line arguments.\n2. **Validation**: Check if the correct number of arguments is provided and if the input file exists.\n3. **Count Rows and Columns**:\n   - Use `wc -l` to count the total number of rows.\n   - Use `head -n 1` combined with `tr` to count the columns based on the delimiter.\n4. **Unique Values per Column**: Use `awk` to process the file, collecting unique values for each column and then sorting and counting them.\n\n### Solution Code\n\n```bash\n#!/bin/bash\n\n# Check for correct number of arguments\nif [ $# -ne 2 ]; then\n    echo \"Usage: $0 input_file delimiter\"\n    exit 1\nfi\n\ninput_file=$1\ndelimiter=$2\n\n# Check if input file exists\nif [ ! -f \"$input_file\" ]; then\n    echo \"Error: Input file not found.\"\n    exit 1\nfi\n\n# Get total number of rows\ntotal_rows=$(wc -l < \"$input_file\")\n\n# Get total number of columns\ntotal_columns=$(head -n 1 \"$input_file\" | tr \"$delimiter\" '\\n' | wc -l)\n\n# Output the results\necho \"Total rows: $total_rows\"\necho \"Total columns: $total_columns\"\necho \"Unique values per column:\"\n\n# Get unique counts for each column using awk\nawk -v FS=\"$delimiter\" '\n{\n    for (i=1; i<=NF; i++) {\n        values[i][$i] = 1\n    }\n}\nEND {\n    for (i=1; i<=NF; i++) {\n        n=asorti(values[i], tmp)\n        print \"Column \" i \" unique values: \" n\n    }\n}\n' \"$input_file\"\n```\n\n### Explanation\n1. **Argument Handling and Validation**:\n   - The script checks if exactly two arguments are provided (input file and delimiter). If not, it prints usage information and exits.\n   - It verifies if the input file exists to avoid processing non-existent files.\n\n2. **Counting Rows and Columns**:\n   - `wc -l` is used to count the total number of lines (rows) in the file.\n   - The first line of the file is extracted using `head -n 1`, and the number of columns is determined by replacing the delimiter with newlines and counting the lines using `tr` and `wc -l`.\n\n3. **Calculating Unique Values**:\n   - `awk` is employed to process the entire file in a single pass.\n   - An associative array (`values`) is used to store unique values for each column.\n   - In the `END` block, the unique values for each column are sorted and counted using `asorti`, and the results are printed.\n\nThis script provides a efficient and straightforward way to analyze CSV files, making it suitable for various data inspection tasks."
    }
]