[
    {
        "role": "user",
        "content": "Develop a Python script that interacts with TensorFlow to train a neural network on the MNIST dataset. The script should include options to configure the number of hidden layers, neurons per layer, and activation functions."
    },
    {
        "role": "assistant",
        "content": "I'm here to help you with your Python script that uses TensorFlow to train a neural network on the MNIST dataset. You want the script to be configurable, allowing users to set the number of hidden layers, the number of neurons in each layer, and the activation functions. That sounds like a great project!\n\nFirst, let's make sure we have all the necessary components. You'll need TensorFlow installed in your Python environment. If you haven't installed it yet, you can do so using pip:\n\n```bash\npip install tensorflow\n```\n\nOnce TensorFlow is installed, we can proceed to write the script. We'll break this down into several steps:\n\n1. **Loading the MNIST dataset**\n2. **Building the neural network model with configurable parameters**\n3. **Compiling the model**\n4. **Training the model**\n5. **Evaluating the model**\n\nLet's start by loading the MNIST dataset. TensorFlow has a built-in function to load this dataset, which is very convenient.\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize the images to [0,1]\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Convert labels to one-hot encoding\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n```\n\nNext, we need to build the neural network model. We want this model to be configurable, meaning that the user can specify the number of hidden layers, the number of neurons in each layer, and the activation functions.\n\nTo achieve this, we can create a function that takes these parameters as inputs and constructs the model accordingly.\n\n```python\ndef create_model(num_hidden_layers, num_neurons, activation_func):\n    model = Sequential()\n    model.add(Flatten(input_shape=(28, 28)))  # Flatten the images to a 1D array\n    \n    for _ in range(num_hidden_layers):\n        model.add(Dense(num_neurons, activation=activation_func))\n    \n    model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n    return model\n```\n\nIn this function, `num_hidden_layers` specifies how many hidden layers to add, `num_neurons` specifies the number of neurons in each hidden layer, and `activation_func` specifies the activation function to use in the hidden layers.\n\nNow, let's compile the model. We'll use the Adam optimizer and categorical crossentropy as the loss function, which are common choices for classification problems.\n\n```python\nmodel = create_model(num_hidden_layers=2, num_neurons=128, activation_func='relu')\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```\n\nIn this example, I've set `num_hidden_layers=2`, `num_neurons=128`, and `activation_func='relu'`. You can adjust these parameters as needed.\n\nNext, we'll train the model using the training data.\n\n```python\nmodel.fit(x_train, y_train, epochs=5, validation_split=0.2)\n```\n\nHere, I've set the number of epochs to 5 and used a validation split of 0.2, meaning that 20% of the training data will be used for validation.\n\nFinally, we'll evaluate the model on the test data to see how well it performs.\n\n```python\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc}')\n```\n\nThis will output the test accuracy of the model.\n\nPutting it all together, here's the complete script:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize the images to [0,1]\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Convert labels to one-hot encoding\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\ndef create_model(num_hidden_layers, num_neurons, activation_func):\n    model = Sequential()\n    model.add(Flatten(input_shape=(28, 28)))  # Flatten the images to a 1D array\n    \n    for _ in range(num_hidden_layers):\n        model.add(Dense(num_neurons, activation=activation_func))\n    \n    model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n    return model\n\n# Create and compile the model\nmodel = create_model(num_hidden_layers=2, num_neurons=128, activation_func='relu')\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=5, validation_split=0.2)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc}')\n```\n\nYou can adjust the `num_hidden_layers`, `num_neurons`, and `activation_func` parameters in the `create_model` function call to experiment with different architectures.\n\nThis script provides a basic framework for training a neural network on the MNIST dataset with configurable parameters. You can further enhance it by adding more features, such as different types of layers, regularization techniques, or more advanced optimization algorithms.\n\nIf you have any specific requirements or need further customization, feel free to ask!"
    }
]