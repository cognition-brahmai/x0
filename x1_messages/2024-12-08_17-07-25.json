[
    {
        "role": "user",
        "content": "Implement a Golang command-line tool that takes a CSV file as input, processes it, and inserts the data into an Elasticsearch index. The tool should also provide options to specify the delimiter used in the CSV file and the name of the Elasticsearch index.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I'm going to try to build a command-line tool in Golang that can take a CSV file, process it, and then insert the data into an Elasticsearch index. Also, I need to make sure that the user can specify the delimiter used in the CSV and the name of the Elasticsearch index.\n\nFirst things first, I need to set up my Go environment. I should make sure that I have Go installed on my machine. I can check that by running `go version` in the terminal. If it's not installed, I'll need to download and install it from golang.org.\n\nNext, I need to create a new Go project. I'll make a directory for my project and then initialize it using `go mod init`. For example, I can name my module \"csv-to-es\".\n\nNow, I need to think about the structure of my tool. It should be a command-line tool, so I'll need to use the \"flag\" package in Go to handle command-line arguments. The user should be able to specify the path to the CSV file, the delimiter used in the CSV, and the name of the Elasticsearch index.\n\nLet me list out the command-line flags I'll need:\n\n- `csv-file`: path to the CSV file\n\n- `delimiter`: the delimiter used in the CSV file (default could be comma)\n\n- `index-name`: name of the Elasticsearch index\n\nI should also consider adding some help text for users who are not familiar with the tool.\n\nNow, I need to handle reading the CSV file. For that, I can use the \"encoding/csv\" package in Go. I'll open the file using `os.Open`, read it using a csv.Reader, and process each row.\n\nBut before that, I need to parse the command-line flags to get the CSV file path, delimiter, and index name.\n\nOnce I have the CSV data, I need to insert it into Elasticsearch. For interacting with Elasticsearch, I can use the official Elasticsearch Go client. I'll need to add it as a dependency using `go get`.\n\nSo, I'll run:\n\n```\n\ngo get github.com/elastic/go-elasticsearch/v8\n\n```\n\nThen, I can import it in my code:\n\n```\n\nes \"github.com/elastic/go-elasticsearch/v8\"\n\n```\n\nI need to create an Elasticsearch client. The client can be configured to connect to the Elasticsearch instance. Assuming that Elasticsearch is running locally on the default port, I can use the default settings.\n\nNow, for each row in the CSV, I need to index it into Elasticsearch. But first, I need to decide on the document type and ID. Maybe I can use a unique field from the CSV as the document ID, or just let Elasticsearch generate one.\n\nI should ask the user to specify which column to use as the document ID, or just use the default behavior of letting Elasticsearch generate IDs.\n\nWait, maybe I should add another flag for specifying the ID column.\n\nSo, an additional flag:\n\n- `id-column`: the column name to use as the document ID\n\nIf the user doesn't specify an ID column, then Elasticsearch can generate the IDs automatically.\n\nNow, when processing the CSV, I need to read the header row to get the column names, and then map each row to a JSON document to index in Elasticsearch.\n\nI need to be careful with data types. Elasticsearch has specific mapping types, so I should make sure that the data is formatted correctly.\n\nMaybe I should first create the index with the appropriate mapping based on the CSV headers and data types.\n\nWait, that could be complicated. Maybe for simplicity, I'll assume that all fields are strings, and let Elasticsearch infer the types.\n\nAlternatively, I can try to detect the data types from the CSV data.\n\nBut that might be too complex for now. I'll assume all fields are strings.\n\nSo, the steps are:\n\n1. Parse command-line flags: csv-file, delimiter, index-name, id-column\n\n2. Read the CSV file using the specified delimiter\n\n3. Extract the header row to get column names\n\n4. For each data row:\n\na. Create a JSON object mapping column names to values\n\nb. Index the document into Elasticsearch, using the specified ID column if provided\n\n5. Provide feedback to the user about the number of documents indexed, any errors, etc.\n\nI should also handle errors gracefully, like if the CSV file doesn't exist, or if there's an issue connecting to Elasticsearch.\n\nLet me think about how to structure the code. I'll have a main function that parses the flags, reads the CSV, and indexes the data.\n\nI can create separate functions for reading the CSV and indexing the data to keep the code organized.\n\nAlso, I should consider batch indexing for better performance, instead of indexing one document at a time.\n\nThe Elasticsearch client has a Bulk API that can be used for this purpose.\n\nSo, I'll collect a batch of documents and use the Bulk API to index them in one request.\n\nI need to decide on the batch size. Maybe 100 documents per batch.\n\nNow, let's think about the JSON document structure. Each document will have fields corresponding to the CSV columns.\n\nIf the user specifies an ID column, I'll use that as the document ID; otherwise, Elasticsearch will generate one.\n\nIn the Bulk API, I need to specify the index name and the document body for each action.\n\nI should also handle any errors returned by the Bulk API.\n\nAlright, I think I have a plan. Let me start writing some pseudocode to outline the steps.\n\nPseudocode:\n\n```\n\nparse command-line flags: csv-file, delimiter, index-name, id-column\n\nopen csv-file\n\nread CSV with specified delimiter\n\nread header row to get column names\n\ncreate Elasticsearch client\n\nif index doesn't exist, create it\n\ninitialize batch list\n\nfor each data row:\n\ncreate JSON document with column names and values\n\nif id-column is specified, use that as document ID\n\nadd index action and document to batch\n\nif batch size reaches 100, execute bulk request and reset batch\n\nafter processing all rows, execute any remaining bulk request\n\nprint summary: number of documents indexed, any errors\n\n```\n\nNow, I need to handle cases where the CSV file doesn't have a header row. Maybe I should add an option for that.\n\nAnother flag:\n\n- `header-row`: whether the CSV file has a header row (default true)\n\nIf there's no header row, then the user needs to specify the column names somehow. Maybe they can provide a comma-separated list of column names via another flag.\n\nSo, an additional flag:\n\n- `columns`: comma-separated list of column names (to be used if no header row)\n\nThen, depending on the value of `header-row`, I'll either read the header row or use the provided column names.\n\nI need to make sure that if `header-row` is false, then `columns` must be provided.\n\nNow, let's think about error handling. For example:\n\n- CSV file doesn't exist or can't be read\n\n- Elasticsearch is not running or not accessible\n\n- Invalid delimiter specified\n\n- Missing required flags\n\nI should provide meaningful error messages for these cases.\n\nAlso, I should consider adding a verbose flag to output more detailed information during processing.\n\nAnother flag:\n\n- `verbose`: provide detailed output during processing\n\nNow, let's think about how to create the Elasticsearch index. I need to create it if it doesn't exist.\n\nI can use the Elasticsearch Indices API to check if the index exists.\n\nIf it doesn't, I can create it with default settings.\n\nSince I'm assuming all fields are strings, I don't need a specific mapping.\n\nBut in reality, Elasticsearch will infer the types based on the indexed data.\n\nAlright, moving on to writing some actual code.\n\nFirst, I'll set up the main function and parse the command-line flags.\n\nI'll use the \"flag\" package for this.\n\nDefine flags:\n\n- csvFile: string, required\n\n- delimiter: string, default \",\"\n\n- indexName: string, required\n\n- idColumn: string, optional\n\n- headerRow: boolean, default true\n\n- columns: string, optional (comma-separated list of column names)\n\n- verbose: boolean, default false\n\nI need to validate the flags:\n\n- csvFile and indexName are required\n\n- if headerRow is false, then columns must be provided\n\n- idColumn must be one of the column names\n\nI'll need to implement this validation.\n\nNow, reading the CSV file.\n\nOpen the file using os.Open, create a csv.Reader with the specified delimiter.\n\nIf headerRow is true, read the header row using reader.Header().\n\nOtherwise, split the columns flag value into a slice of column names.\n\nThen, read all data rows using reader.ReadAll().\n\nNow, create the Elasticsearch client.\n\nUse the default configuration to connect to localhost:9200.\n\nCheck if the index exists using the Indices API.\n\nIf not, create it.\n\nThen, process the data rows in batches.\n\nFor each batch, build a list of bulk actions.\n\nEach action is a map with \"index\" as the operation and the document ID if provided.\n\nThen, the document body is a map of column names to values.\n\nConvert these maps to JSON and add them to the bulk request.\n\nExecute the bulk request and handle any errors.\n\nAfter processing all batches, print a summary.\n\nNow, let's think about how to handle the ID column.\n\nIf idColumn is specified, find its index in the column names.\n\nFor each row, use that column's value as the document ID.\n\nOtherwise, let Elasticsearch generate the ID.\n\nIn the bulk request, specify the \"id\" field in the index action if an ID is provided.\n\nNow, implement the function to execute the bulk request.\n\nUse the Elasticsearch Bulk API.\n\nPass the bulk request as a bytes.Reader.\n\nHandle the response and check for errors.\n\nIf there are errors, print them out.\n\nNow, consider handling retries for failed bulk requests.\n\nBut for simplicity, I'll assume that failed documents are reported, and the user can retry indexing them manually.\n\nNext, think about how to handle different data types.\n\nAs mentioned earlier, assume all fields are strings.\n\nBut if I want to handle different types, I can try to detect the type based on the column name or value.\n\nFor example, if the column name ends with \"_id\", assume it's a keyword.\n\nBut that might be too complex for now.\n\nStick with strings for all fields.\n\nNow, consider adding a option to specify the Elasticsearch URL if it's not running locally.\n\nAnother flag:\n\n- `es-url`: URL of the Elasticsearch instance (default \"http://localhost:9200\")\n\nThen, configure the Elasticsearch client with this URL.\n\nNow, think about how to handle large CSV files.\n\nReading the entire CSV into memory might not be feasible.\n\nSo, instead of reading all rows at once, process the CSV row by row and build batches incrementally.\n\nSo, read one row, process it, add it to the batch, and when the batch is full, execute the bulk request.\n\nThis way, the tool can handle large CSV files without consuming too much memory.\n\nYes, that's a better approach.\n\nSo, modify the code to read rows one by one in a loop, processing each row and adding it to the batch.\n\nWhen the batch reaches the specified size, execute the bulk request and reset the batch.\n\nAfter processing all rows, execute any remaining bulk request.\n\nNow, decide on the batch size.\n\nMake it a configurable flag, default to 100.\n\nAnother flag:\n\n- `batch-size`: number of documents per bulk request (default 100)\n\nNow, implement the bulk request building.\n\nEach bulk action consists of two lines:\n\n1. The index action metadata as JSON\n\n2. The document source as JSON\n\nSo, for each document in the batch, create these two JSON objects and join them with a newline.\n\nFinally, end with a newline.\n\nThen, pass this string as bytes to the Elasticsearch Bulk API.\n\nNow, think about how to handle errors in the bulk response.\n\nThe Bulk API response contains items with their status.\n\nIf there are errors, I need to collect and report them.\n\nI can iterate through the items in the response and check for errors.\n\nIf there are errors, print them out, maybe log the problematic document.\n\nNow, consider adding a progress indicator, especially for large CSV files.\n\nIf the verbose flag is set, print a dot or something for each batch processed.\n\nAt the end, print the total number of documents indexed.\n\nNow, think about testing the tool.\n\nI need to create sample CSV files with different delimiters and headers.\n\nTest with and without ID columns.\n\nVerify that the data is correctly indexed in Elasticsearch.\n\nAlso, test with invalid CSV files, missing flags, etc., to ensure proper error handling.\n\nNow, start writing the actual code.\n\nFirst, import the necessary packages:\n\n```\n\nimport (\n\n \"encoding/csv\"\n\n \"flag\"\n\n \"fmt\"\n\n \"github.com/elastic/go-elasticsearch/v8\"\n\n \"github.com/elastic/go-elasticsearch/v8/esapi\"\n\n \"io\"\n\n \"os\"\n\n)\n\n```\n\nDefine constants for default values:\n\n```\n\nconst (\n\n defaultDelimiter = \",\"\n\n defaultIndexName = \"csv_data\"\n\n defaultESURL = \"http://localhost:9200\"\n\n defaultBatchSize = 100\n\n)\n\n```\n\nDefine variables for flags:\n\n```\n\nvar (\n\n csvFile    string\n\n delimiter  string\n\n indexName  string\n\n idColumn   string\n\n headerRow  bool\n\n columns    string\n\n verbose    bool\n\n esURL      string\n\n batchSize  int\n\n)\n\n```\n\nIn the main function, parse the flags:\n\n```\n\nflag.StringVar(&csvFile, \"csv-file\", \"\", \"Path to the CSV file\")\n\nflag.StringVar(&delimiter, \"delimiter\", defaultDelimiter, \"Delimiter used in the CSV file\")\n\nflag.StringVar(&indexName, \"index-name\", defaultIndexName, \"Name of the Elasticsearch index\")\n\nflag.StringVar(&idColumn, \"id-column\", \"\", \"Column name to use as document ID\")\n\nflag.BoolVar(&headerRow, \"header-row\", true, \"Whether the CSV file has a header row\")\n\nflag.StringVar(&columns, \"columns\", \"\", \"Comma-separated list of column names (if no header row)\")\n\nflag.BoolVar(&verbose, \"verbose\", false, \"Provide detailed output during processing\")\n\nflag.StringVar(&esURL, \"es-url\", defaultESURL, \"URL of the Elasticsearch instance\")\n\nflag.IntVar(&batchSize, \"batch-size\", defaultBatchSize, \"Number of documents per bulk request\")\n\nflag.Parse()\n\n```\n\nValidate the flags:\n\n```\n\nif csvFile == \"\" || indexName == \"\" {\n\n fmt.Println(\"Error: csv-file and index-name are required.\")\n\n flag.PrintDefaults()\n\n os.Exit(1)\n\n}\n\nif !headerRow && columns == \"\" {\n\n fmt.Println(\"Error: columns must be provided if header-row is false.\")\n\n flag.PrintDefaults()\n\n os.Exit(1)\n\n}\n\n```\n\nNow, create the Elasticsearch client:\n\n```\n\nesConfig := es.Config{\n\n Addresses: []string{esURL},\n\n}\n\nesClient, err := es.NewClient(esConfig)\n\nif err != nil {\n\n fmt.Printf(\"Error creating Elasticsearch client: %v\\n\", err)\n\n os.Exit(1)\n\n}\n\n```\n\nCheck if the index exists:\n\n```\n\nexists, err := esClient.Indices.Exists([]string{indexName}, nil)\n\nif err != nil {\n\n fmt.Printf(\"Error checking index existence: %v\\n\", err)\n\n os.Exit(1)\n\n}\n\nif !exists {\n\n // Create the index\n\n createResponse, err := esClient.Indices.Create(indexName, nil)\n\n if err != nil {\n\n fmt.Printf(\"Error creating index: %v\\n\", err)\n\n os.Exit(1)\n\n }\n\n defer createResponse.Body.Close()\n\n if createResponse.IsError() {\n\n fmt.Printf(\"Error creating index: %s\\n\", createResponse.String())\n\n os.Exit(1)\n\n }\n\n if verbose {\n\n fmt.Printf(\"Index %s created.\\n\", indexName)\n\n }\n\n}\n\n```\n\nNow, open the CSV file:\n\n```\n\nfile, err := os.Open(csvFile)\n\nif err != nil {\n\n fmt.Printf(\"Error opening CSV file: %v\\n\", err)\n\n os.Exit(1)\n\n}\n\ndefer file.Close()\n\n```\n\nCreate a CSV reader:\n\n```\n\nr := csv.NewReader(file)\n\nr.Comma = rune(delimiter[0])\n\n```\n\nRead the header row or use provided columns:\n\n```\n\nvar columnNames []string\n\nif headerRow {\n\n columnNames, err = r.Read()\n\n if err != nil {\n\n fmt.Printf(\"Error reading header row: %v\\n\", err)\n\n os.Exit(1)\n\n }\n\n} else {\n\n columnNames = strings.Split(columns, \",\")\n\n}\n\n```\n\nIf idColumn is specified, find its index:\n\n```\n\nvar idColumnIndex int\n\nif idColumn != \"\" {\n\n for i, name := range columnNames {\n\n if name == idColumn {\n\n idColumnIndex = i\n\n break\n\n }\n\n }\n\n if idColumnIndex == 0 && columnNames[0] != idColumn {\n\n fmt.Printf(\"Error: specified id-column '%s' not found in CSV columns.\\n\", idColumn)\n\n os.Exit(1)\n\n }\n\n}\n\n```\n\nNow, process the CSV rows in batches:\n\n```\n\nbatch := make([]string, 0, batchSize*2) // each document has two lines\n\ncount := 0\n\nfor {\n\n row, err := r.Read()\n\n if err == io.EOF {\n\n break\n\n }\n\n if err != nil {\n\n fmt.Printf(\"Error reading CSV row: %v\\n\", err)\n\n continue\n\n }\n\n doc := make(map[string]string)\n\n for i, value := range row {\n\n doc[columnNames[i]] = value\n\n }\n\n var docID string\n\n if idColumn != \"\" {\n\n docID = row[idColumnIndex]\n\n }\n\n action := map[string]interface{}{\n\n \"index\": map[string]interface{}{\n\n \"_index\": indexName,\n\n },\n\n }\n\n if docID != \"\" {\n\n action[\"index\"].(map[string]interface{})[\"_id\"] = docID\n\n }\n\n actionJson, _ := json.Marshal(action)\n\n docJson, _ := json.Marshal(doc)\n\n batch = append(batch, string(actionJson), string(docJson)+\"\\n\")\n\n count++\n\n if len(batch) >= batchSize*2 {\n\n // Execute bulk request\n\n bulkRequest := strings.Join(batch, \"\\n\") + \"\\n\"\n\n req := esapi.BulkRequest{\n\n Body: strings.NewReader(bulkRequest),\n\n }\n\n resp, err := esClient.Bulk(req)\n\n if err != nil {\n\n fmt.Printf(\"Error executing bulk request: %v\\n\", err)\n\n continue\n\n }\n\n defer resp.Body.Close()\n\n if resp.IsError() {\n\n fmt.Printf(\"Bulk request failed: %s\\n\", resp.String())\n\n } else {\n\n if verbose {\n\n fmt.Printf(\"Bulk request succeeded: %s\\n\", resp.String())\n\n }\n\n }\n\n batch = batch[:0]\n\n}\n\n}\n\n// Execute any remaining bulk request\n\nif len(batch) > 0 {\n\n bulkRequest := strings.Join(batch, \"\\n\") + \"\\n\"\n\n req := esapi.BulkRequest{\n\n Body: strings.NewReader(bulkRequest),\n\n }\n\n resp, err := esClient.Bulk(req)\n\n if err != nil {\n\n fmt.Printf(\"Error executing bulk request: %v\\n\", err)\n\n } else {\n\n defer resp.Body.Close()\n\n if resp.IsError() {\n\n fmt.Printf(\"Bulk request failed: %s\\n\", resp.String())\n\n } else {\n\n if verbose {\n\n fmt.Printf(\"Bulk request succeeded: %s\\n\", resp.String())\n\n }\n\n }\n\n }\n\n}\n\nfmt.Printf(\"%d documents indexed.\\n\", count)\n\n```\n\nThat's the basic structure of the tool.\n\nI need to make sure to handle errors properly and provide useful feedback to the user.\n\nAlso, consider adding more features like mapping column types, handling quotes in CSV fields, etc., but for now, this should suffice.\n\nI should also think about how to package and distribute this tool.\n\nMaybe create a GitHub repository for it, write some documentation, and provide installation instructions.\n\nUsers can download the binary or build it from source.\n\nAlternatively, I can create a Docker image for easier deployment.\n\nBut for now, I'll focus on getting the tool working correctly.\n\nI need to test it thoroughly with different CSV files and scenarios to ensure reliability.\n\n## Final Solution\nTo implement a Golang command-line tool that ingests a CSV file and indexes its data into Elasticsearch, follow the steps and code provided below. This tool allows users to specify the CSV delimiter and the Elasticsearch index name via command-line flags.\n\n### Approach\n\n1. **Parse Command-Line Flags:** \n   - `csv-file`: Path to the CSV file.\n   - `delimiter`: Delimiter used in the CSV file (default is comma).\n   - `index-name`: Name of the Elasticsearch index.\n   - `id-column`: Column name to use as the document ID.\n   - `header-row`: Whether the CSV file has a header row (default is true).\n   - `columns`: Comma-separated list of column names (if no header row).\n   - `verbose`: Provide detailed output during processing.\n   - `es-url`: URL of the Elasticsearch instance (default is \"http://localhost:9200\").\n   - `batch-size`: Number of documents per bulk request (default is 100).\n\n2. **Read and Process CSV:**\n   - Open and read the CSV file using the specified delimiter.\n   - Extract column names from the header row or use provided column names.\n\n3. **Elasticsearch Operations:**\n   - Create an Elasticsearch client and check if the specified index exists.\n   - Create the index if it doesn't exist.\n   - Index data into Elasticsearch using bulk API for efficiency.\n\n4. **Batch Processing:**\n   - Process CSV rows in batches to manage memory and improve performance.\n\n### Solution Code\n\n```go\npackage main\n\nimport (\n    \"encoding/csv\"\n    \"flag\"\n    \"fmt\"\n    \"github.com/elastic/go-elasticsearch/v8\"\n    \"github.com/elastic/go-elasticsearch/v8/esapi\"\n    \"io\"\n    \"os\"\n    \"strings\"\n)\n\nconst (\n    defaultDelimiter = \",\"\n    defaultIndexName = \"csv_data\"\n    defaultESURL     = \"http://localhost:9200\"\n    defaultBatchSize = 100\n)\n\nfunc main() {\n    var (\n        csvFile    string\n        delimiter  string\n        indexName  string\n        idColumn   string\n        headerRow  bool\n        columns    string\n        verbose    bool\n        esURL      string\n        batchSize  int\n    )\n\n    flag.StringVar(&csvFile, \"csv-file\", \"\", \"Path to the CSV file\")\n    flag.StringVar(&delimiter, \"delimiter\", defaultDelimiter, \"Delimiter used in the CSV file\")\n    flag.StringVar(&indexName, \"index-name\", defaultIndexName, \"Name of the Elasticsearch index\")\n    flag.StringVar(&idColumn, \"id-column\", \"\", \"Column name to use as document ID\")\n    flag.BoolVar(&headerRow, \"header-row\", true, \"Whether the CSV file has a header row\")\n    flag.StringVar(&columns, \"columns\", \"\", \"Comma-separated list of column names (if no header row)\")\n    flag.BoolVar(&verbose, \"verbose\", false, \"Provide detailed output during processing\")\n    flag.StringVar(&esURL, \"es-url\", defaultESURL, \"URL of the Elasticsearch instance\")\n    flag.IntVar(&batchSize, \"batch-size\", defaultBatchSize, \"Number of documents per bulk request\")\n    flag.Parse()\n\n    if csvFile == \"\" || indexName == \"\" {\n        fmt.Println(\"Error: csv-file and index-name are required.\")\n        flag.PrintDefaults()\n        os.Exit(1)\n    }\n    if !headerRow && columns == \"\" {\n        fmt.Println(\"Error: columns must be provided if header-row is false.\")\n        flag.PrintDefaults()\n        os.Exit(1)\n    }\n\n    esConfig := elasticsearch.Config{\n        Addresses: []string{esURL},\n    }\n    esClient, err := elasticsearch.NewClient(esConfig)\n    if err != nil {\n        fmt.Printf(\"Error creating Elasticsearch client: %v\\n\", err)\n        os.Exit(1)\n    }\n\n    exists, err := esClient.Indices.Exists([]string{indexName}, nil)\n    if err != nil {\n        fmt.Printf(\"Error checking index existence: %v\\n\", err)\n        os.Exit(1)\n    }\n    if !exists {\n        createResponse, err := esClient.Indices.Create(indexName, nil)\n        if err != nil {\n            fmt.Printf(\"Error creating index: %v\\n\", err)\n            os.Exit(1)\n        }\n        defer createResponse.Body.Close()\n        if createResponse.IsError() {\n            fmt.Printf(\"Error creating index: %s\\n\", createResponse.String())\n            os.Exit(1)\n        }\n        if verbose {\n            fmt.Printf(\"Index %s created.\\n\", indexName)\n        }\n    }\n\n    file, err := os.Open(csvFile)\n    if err != nil {\n        fmt.Printf(\"Error opening CSV file: %v\\n\", err)\n        os.Exit(1)\n    }\n    defer file.Close()\n\n    r := csv.NewReader(file)\n    r.Comma = rune(delimiter[0])\n\n    var columnNames []string\n    if headerRow {\n        columnNames, err = r.Read()\n        if err != nil {\n            fmt.Printf(\"Error reading header row: %v\\n\", err)\n            os.Exit(1)\n        }\n    } else {\n        columnNames = strings.Split(columns, \",\")\n    }\n\n    var idColumnIndex int\n    if idColumn != \"\" {\n        for i, name := range columnNames {\n            if name == idColumn {\n                idColumnIndex = i\n                break\n            }\n        }\n        if idColumnIndex == 0 && columnNames[0] != idColumn {\n            fmt.Printf(\"Error: specified id-column '%s' not found in CSV columns.\\n\", idColumn)\n            os.Exit(1)\n        }\n    }\n\n    batch := make([]string, 0, batchSize*2)\n    count := 0\n    for {\n        row, err := r.Read()\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            fmt.Printf(\"Error reading CSV row: %v\\n\", err)\n            continue\n        }\n        doc := make(map[string]string)\n        for i, value := range row {\n            doc[columnNames[i]] = value\n        }\n        var docID string\n        if idColumn != \"\" {\n            docID = row[idColumnIndex]\n        }\n        action := map[string]interface{}{\n            \"index\": map[string]interface{}{\n                \"_index\": indexName,\n            },\n        }\n        if docID != \"\" {\n            action[\"index\"].(map[string]interface{})[\"_id\"] = docID\n        }\n        actionJson, _ := json.Marshal(action)\n        docJson, _ := json.Marshal(doc)\n        batch = append(batch, string(actionJson), string(docJson)+\"\\n\")\n        count++\n        if len(batch) >= batchSize*2 {\n            bulkRequest := strings.Join(batch, \"\\n\") + \"\\n\"\n            req := esapi.BulkRequest{\n                Body: strings.NewReader(bulkRequest),\n            }\n            resp, err := esClient.Bulk(req)\n            if err != nil {\n                fmt.Printf(\"Error executing bulk request: %v\\n\", err)\n                continue\n            }\n            defer resp.Body.Close()\n            if resp.IsError() {\n                fmt.Printf(\"Bulk request failed: %s\\n\", resp.String())\n            } else if verbose {\n                fmt.Printf(\"Bulk request succeeded: %s\\n\", resp.String())\n            }\n            batch = batch[:0]\n        }\n    }\n    if len(batch) > 0 {\n        bulkRequest := strings.Join(batch, \"\\n\") + \"\\n\"\n        req := esapi.BulkRequest{\n            Body: strings.NewReader(bulkRequest),\n        }\n        resp, err := esClient.Bulk(req)\n        if err != nil {\n            fmt.Printf(\"Error executing bulk request: %v\\n\", err)\n        } else {\n            defer resp.Body.Close()\n            if resp.IsError() {\n                fmt.Printf(\"Bulk request failed: %s\\n\", resp.String())\n            } else if verbose {\n                fmt.Printf(\"Bulk request succeeded: %s\\n\", resp.String())\n            }\n        }\n    }\n    fmt.Printf(\"%d documents indexed.\\n\", count)\n}\n```\n\n### Usage\n\n1. **Install dependencies:**\n   ```sh\n   go get github.com/elastic/go-elasticsearch/v8\n   ```\n\n2. **Compile the tool:**\n   ```sh\n   go build -o csv-to-es main.go\n   ```\n\n3. **Run the tool with required flags:**\n   ```sh\n   ./csv-to-es -csv-file=path/to/file.csv -index-name=my_index\n   ```\n\nThis tool reads a CSV file, processes it based on the provided flags, and indexes the data into the specified Elasticsearch index efficiently using bulk API requests."
    }
]