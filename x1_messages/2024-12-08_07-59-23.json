[
    {
        "role": "user",
        "content": "Implement a Python script that takes a directory path as a command-line argument and recursively finds all duplicate files within the directory based on their content. The script should print out the list of duplicate files along with their file paths."
    },
    {
        "role": "assistant",
        "content": "To implement a Python script that finds duplicate files based on their content within a given directory and its subdirectories, we can follow these steps:\n\n1. **Traverse the Directory:** Use Python's `os` module to walk through the directory tree.\n2. **Calculate File Hashes:** For each file, calculate its hash (e.g., using SHA1) to uniquely identify its content.\n3. **Track Duplicates:** Keep track of files that have the same hash, indicating they are duplicates.\n4. **Output the Results:** Print the list of duplicate files along with their file paths.\n\nHere's a step-by-step implementation:\n\n### Step 1: Import Necessary Modules\n\n```python\nimport os\nimport sys\nimport hashlib\n```\n\n### Step 2: Define a Function to Calculate File Hash\n\nWe'll use the SHA1 hash function for its balance between speed and uniqueness.\n\n```python\ndef calculate_hash(file_path, block_size=65536):\n    sha1 = hashlib.sha1()\n    with open(file_path, 'rb') as f:\n        while True:\n            data = f.read(block_size)\n            if not data:\n                break\n            sha1.update(data)\n    return sha1.hexdigest()\n```\n\n### Step 3: Define a Function to Find Duplicates\n\nThis function will traverse the directory tree and collect files with the same hash.\n\n```python\ndef find_duplicates(root_dir):\n    file_hash_dict = {}\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            file_hash = calculate_hash(file_path)\n            if file_hash in file_hash_dict:\n                file_hash_dict[file_hash].append(file_path)\n            else:\n                file_hash_dict[file_hash] = [file_path]\n    duplicates = {hash_val: paths for hash_val, paths in file_hash_dict.items() if len(paths) > 1}\n    return duplicates\n```\n\n### Step 4: Main Function to Handle Command-Line Arguments and Print Duplicates\n\n```python\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python find_duplicates.py <directory>\")\n        sys.exit(1)\n    \n    directory = sys.argv[1]\n    if not os.path.isdir(directory):\n        print(\"The specified path is not a directory.\")\n        sys.exit(1)\n    \n    duplicates = find_duplicates(directory)\n    \n    if duplicates:\n        print(\"Duplicate files found:\")\n        for hash_val, paths in duplicates.items():\n            print(f\"Hash: {hash_val}\")\n            for path in paths:\n                print(f\"  {path}\")\n    else:\n        print(\"No duplicate files found.\")\n```\n\n### Step 5: Execute the Main Function\n\n```python\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n1. **Hash Calculation:** The `calculate_hash` function reads files in chunks to handle large files efficiently and computes their SHA1 hash.\n2. **Directory Traversal:** The `os.walk` function is used to traverse the directory tree recursively.\n3. **Tracking Duplicates:** Files with the same hash are grouped together in a dictionary where the hash is the key.\n4. **Command-Line Argument Handling:** The script expects one command-line argument: the directory path to search for duplicates.\n\nThis script provides a straightforward way to identify duplicate files based on their content within a specified directory and its subdirectories."
    }
]