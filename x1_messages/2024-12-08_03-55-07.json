[
    {
        "role": "user",
        "content": "Write a Python script that monitors a directory for new files and automatically compresses them using the gzip library. The script should maintain a log of compressed files and their original sizes."
    },
    {
        "role": "assistant",
        "content": "Alright, I'm going to help you write a Python script that watches a specific directory for new files and automatically compresses them using the gzip library. Additionally, it will keep a log of which files have been compressed and what their original sizes were.\n\nFirst, we need to import the necessary libraries. We'll use `os` to interact with the directory, `time` to handle delays, `shutil` for file operations, and `gzip` for compression.\n\n```python\nimport os\nimport time\nimport shutil\nimport gzip\n```\n\nNext, we need to set up some variables. You'll need to specify the directory to monitor and the log file where compression details will be recorded.\n\n```python\nmonitor_directory = '/path/to/directory'  # Change this to the directory you want to monitor\nlog_file = 'compression_log.txt'\n```\n\nWe should also check if the log file exists; if not, we'll create it.\n\n```python\nif not os.path.exists(log_file):\n    with open(log_file, 'w') as log:\n        log.write(\"Compressed File\\tOriginal Size\\n\")\n```\n\nNow, we need a way to monitor the directory for new files. One common approach is to keep track of the files already present and periodically check for any new additions.\n\nLet's start by getting the list of initial files in the directory.\n\n```python\ninitial_files = set(os.listdir(monitor_directory))\n```\n\nWe'll set up a loop that runs indefinitely, checking for new files at regular intervals.\n\n```python\nwhile True:\n    time.sleep(5)  # Check every 5 seconds\n    current_files = set(os.listdir(monitor_directory))\n    new_files = current_files - initial_files\n    \n    for file in new_files:\n        file_path = os.path.join(monitor_directory, file)\n        if os.path.isfile(file_path):\n            original_size = os.path.getsize(file_path)\n            compressed_file = file_path + '.gz'\n            with open(file_path, 'rb') as f_in:\n                with gzip.open(compressed_file, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n            # Log the compression\n            with open(log_file, 'a') as log:\n                log.write(f\"{compressed_file}\\t{original_size}\\n\")\n            # Optionally, remove the original file\n            # os.remove(file_path)\n            print(f\"Compressed {file} to {compressed_file}\")\n    \n    initial_files.update(new_files)\n```\n\nIn this script:\n\n- We sleep for 5 seconds in each iteration to avoid high CPU usage.\n\n- We get the current list of files and find the difference from the initial list to identify new files.\n\n- For each new file, we record its original size, compress it using gzip, and log the compression details.\n\n- Optionally, you can remove the original file after compression by uncommenting the `os.remove(file_path)` line.\n\nMake sure to replace `'/path/to/directory'` with the actual path of the directory you want to monitor.\n\nThis script will run indefinitely, so you might want to set up a way to stop it, perhaps by checking for a specific file or using keyboard interrupts.\n\nAlso, consider handling exceptions, such as permission errors or issues during compression, to make the script more robust.\n\n## Final Solution\nTo solve this problem, we need a Python script that monitors a specified directory for new files and automatically compresses them using the gzip library. Additionally, the script should maintain a log of compressed files along with their original sizes.\n\n### Approach\n\n1. **Import necessary libraries**: We use `os` for directory operations, `time` for delays, `shutil` for file operations, and `gzip` for compression.\n2. **Set up variables**: Specify the directory to monitor and the log file path.\n3. **Initialize log file**: Ensure the log file exists and has a header if it's newly created.\n4. **Monitor the directory**: Track initial files and periodically check for new files.\n5. **Compress new files**: When new files are detected, compress them using gzip and log the details.\n6. **Update the list of files**: Keep track of processed files to avoid re-compressing them.\n\n### Solution Code\n\n```python\nimport os\nimport time\nimport shutil\nimport gzip\n\n# Set the directory to monitor and the log file\nmonitor_directory = '/path/to/directory'  # Change this to the directory you want to monitor\nlog_file = 'compression_log.txt'\n\n# Ensure the log file exists\nif not os.path.exists(log_file):\n    with open(log_file, 'w') as log:\n        log.write(\"Compressed File\\tOriginal Size\\n\")\n\n# Get initial list of files in the directory\ninitial_files = set(os.listdir(monitor_directory))\n\nwhile True:\n    time.sleep(5)  # Check every 5 seconds\n    current_files = set(os.listdir(monitor_directory))\n    new_files = current_files - initial_files\n    \n    for file in new_files:\n        file_path = os.path.join(monitor_directory, file)\n        if os.path.isfile(file_path):\n            original_size = os.path.getsize(file_path)\n            compressed_file = file_path + '.gz'\n            with open(file_path, 'rb') as f_in:\n                with gzip.open(compressed_file, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n            # Log the compression\n            with open(log_file, 'a') as log:\n                log.write(f\"{compressed_file}\\t{original_size}\\n\")\n            # Optionally, remove the original file\n            # os.remove(file_path)\n            print(f\"Compressed {file} to {compressed_file}\")\n    \n    initial_files.update(new_files)\n```\n\n### Explanation\n\n1. **Import Libraries**: We import the required libraries at the beginning.\n2. **Set Up Variables**: Define the directory to monitor and the log file.\n3. **Initialize Log File**: Create the log file if it doesn't exist and write the header.\n4. **Monitor Directory**: Use a loop to periodically check for new files in the directory.\n5. **Compress Files**: When new files are found, compress them using gzip and log the details.\n6. **Update File List**: Keep track of processed files to avoid re-compressing them in future iterations.\n\nThis script will run indefinitely, checking for new files every 5 seconds. You can adjust the sleep time as needed. Optionally, you can implement additional features like handling different file types or adding more detailed logging."
    }
]