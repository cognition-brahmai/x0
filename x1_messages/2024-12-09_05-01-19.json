[
    {
        "role": "user",
        "content": "Develop a Python script that reads a text file containing URLs, one per line, and prints out the HTTP status code for each URL using the requests library.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that reads URLs from a text file and checks their HTTP status codes using the `requests` library. Sounds straightforward, but I need to make sure I cover all the bases here.\n\nFirst things first, I need to make sure that the `requests` library is installed because it's not a built-in Python module. I can install it using pip if it's not already available. So, in the script, I should probably include a note or a check for that.\n\nNext, I need to handle reading from a text file. The file should contain one URL per line. I should think about possible edge cases, like empty lines or lines with just whitespace. Maybe I should skip those or handle them in some way.\n\nAlso, I need to consider that some URLs might not start with \"http://\" or \"https://\". Should I assume that all URLs in the file are properly formatted with the scheme included? Or should I try to add it if it's missing? That could be a nice touch, but it might complicate things a bit.\n\nWhen making the requests, I should consider that some URLs might take longer to respond than others. Maybe I should set a timeout to avoid hanging on slow responses. Let's say, a timeout of 5 seconds?\n\nError handling is crucial here. What if the file doesn't exist? Or if there's a problem with the requests, like connection errors or too many redirects? I should catch those exceptions and maybe print a message indicating what went wrong.\n\nAdditionally, printing the status codes is straightforward, but maybe I could categorize them or provide some extra information based on the status code. For example, if it's a 404, I could note that the page was not found.\n\nPerformance-wise, if the file has a lot of URLs, making sequential requests might take a while. Perhaps I could look into making asynchronous requests to speed things up, but that might be overkill for a simple script.\n\nLet me outline the steps I need to take:\n\n1. Import the necessary modules: `requests` and maybe `os` to handle file operations more gracefully.\n\n2. Prompt the user for the filename containing the URLs.\n\n3. Open the file and read the URLs into a list, skipping any empty lines.\n\n4. Iterate over the list of URLs, making a HEAD request to each one to get the status code.\n\n5. Print the URL along with its status code.\n\n6. Handle any exceptions that occur during file operations or requests.\n\nWait, should I use HEAD requests or GET requests? HEAD requests are lighter because they don't retrieve the body, which is helpful for just checking the status code.\n\nBut I should be aware that some servers might handle HEAD requests differently than GET requests. Some might not support HEAD at all, in which case the requests library should fall back to GET.\n\nHmm, maybe I should just use GET requests to be safe, even though it might be a bit heavier.\n\nAlso, I should think about rate limiting. If there are many URLs, hammering requests one after another might be undesirable, especially if it's the same domain. Maybe I should include a small delay between requests.\n\nLet me consider using the `requests.head` method with a timeout. I'll catch exceptions like `requests.exceptions.RequestException` and handle them accordingly.\n\nFor the output, maybe I can format it neatly, like printing the URL and the status code aligned in columns.\n\nLet me think about how to handle URLs without the scheme. I can prepend \"http://\" if the URL doesn't start with \"http://\" or \"https://\". But sometimes, URLs might start with \"www.\", so I need to handle that case.\n\nMaybe I can check if the URL starts with \"http://\" or \"https://\", and if not, assume it's \"http://\" and try that. If it fails, perhaps try \"https://\" as well.\n\nBut that might complicate the script too much. Maybe I'll just assume that all URLs have the scheme included.\n\nWait, no. That might be too restrictive. Maybe I'll allow URLs without the scheme and prepend \"http://\" in those cases.\n\nI need to be careful with that, as some URLs might be using other schemes like \"ftp://\" or \"ssh://\", but for the sake of this script, I'll assume it's HTTP or HTTPS.\n\nLet me sketch a rough code structure:\n\ntry:\n\n    filename = input(\"Enter the filename containing URLs: \")\n\n    with open(filename, 'r') as file:\n\n        urls = [line.strip() for line in file if line.strip()]\n\nexcept FileNotFoundError:\n\n    print(\"File not found.\")\n\n    exit()\n\nfor url in urls:\n\n    try:\n\n        response = requests.head(url, allow_redirects=True, timeout=5)\n\n        print(f\"{url}: {response.status_code}\")\n\n    except requests.exceptions.RequestException as e:\n\n        print(f\"{url}: {e}\")\n\nThat's a basic structure. I need to think about URL formatting before making the request.\n\nLet me create a function to normalize the URL, ensuring it has the scheme.\n\ndef normalize_url(url):\n\n    if not url.lower().startswith(\"http://\") and not url.lower().startswith(\"https://\"):\n\n        url = \"http://\" + url\n\n    return url\n\nThen, in the loop:\n\nfor url in urls:\n\n    url = normalize_url(url)\n\n    try:\n\n        response = requests.head(url, allow_redirects=True, timeout=5)\n\n        print(f\"{url}: {response.status_code}\")\n\n    except requests.exceptions.RequestException as e:\n\n        print(f\"{url}: {e}\")\n\nThat should handle URLs with and without the scheme.\n\nI should also think about idna encoding for international domains, but `requests` should handle that internally.\n\nMaybe I can add a option to follow redirects or not. By setting `allow_redirects=True`, it will follow redirects and give the final status code. That might be what I want.\n\nBut perhaps I should consider not following redirects to see the initial status code.\n\nWait, following redirects is probably more useful for checking if a URL is alive, as it accounts for permanent redirects.\n\nBut maybe I'll add a option for the user to choose whether to follow redirects or not.\n\nHmm, that might be too advanced for a simple script. I'll keep it as follows redirects for now.\n\nLet me think about the output. Maybe I can categorize the status codes into groups like 2xx (success), 3xx (redirect), 4xx (client error), 5xx (server error), and print them accordingly.\n\nAlternatively, I can just print the URL and the status code as is.\n\nFor now, I'll keep it simple and just print the URL and the status code.\n\nI should also think about the script's exit status. Maybe set it to non-zero if there were any errors.\n\nBut that might be too strict, as some URLs might fail while others succeed.\n\nAlternatively, I could count the number of successful and failed URLs and print a summary at the end.\n\nThat's a good idea.\n\nSo, I'll keep track of the number of URLs processed, the number of successes, and the number of failures.\n\nAt the end, print a summary.\n\nAlso, maybe write the results to an output file instead of printing to the console, especially if there are many URLs.\n\nBut for simplicity, I'll stick with printing to the console.\n\nLet me update the code structure accordingly.\n\nimport requests\n\ndef normalize_url(url):\n\n    if not url.lower().startswith(\"http://\") and not url.lower().startswith(\"https://\"):\n\n        url = \"http://\" + url\n\n    return url\n\ndef main():\n\n    try:\n\n        filename = input(\"Enter the filename containing URLs: \")\n\n        with open(filename, 'r') as file:\n\n            urls = [line.strip() for line in file if line.strip()]\n\n    except FileNotFoundError:\n\n        print(\"File not found.\")\n\n        return\n\n    total = len(urls)\n\n    successes = 0\n\n    failures = 0\n\n    for url in urls:\n\n        url = normalize_url(url)\n\n        try:\n\n            response = requests.head(url, allow_redirects=True, timeout=5)\n\n            print(f\"{url}: {response.status_code}\")\n\n            successes += 1\n\n        except requests.exceptions.RequestException as e:\n\n            print(f\"{url}: {e}\")\n\n            failures += 1\n\n    print(\"\\nSummary:\")\n\n    print(f\"Total URLs: {total}\")\n\n    print(f\"Successful checks: {successes}\")\n\n    print(f\"Failed checks: {failures}\")\n\nif __name__ == \"__main__\":\n\n    main()\n\nThat looks pretty good. I should test this with a sample file to see if it works as expected.\n\nI should also consider adding a shebang at the top if I'm planning to run this on a Unix-like system.\n\nBut since the user might be on Windows, I'll omit it for now.\n\nMaybe I should add some docstrings and comments for clarity, but since it's a simple script, it might not be necessary.\n\nLet me think about potential exceptions that could occur during requests.\n\n- ConnectionError: If there's a network problem like DNS failure, refused connection, etc.\n\n- Timeout: If the request times out.\n\n- TooManyRedirects: If the response exceeds the maximum number of redirects.\n\n- InvalidURL: If the URL is invalid.\n\nI'm already catching `requests.exceptions.RequestException`, which is a base exception class for all requests exceptions, so it should catch all of these.\n\nBut I should make sure that `requests` is imported correctly.\n\nAlso, I need to remember to handle UnicodeEncodeError if the URL contains non-ASCII characters, but `requests` should handle that.\n\nWait, no, `requests` should handle encoding issues, but I might need to ensure that the file is read with the correct encoding.\n\nAssuming the file is in UTF-8, which is standard, I should be fine.\n\nBut to be safe, I can specify the encoding when opening the file.\n\nLike this:\n\nwith open(filename, 'r', encoding='utf-8') as file:\n\n    urls = [line.strip() for line in file if line.strip()]\n\nThat should handle most cases.\n\nAlso, perhaps I should validate the URL format before making the request, to catch obvious errors.\n\nBut that might be overkill, as `requests` will likely raise an exception if the URL is invalid.\n\nI'll skip that for now.\n\nAnother thing: some URLs might have query parameters or fragments. I need to make sure that they are handled correctly.\n\n`requests` should handle them without issues.\n\nLet me think about how to handle URLs that require authentication or have other request requirements.\n\nWell, for a basic script, I'll assume that no authentication is needed.\n\nIf a URL requires authentication, the script will likely receive a 401 Unauthorized response, which is fine.\n\nI should also consider that some sites might block requests that don't come from a browser by checking the User-Agent header.\n\nMaybe I should set a User-Agent header to mimic a browser request.\n\nThat's a good point.\n\nI can set the User-Agent to something like \"Mozilla/5.0\" in the headers.\n\nLet me add that.\n\nSo, in the requests.head method:\n\nheaders = {'User-Agent': 'Mozilla/5.0'}\n\nresponse = requests.head(url, headers=headers, allow_redirects=True, timeout=5)\n\nThat should help in cases where the server checks the User-Agent.\n\nAlright, I think that covers most of the scenarios.\n\nLet me write the final script accordingly.\n\n## Final Solution\nTo solve this problem, we need to develop a Python script that reads URLs from a text file, one per line, and prints out the HTTP status code for each URL using the `requests` library.\n\n### Approach\n\n1. **Reading the File**: Read the list of URLs from a specified text file, skipping any empty lines.\n2. **URL Normalization**: Ensure each URL starts with \"http://\" or \"https://\". If not, prepend \"http://\" to the URL.\n3. **HTTP Request**: Use the `requests` library to send a HEAD request to each URL and retrieve the status code. Use a User-Agent to mimic a browser request and set a timeout to handle slow responses.\n4. **Error Handling**: Catch and handle exceptions that may occur during file operations or HTTP requests.\n5. **Summary**: Provide a summary of the total URLs checked, the number of successful checks, and the number of failed checks.\n\n### Solution Code\n\n```python\nimport requests\n\ndef normalize_url(url):\n    if not url.lower().startswith(\"http://\") and not url.lower().startswith(\"https://\"):\n        url = \"http://\" + url\n    return url\n\ndef main():\n    try:\n        filename = input(\"Enter the filename containing URLs: \")\n        with open(filename, 'r', encoding='utf-8') as file:\n            urls = [line.strip() for line in file if line.strip()]\n    except FileNotFoundError:\n        print(\"File not found.\")\n        return\n\n    total = len(urls)\n    successes = 0\n    failures = 0\n\n    headers = {'User-Agent': 'Mozilla/5.0'}\n\n    for url in urls:\n        url = normalize_url(url)\n        try:\n            response = requests.head(url, headers=headers, allow_redirects=True, timeout=5)\n            print(f\"{url}: {response.status_code}\")\n            successes += 1\n        except requests.exceptions.RequestException as e:\n            print(f\"{url}: {e}\")\n            failures += 1\n\n    print(\"\\nSummary:\")\n    print(f\"Total URLs: {total}\")\n    print(f\"Successful checks: {successes}\")\n    print(f\"Failed checks: {failures}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n1. **Reading the File**: The script reads the file specified by the user, stripping any empty lines to ensure only valid URLs are processed.\n2. **URL Normalization**: Each URL is checked to see if it starts with \"http://\" or \"https://\". If not, \"http://\" is prepended to make it a valid URL.\n3. **HTTP Request**: A HEAD request is sent to each URL with a User-Agent header to simulate a browser request and a timeout of 5 seconds to manage slow responses.\n4. **Error Handling**: The script handles file not found errors and any exceptions that occur during HTTP requests, such as connection errors or invalid URLs.\n5. **Summary**: After checking all URLs, a summary is printed to provide an overview of the operations performed, including counts of successful and failed checks.\n\nThis script provides a straightforward way to check the status codes of multiple URLs from a file, making it useful for tasks such as validating website links or monitoring URL availability."
    }
]