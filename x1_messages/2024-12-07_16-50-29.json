[
    {
        "role": "user",
        "content": "Design a web scraper in Python that extracts the top 10 trending repositories on GitHub, along with their star count and primary programming language used. The scraper should output the results in a formatted table."
    },
    {
        "role": "assistant",
        "content": "I'm going to design a web scraper in Python that extracts the top 10 trending repositories on GitHub, along with their star count and primary programming language used. Then, it will output the results in a formatted table.\n\nFirst, I need to understand how GitHub's trending page is structured. The URL for trending repositories is https://github.com/trending. I'll need to inspect the HTML of this page to see how the repository information is presented.\n\nI should use a library like Beautiful Soup to parse the HTML and extract the required data. I'll need to send an HTTP request to the GitHub trending page, retrieve the HTML content, and then parse it to find the repository names, star counts, and primary programming languages.\n\nOnce I have the data, I can format it into a table using a library like tabulate for better readability.\n\nLet me outline the steps:\n\n1. Send an HTTP GET request to https://github.com/trending.\n\n2. Parse the HTML content using Beautiful Soup.\n\n3. Find the sections that contain the repository information.\n\n4. Extract the repository name, star count, and primary programming language for the top 10 repositories.\n\n5. Organize this data into a list of dictionaries or a similar structure.\n\n6. Use the tabulate library to print the data in a formatted table.\n\nI should also consider handling any potential errors, such as failing to connect to the GitHub server or changes in the HTML structure of the page.\n\nLet me start by writing the code to send the HTTP request and parse the HTML.\n\nI'll need to import the necessary libraries:\n\n```python\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nfrom tabulate import tabulate\n\n```\n\nNext, I'll send the GET request to the GitHub trending page:\n\n```python\n\nurl = 'https://github.com/trending'\n\nresponse = requests.get(url)\n\n```\n\nI should check if the request was successful:\n\n```python\n\nif response.status_code != 200:\n\nprint(\"Failed to retrieve the page\")\n\nexit()\n\n```\n\nNow, I'll parse the HTML content using Beautiful Soup:\n\n```python\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n```\n\nNext, I need to find the elements that contain the repository information. By inspecting the GitHub trending page, I can see that each trending repository is contained within a `<article>` tag with the class `Box-row`.\n\nI'll find all such `<article>` tags:\n\n```python\n\narticles = soup.find_all('article', class_='Box-row')\n\n```\n\nNow, for each article, I need to extract the repository name, star count, and primary programming language.\n\nLooking at the HTML, the repository name is within an `<h1>` tag with class `h3 lh-condensed', and the star count is within a relative time tag with class 'no-wrap d-flex flex-nowrap'.\n\nThe primary programming language is within a span with class 'd-inline-block ml-0 mr-3'.\n\nI need to extract these elements for each article.\n\nI'll create a list to hold the repository data:\n\n```python\n\nrepos = []\n\n```\n\nThen, loop through the first 10 articles:\n\n```python\n\nfor article in articles[:10]:\n\n# Extract repository name\n\nrepo_name = article.h1.a['href'].strip()\n\n# Extract star count\n\nstars = article.find('a', class_='Link--muted').text.strip()\n\n# Extract primary programming language\n\nlanguage_span = article.find('span', itemprop='programmingLanguage')\n\nlanguage = language_span.text.strip() if language_span else 'N/A'\n\n# Append to the list\n\nrepos.append({\n\n'name': repo_name,\n\n'stars': stars,\n\n'language': language\n\n})\n\n```\n\nNow, I'll use the tabulate library to print the data in a formatted table.\n\nFirst, I need to install the tabulate library if I haven't already:\n\n```bash\n\npip install tabulate\n\n```\n\nThen, I can create a table from the list of dictionaries:\n\n```python\n\nheaders = ['Repository', 'Stars', 'Language']\n\ntable = tabulate(repos, headers=headers, tablefmt='grid')\n\nprint(table)\n\n```\n\nPutting it all together, the complete script would look like this:\n\n```python\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nfrom tabulate import tabulate\n\ndef main():\n\nurl = 'https://github.com/trending'\n\nresponse = requests.get(url)\n\nif response.status_code != 200:\n\nprint(\"Failed to retrieve the page\")\n\nreturn\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\narticles = soup.find_all('article', class_='Box-row')\n\nrepos = []\n\nfor article in articles[:10]:\n\n# Extract repository name\n\nrepo_name = article.h1.a['href'].strip()\n\n# Extract star count\n\nstars = article.find('a', class_='Link--muted').text.strip()\n\n# Extract primary programming language\n\nlanguage_span = article.find('span', itemprop='programmingLanguage')\n\nlanguage = language_span.text.strip() if language_span else 'N/A'\n\n# Append to the list\n\nrepos.append({\n\n'name': repo_name,\n\n'stars': stars,\n\n'language': language\n\n})\n\n# Print the table\n\nheaders = ['Repository', 'Stars', 'Language']\n\ntable = tabulate(repos, headers=headers, tablefmt='grid')\n\nprint(table)\n\nif __name__ == '__main__':\n\nmain()\n\n```\n\nI should test this script to ensure it works correctly. I might need to adjust the CSS selectors if the HTML structure changes.\n\nAlso, I should note that web scraping GitHub is subject to their terms of service. GitHub provides a public API that can be used for fetching repository data, which is a more reliable and acceptable way to access their data.\n\nUsing the GitHub API would be a better approach in terms of reliability and compliance. Let me consider rewriting the script to use the GitHub API instead.\n\nFirst, I'll look into the GitHub API documentation to see how to fetch trending repositories.\n\nUpon checking, I realize that GitHub's public API doesn't directly support fetching trending repositories. The trending page is a special feature without a corresponding API endpoint.\n\nTherefore, web scraping might be the only way to fetch this data, although it's not recommended due to potential changes in the HTML structure.\n\nAlternatively, I can use a third-party service or library that provides access to trending repositories.\n\nI found a library called `github-trending`, which seems to scrape the GitHub trending page.\n\nI can install it using pip:\n\n```bash\n\npip install github-trending\n\n```\n\nThen, I can use it in my script:\n\n```python\n\nfrom github_trending import trending\n\ndef main():\n\nrepos = trending()\n\nfor repo in repos[:10]:\n\nprint(f\"Repository: {repo.full_name}\")\n\nprint(f\"Stars: {repo.stargazers_count}\")\n\nprint(f\"Language: {repo.language}\")\n\nprint(\"-\" * 40)\n\nif __name__ == '__main__':\n\nmain()\n\n```\n\nThis seems simpler and more maintainable than writing my own scraper.\n\nHowever, since the task is to design a web scraper, I'll proceed with the original plan.\n\nI should also handle any exceptions that might occur during the HTTP request or parsing process.\n\nHere's an updated version of the script with exception handling:\n\n```python\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nfrom tabulate import tabulate\n\ndef main():\n\ntry:\n\nurl = 'https://github.com/trending'\n\nresponse = requests.get(url)\n\nresponse.raise_for_status()  # Raise an exception for HTTP errors\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\narticles = soup.find_all('article', class_='Box-row')\n\nrepos = []\n\nfor article in articles[:10]:\n\n# Extract repository name\n\nrepo_name = article.h1.a['href'].strip()\n\n# Extract star count\n\nstars = article.find('a', class_='Link--muted').text.strip()\n\n# Extract primary programming language\n\nlanguage_span = article.find('span', itemprop='programmingLanguage')\n\nlanguage = language_span.text.strip() if language_span else 'N/A'\n\n# Append to the list\n\nrepos.append({\n\n'name': repo_name,\n\n'stars': stars,\n\n'language': language\n\n})\n\n# Print the table\n\nheaders = ['Repository', 'Stars', 'Language']\n\ntable = tabulate(repos, headers=headers, tablefmt='grid')\n\nprint(table)\n\nexcept requests.exceptions.RequestException as e:\n\nprint(f\"Error fetching the page: {e}\")\n\nexcept Exception as e:\n\nprint(f\"An error occurred: {e}\")\n\nif __name__ == '__main__':\n\nmain()\n\n```\n\nThis script now includes try-except blocks to handle potential errors during the HTTP request and parsing.\n\nI should also consider adding user-agent headers to the HTTP request to mimic a browser request, as some websites block requests without a user-agent.\n\nHere's how to add a user-agent header:\n\n```python\n\nheaders = {\n\n'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n\nresponse = requests.get(url, headers=headers)\n\n```\n\nI'll update the script accordingly.\n\nFinally, I should test the script to ensure it retrieves and displays the data correctly.\n\n## Final Solution\nTo design a web scraper in Python that extracts the top 10 trending repositories on GitHub, along with their star count and primary programming language used, and outputs the results in a formatted table, follow the steps below.\n\n### Approach\n\n1. **Send an HTTP Request**: Retrieve the HTML content of GitHub's trending page.\n2. **Parse the HTML**: Use Beautiful Soup to extract repository names, star counts, and primary programming languages.\n3. **Format and Display Data**: Use the tabulate library to present the extracted data in a formatted table.\n\n### Solution Code\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nfrom tabulate import tabulate\n\ndef main():\n    try:\n        url = 'https://github.com/trending'\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        articles = soup.find_all('article', class_='Box-row')\n\n        repos = []\n        for article in articles[:10]:\n            # Extract repository name\n            repo_name = article.h1.a['href'].strip()\n            # Extract star count\n            stars = article.find('a', class_='Link--muted').text.strip()\n            # Extract primary programming language\n            language_span = article.find('span', itemprop='programmingLanguage')\n            language = language_span.text.strip() if language_span else 'N/A'\n            # Append to the list\n            repos.append({\n                'name': repo_name,\n                'stars': stars,\n                'language': language\n            })\n\n        # Print the table\n        headers = ['Repository', 'Stars', 'Language']\n        table = tabulate(repos, headers=headers, tablefmt='grid')\n        print(table)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching the page: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n### Explanation\n\n1. **HTTP Request**: The `requests` library is used to send a GET request to GitHub's trending page with a user-agent header to simulate a browser request.\n2. **HTML Parsing**: The `BeautifulSoup` library parses the HTML content, and repository data is extracted from the appropriate HTML elements.\n3. **Data Formatting**: The `tabulate` library is used to create a formatted table from the extracted data, making it easier to read.\n\nThis script provides a simple way to scrape and display trending GitHub repositories, handling potential errors gracefully."
    }
]