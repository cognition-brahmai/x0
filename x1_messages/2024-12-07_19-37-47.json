[
    {
        "role": "user",
        "content": "Implement a simple LRU (Least Recently Used) cache in Python that supports the following operations: get(key), put(key, value), and delete(key). The cache should have a maximum size specified during initialization, and automatically evict the least recently used item when it reaches its capacity.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Sure! Implementing an LRU cache in Python can be efficiently done using a combination of a dictionary and a doubly-linked list. The dictionary will provide O(1) access time for get and put operations, while the doubly-linked list will help in maintaining the order of elements based on their usage.\n\nHere's a step-by-step guide to implementing such a cache:\n\n### Step 1: Define the Node Class\nWe need a node class for the doubly-linked list that keeps track of the key, value, and references to the next and previous nodes.\n\n```python\nclass Node:\n    def __init__(self, key, value):\n        self.key = key\n        self.value = value\n        self.prev = None\n        self.next = None\n```\n\n### Step 2: Implement the LRUCache Class\nThis class will manage the cache operations and maintain the LRU order.\n\n```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.head = Node(0, 0)  # Dummy head node\n        self.tail = Node(0, 0)  # Dummy tail node\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _add_node(self, node):\n        # Add a new node right after the head\n        node.prev = self.head\n        node.next = self.head.next\n        self.head.next.prev = node\n        self.head.next = node\n\n    def _remove_node(self, node):\n        # Remove a node from the list\n        prev = node.prev\n        new_next = node.next\n        prev.next = new_next\n        new_next.prev = prev\n\n    def _move_to_head(self, node):\n        # Move a node to the head\n        self._remove_node(node)\n        self._add_node(node)\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            node = self.cache[key]\n            self._move_to_head(node)\n            return node.value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            node = self.cache[key]\n            node.value = value\n            self._move_to_head(node)\n        else:\n            new_node = Node(key, value)\n            self.cache[key] = new_node\n            self._add_node(new_node)\n            if len(self.cache) > self.capacity:\n                # Remove the tail\n                del self.cache[self.tail.prev.key]\n                self._remove_node(self.tail.prev)\n\n    def delete(self, key: int) -> None:\n        if key in self.cache:\n            node = self.cache[key]\n            self._remove_node(node)\n            del self.cache[key]\n```\n\n### Explanation\n1. **Node Class**: Represents each element in the doubly-linked list with references to the previous and next nodes.\n2. **LRUCache Class**:\n   - **Initialization**: Sets up the cache with a given capacity and creates dummy head and tail nodes to simplify additions and deletions.\n   - **_add_node**: Adds a new node right after the head.\n   - **_remove_node**: Removes a node from the list.\n   - **_move_to_head**: Moves a node to the head, indicating it was recently used.\n   - **get**: Retrieves a value by key and moves the corresponding node to the head.\n   - **put**: Adds or updates a key-value pair and moves the node to the head. If the cache exceeds capacity, the least recently used item (tail) is removed.\n   - **delete**: Removes a key from the cache and the corresponding node from the list.\n\nThis implementation ensures that all operations (get, put, delete) are performed in O(1) time complexity, making it efficient for scenarios where cache operations are frequent."
    }
]