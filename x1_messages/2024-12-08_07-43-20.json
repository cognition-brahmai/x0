[
    {
        "role": "user",
        "content": "Write a Python script to crawl a given website URL and extract all internal and external links found on the page. The script should accept command line arguments for the target URL and an optional maximum depth for crawling. Use the Beautiful Soup library for HTML parsing.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that crawls a website and extracts all the internal and external links. It should take a URL as a command line argument and optionally a maximum depth for how far it should crawl. And I'm supposed to use the Beautiful Soup library for parsing the HTML.\n\nFirst, I need to make sure I have Beautiful Soup installed. I can install it using pip if I don't have it already. So, in the terminal, I can run:\n\n```bash\npip install beautifulsoup4\n```\n\nAlso, I'll need to handle URLs, so I'll import the `requests` library to fetch the pages and `urllib.parse` to handle URL operations like joining relative URLs to base URLs.\n\nI should plan out how the script will work. It needs to accept command line arguments, so I'll use the `argparse` module for that. The user should provide a URL, and optionally, a maximum depth.\n\nNext, I need to think about how to crawl the pages. Since depth is involved, it sounds like a breadth-first search or depth-first search problem. I'll probably use a queue to handle the URLs to visit, especially since BFS is typically queue-based.\n\nI need to distinguish between internal and external links. To do that, I'll need to compare the base domain of the target URL with the domains of the links found on the page. If they match, it's internal; otherwise, it's external.\n\nI should also avoid crawling the same URL multiple times, so I'll need to keep a set of visited URLs.\n\nError handling is important too. For example, some URLs might not be accessible, or there might be infinite loops in redirection. So, I'll need to handle exceptions like timeouts and connection errors.\n\nLet me start by setting up the argument parser.\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description='Crawl a website and extract links.')\nparser.add_argument('url', help='The URL to start crawling from.')\nparser.add_argument('--max_depth', type=int, default=2, help='The maximum depth to crawl. Default is 2.')\nargs = parser.parse_args()\n```\n\nOkay, so the user provides a URL and an optional maximum depth. Now, I need to fetch the page at that URL.\n\nI'll use the `requests` library for that.\n\n```python\nimport requests\n\nresponse = requests.get(args.url)\nif response.status_code != 200:\n    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n    exit()\n```\n\nAssuming the page is fetched successfully, I can parse it with Beautiful Soup.\n\n```python\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n```\n\nNow, I need to find all the links in the page. Links are typically in `<a>` tags, so I'll find all `<a>` tags and extract their `href` attributes.\n\n```python\nlinks = soup.find_all('a', href=True)\nfor link in links:\n    print(link['href'])\n```\n\nBut these URLs might be relative. I need to convert them to absolute URLs using the base URL.\n\n```python\nfrom urllib.parse import urljoin\n\nbase_url = args.url\nabsolute_links = [urljoin(base_url, link['href']) for link in links]\n```\n\nNow, I need to determine which ones are internal and which are external. To do that, I need to compare the domains.\n\nI can use `urlparse` from `urllib.parse` to get the netloc (domain) part of the URL.\n\n```python\nfrom urllib.parse import urlparse\n\nbase_domain = urlparse(base_url).netloc\ninternal_links = []\nexternal_links = []\n\nfor link in absolute_links:\n    domain = urlparse(link).netloc\n    if domain == base_domain:\n        internal_links.append(link)\n    else:\n        external_links.append(link)\n```\n\nThis takes care of the initial page. Now, I need to crawl deeper up to the specified depth.\n\nI'll use a queue to manage the URLs to visit, along with their current depth.\n\nI need to keep track of visited URLs to avoid duplicates.\n\n```python\nfrom collections import deque\n\nvisited = set()\nqueue = deque([(args.url, 0)])  # (url, current_depth)\n\nwhile queue:\n    url, depth = queue.popleft()\n    if url in visited:\n        continue\n    visited.add(url)\n    \n    if depth > args.max_depth:\n        continue\n    \n    try:\n        response = requests.get(url)\n        if response.status_code != 200:\n            continue\n    except requests.RequestException as e:\n        print(\"Error fetching\", url, \":\", str(e))\n        continue\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = soup.find_all('a', href=True)\n    absolute_links = [urljoin(url, link['href']) for link in links]\n    \n    for link in absolute_links:\n        domain = urlparse(link).netloc\n        if domain == base_domain:\n            internal_links.append(link)\n            if depth + 1 <= args.max_depth:\n                queue.append((link, depth + 1))\n        else:\n            external_links.append(link)\n```\n\nThis should crawl the site up to the specified depth, collecting internal and external links along the way.\n\nI should probably print or do something with these links at the end.\n\nMaybe write them to a file or just print them.\n\n```python\nprint(\"Internal links:\")\nfor link in internal_links:\n    print(link)\n\nprint(\"\\nExternal links:\")\nfor link in external_links:\n    print(link)\n```\n\nI should also consider rate limiting to be nice to the server I'm crawling. I can add a delay between requests.\n\n```python\nimport time\n\ntime.sleep(1)  # Sleep for 1 second between requests\n```\n\nAdditionally, some links might be fragments or point to the same page. I should handle that.\n\nAlso, I should handle URL normalization to canonicalize URLs.\n\nMaybe use the `canonical` tag in HTML to get the preferred version of the URL.\n\nThis is getting a bit complex, but for a basic script, the above should suffice.\n\nLet me think about potential issues.\n\n1. Infinite loops: Prevented by using a set of visited URLs.\n\n2. Relative URLs: Handled by `urljoin`.\n\n3. Different protocols: HTTP vs HTTPS. `urljoin` should handle this.\n\n4. Non-HTTP URLs: Like mailto:, ftp:, etc. I might want to skip those.\n\n5. JavaScript-generated links: Beautiful Soup won't see those.\n\n6. Robots.txt: I should check if crawling is allowed.\n\nFor robots.txt, I can use the `robotparser` module.\n\n```python\nfrom urllib.robotparser import RobotFileParser\n\nrp = RobotFileParser()\nrp.set_url(urlparse(args.url).scheme + '//' + urlparse(args.url).netloc + '/robots.txt')\nrp.read()\nif not rp.can_fetch('*', args.url):\n    print(\"Crawling not allowed by robots.txt\")\n    exit()\n```\n\nI should check this before starting the crawl.\n\nAlso, I should handle different content types. Maybe the page is not HTML.\n\nI can check the content type in the response headers.\n\nBut for simplicity, I'll assume it's HTML.\n\nAnother consideration is handling query parameters and hashes.\n\nI might want to normalize URLs by removing unnecessary parameters.\n\nThis is getting quite involved. For a basic script, I'll keep it as is.\n\nLet me write the complete script now.\n\n## Final Solution\nTo crawl a given website URL and extract all internal and external links found on the page, you can use the following Python script. This script accepts command line arguments for the target URL and an optional maximum depth for crawling. It uses the Beautiful Soup library for HTML parsing.\n\n### Solution Code\n\n```python\nimport argparse\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse, urljoin\nfrom collections import deque\n\ndef main():\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description='Crawl a website and extract links.')\n    parser.add_argument('url', help='The URL to start crawling from.')\n    parser.add_argument('--max_depth', type=int, default=2, help='The maximum depth to crawl. Default is 2.')\n    args = parser.parse_args()\n\n    # Check robots.txt\n    rp = RobotFileParser()\n    rp.set_url(urlparse(args.url).scheme + '://' + urlparse(args.url).netloc + '/robots.txt')\n    rp.read()\n    if not rp.can_fetch('*', args.url):\n        print(\"Crawling not allowed by robots.txt\")\n        return\n\n    # Initialize variables\n    base_domain = urlparse(args.url).netloc\n    visited = set()\n    queue = deque([(args.url, 0)])  # (url, current_depth)\n    internal_links = []\n    external_links = []\n\n    while queue:\n        url, depth = queue.popleft()\n        if url in visited:\n            continue\n        visited.add(url)\n        \n        if depth > args.max_depth:\n            continue\n        \n        try:\n            response = requests.get(url)\n            if response.status_code != 200:\n                continue\n        except requests.RequestException as e:\n            print(\"Error fetching\", url, \":\", str(e))\n            continue\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        links = soup.find_all('a', href=True)\n        absolute_links = [urljoin(url, link['href']) for link in links]\n        \n        for link in absolute_links:\n            domain = urlparse(link).netloc\n            if domain == base_domain:\n                internal_links.append(link)\n                if depth + 1 <= args.max_depth:\n                    queue.append((link, depth + 1))\n            else:\n                external_links.append(link)\n        \n        # Be nice: sleep for 1 second before next request\n        time.sleep(1)\n\n    # Print the results\n    print(\"Internal links:\")\n    for link in internal_links:\n        print(link)\n\n    print(\"\\nExternal links:\")\n    for link in external_links:\n        print(link)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n1. **Argument Parsing**: The script uses `argparse` to accept the target URL and an optional maximum depth for crawling.\n2. **Robots.txt Compliance**: It checks if crawling is allowed by parsing the target site's `robots.txt` file.\n3. **URL Queue and Visited Set**: A queue is used to manage URLs to be processed, along with their crawling depth, and a set tracks visited URLs to prevent duplication.\n4. **HTML Parsing and Link Extraction**: Using Beautiful Soup, the script extracts all `<a>` tags and their `href` attributes, converts relative URLs to absolute ones, and classifies them as internal or external based on the domain.\n5. **Crawling with Depth Control**: The script crawls pages up to the specified depth, enqueueing new internal links for further processing.\n6. **Politeness**: Includes a 1-second delay between requests to avoid overloading the server.\n\nThis script provides a basic web crawler that respects `robots.txt`, handles both internal and external links, and limits its depth of crawling."
    }
]