[
    {
        "role": "user",
        "content": "Develop a Python script that uses TensorFlow to train a neural network on the MNIST dataset of handwritten digits. The program should accept command-line arguments for the number of training epochs and batch size, and display the accuracy of the trained model on the test set."
    },
    {
        "role": "assistant",
        "content": "I'm here to help you with your Python script using TensorFlow to train a neural network on the MNIST dataset. Let's break this down step by step.\n\nFirst, you'll need to make sure you have TensorFlow installed. If you haven't installed it yet, you can do so using pip:\n\n```bash\npip install tensorflow\n```\n\nOnce TensorFlow is installed, you can start by importing the necessary modules and loading the MNIST dataset. TensorFlow has the MNIST dataset built-in, which makes it convenient to use.\n\nHere's a basic outline of what your script should do:\n\n1. Parse command-line arguments for the number of training epochs and batch size.\n2. Load the MNIST dataset.\n3. Preprocess the data (normalize the images and one-hot encode the labels).\n4. Define the neural network model.\n5. Compile the model with an optimizer, loss function, and metrics.\n6. Train the model using the training data.\n7. Evaluate the model using the test data and display the accuracy.\n\nLet's start by writing the code to parse command-line arguments. You can use the `argparse` module in Python for this purpose.\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description='Train a neural network on MNIST.')\nparser.add_argument('--epochs', type=int, default=5, help='Number of training epochs')\nparser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\nargs = parser.parse_args()\n```\n\nNext, import TensorFlow and load the MNIST dataset.\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\n# Load MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n```\n\nPreprocess the data. Normalize the images to have pixel values between 0 and 1, and one-hot encode the labels.\n\n```python\n# Normalize images\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# One-hot encode labels\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n```\n\nNow, define the neural network model. For simplicity, let's use a sequential model with a few dense layers.\n\n```python\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n```\n\nCompile the model, specifying the optimizer, loss function, and metrics to track.\n\n```python\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```\n\nTrain the model using the training data, specifying the number of epochs and batch size from the command-line arguments.\n\n```python\nmodel.fit(x_train, y_train,\n          batch_size=args.batch_size,\n          epochs=args.epochs,\n          validation_split=0.1)\n```\n\nFinally, evaluate the model on the test set and display the accuracy.\n\n```python\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc}')\n```\n\nPutting it all together, your script should look like this:\n\n```python\nimport argparse\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\n# Parse command-line arguments\nparser = argparse.ArgumentParser(description='Train a neural network on MNIST.')\nparser.add_argument('--epochs', type=int, default=5, help='Number of training epochs')\nparser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\nargs = parser.parse_args()\n\n# Load MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess data\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# Define the model\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train,\n          batch_size=args.batch_size,\n          epochs=args.epochs,\n          validation_split=0.1)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc}')\n```\n\nYou can run this script from the command line and specify the number of epochs and batch size, like so:\n\n```bash\npython mnist_trainer.py --epochs 10 --batch_size 64\n```\n\nThis should train the neural network on the MNIST dataset with the specified parameters and display the test accuracy upon completion.\n\n## Final Solution\nTo create a Python script that uses TensorFlow to train a neural network on the MNIST dataset of handwritten digits, accepting command-line arguments for the number of training epochs and batch size, and displaying the accuracy of the trained model on the test set, follow these steps:\n\n1. **Install TensorFlow**:\n   Ensure TensorFlow is installed. If not, install it using:\n   ```bash\n   pip install tensorflow\n   ```\n\n2. **Import Necessary Modules**:\n   Import required modules including TensorFlow, argparse for command-line arguments, and utilities from TensorFlow.\n\n3. **Parse Command-Line Arguments**:\n   Use `argparse` to accept command-line arguments for epochs and batch size.\n\n4. **Load and Preprocess MNIST Data**:\n   - Load the MNIST dataset.\n   - Normalize pixel values to [0,1].\n   - One-hot encode labels.\n\n5. **Define and Compile the Neural Network Model**:\n   - Create a sequential model with flatten, dense layers.\n   - Compile the model with Adam optimizer and categorical crossentropy loss.\n\n6. **Train the Model**:\n   - Use the training data with specified batch size and epochs.\n\n7. **Evaluate and Display Test Accuracy**:\n   - Evaluate the model on the test set and print the accuracy.\n\n### Solution Code\n\n```python\nimport argparse\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\n# Parse command-line arguments\nparser = argparse.ArgumentParser(description='Train a neural network on MNIST.')\nparser.add_argument('--epochs', type=int, default=5, help='Number of training epochs')\nparser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\nargs = parser.parse_args()\n\n# Load MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess data\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# Define the model\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train,\n          batch_size=args.batch_size,\n          epochs=args.epochs,\n          validation_split=0.1)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc}')\n```\n\n### Explanation\n\n1. **Argument Parsing**:\n   - Use `argparse` to handle command-line inputs for epochs and batch size.\n\n2. **Data Loading and Preprocessing**:\n   - Load MNIST data using `mnist.load_data()`.\n   - Normalize pixel values to range [0,1].\n   - Convert labels to one-hot encoded vectors.\n\n3. **Model Definition**:\n   - Create a sequential model with input flattening and fully connected layers.\n   - Use ReLU activation for hidden layers and softmax for the output layer.\n\n4. **Model Compilation**:\n   - Compile the model with Adam optimizer and categorical crossentropy loss.\n\n5. **Training**:\n   - Train the model using specified epochs and batch size, with 10% of the training data reserved for validation.\n\n6. **Evaluation**:\n   - Evaluate the trained model on the test set and print the accuracy."
    }
]