[
    {
        "role": "user",
        "content": "Create a Python script that interacts with the Twitter API to retrieve tweets containing a specific hashtag, processes the text using NLTK for sentiment analysis, and visualizes the results using Matplotlib.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Alright, I've got this task to create a Python script that interacts with the Twitter API to fetch tweets containing a specific hashtag, performs sentiment analysis on the tweet text using NLTK, and then visualizes the results using Matplotlib. Sounds like a fun project! Let's break it down into manageable steps.\n\nFirst, I need to access the Twitter API. To do that, I'll have to set up a developer account on Twitter and create an app to get the necessary API keys and access tokens. Once I have those, I can use the Tweepy library in Python, which is a handy wrapper for the Twitter API.\n\nNext, I need to search for tweets containing a specific hashtag. Tweepy should make this straightforward. I can use the Cursor object to paginate through the results and collect a good number of tweets.\n\nOnce I have the tweets, I'll need to process their text for sentiment analysis. That's where NLTK comes in. I should install NLTK and download the necessary corpora and tokenizers. Then, I can use NLTK's sentiment analysis tools, perhaps the VADER sentimentIntensityAnalyzer, which is specifically tuned for social media text.\n\nAfter analyzing the sentiment of each tweet, I'll collect the results and use Matplotlib to visualize them. Maybe a bar chart showing the distribution of positive, negative, and neutral sentiments.\n\nLet me think about the structure of the script. I should start by importing all the necessary libraries: Tweepy for Twitter API access, NLTK for sentiment analysis, and Matplotlib for visualization.\n\nThen, I need to set up the Twitter API credentials. I'll have to define variables for the consumer key, consumer secret, access token, and access token secret. It's important to keep these secure and not hardcode them into the script. Maybe I can use environment variables or a separate config file.\n\nOnce the credentials are set, I can authenticate with the Twitter API using Tweepy's OAuthHandler.\n\nAfter that, I'll define the hashtag I want to search for. Maybe make it configurable so that the user can input any hashtag they're interested in.\n\nThen, I'll use Tweepy's Cursor to fetch the tweets containing that hashtag. I should consider rate limits and possibly handle pagination to get more tweets.\n\nOnce I have the tweets, I'll extract the text and process it with NLTK. I'll initialize the SentimentIntensityAnalyzer from NLTK and use it to get the sentiment scores for each tweet.\n\nI should categorize each tweet into positive, negative, or neutral based on the compound score from VADER. Maybe set a threshold for what constitutes positive, negative, or neutral.\n\nAfter analyzing all the tweets, I'll count the number of tweets in each category and then use Matplotlib to plot a bar chart showing the distribution.\n\nLet me sketch a rough outline of the script:\n\n1. Import necessary libraries: tweepy, nltk, matplotlib\n\n2. Set up Twitter API credentials (perhaps using environment variables)\n\n3. Authenticate with Twitter using OAuthHandler\n\n4. Define the hashtag to search for\n\n5. Use Tweepy Cursor to fetch tweets containing the hashtag\n\n6. Process each tweet's text with NLTK's SentimentIntensityAnalyzer\n\n7. Categorize sentiment of each tweet (positive, negative, neutral)\n\n8. Count tweets in each category\n\n9. Use Matplotlib to plot a bar chart of the counts\n\nThat seems straightforward. Now, let's think about potential pitfalls.\n\nFirst, handling Twitter's rate limits. I need to be careful not to exceed the API rate limits. Tweepy's Cursor handles pagination, but I still need to manage how many requests I'm making.\n\nSecond, dealing with tweet text. Tweets can contain URLs, mentions, emojis, etc. I might need to preprocess the text to clean it up before feeding it into NLTK.\n\nThird, sentiment analysis accuracy. VADER is good for social media text, but it's not perfect. I should keep in mind that sentiment analysis on social media can be tricky due to sarcasm, irony, and context.\n\nFourth, error handling. The script should handle cases where the hashtag has no tweets, or if there are issues with API connectivity.\n\nFifth, performance. If I try to fetch a huge number of tweets, it might take time to process them all. I should consider optimizing the script if needed.\n\nAlright, let's start by setting up the environment. I need to make sure I have Python installed, and then install the required libraries: Tweepy, NLTK, and Matplotlib.\n\nI can install them using pip:\n\n```\n\npip install tweepy nltk matplotlib\n\n```\n\nNext, I need to set up a Twitter developer account and create an app to get the API keys and access tokens.\n\nOnce I have those, I can proceed to write the script.\n\nLet me start by importing the necessary libraries:\n\n```python\n\nimport tweepy\n\nimport nltk\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport matplotlib.pyplot as plt\n\n```\n\nI should also download the necessary NLTK data. For VADER, I need the 'vader_lexicon':\n\n```python\n\nnltk.download('vader_lexicon')\n\n```\n\nNow, setting up the Twitter API credentials. As mentioned earlier, it's best not to hardcode them. I can use environment variables or a config file. For simplicity, I'll assume they are stored in environment variables.\n\nFirst, I need to import the os module to access environment variables:\n\n```python\n\nimport os\n\n```\n\nThen, define the credentials:\n\n```python\n\nconsumer_key = os.environ.get('TWITTER_CONSUMER_KEY')\n\nconsumer_secret = os.environ.get('TWITTER_CONSUMER_SECRET')\n\naccess_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n\naccess_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n\n```\n\nNext, authenticate with Twitter:\n\n```python\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth)\n\n```\n\nNow, define the hashtag to search for. I'll make this configurable by using an input function:\n\n```python\n\nhashtag = input(\"Enter the hashtag to search for: \")\n\n```\n\nAlternatively, for simplicity, I can hardcode it for now:\n\n```python\n\nhashtag = \"#example\"\n\n```\n\nThen, use Tweepy's Cursor to fetch tweets containing the hashtag. I'll fetch, say, 100 tweets:\n\n```python\n\ncursor = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\").items(100)\n\n```\n\nI'll store the tweets in a list:\n\n```python\n\ntweets = [tweet.text for tweet in cursor]\n\n```\n\nNow, initialize the SentimentIntensityAnalyzer from NLTK:\n\n```python\n\nsia = SentimentIntensityAnalyzer()\n\n```\n\nThen, analyze the sentiment of each tweet:\n\n```python\n\nsentiments = []\n\nfor tweet in tweets:\n\nscore = sia.polarity_scores(tweet)\n\ncompound = score['compound']\n\nif compound >= 0.05:\n\nsentiments.append('positive')\n\nelif compound <= -0.05:\n\nsentiments.append('negative')\n\nelse:\n\nsentiments.append('neutral')\n\n```\n\nHere, I'm using a threshold of \u00b10.05 to categorize the sentiment. Adjusting these thresholds can affect the distribution.\n\nNext, count the number of tweets in each category:\n\n```python\n\nfrom collections import Counter\n\ncounts = Counter(sentiments)\n\n```\n\nFinally, plot the results using Matplotlib:\n\n```python\n\nlabels = ['Positive', 'Negative', 'Neutral']\n\nsizes = [counts['positive'], counts['negative'], counts['neutral']]\n\ncolors = ['green', 'red', 'blue']\n\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n\nplt.axis('equal')\n\nplt.show()\n\n```\n\nAlternatively, for a bar chart:\n\n```python\n\nplt.bar(labels, sizes, color=colors)\n\nplt.xlabel('Sentiment')\n\nplt.ylabel('Number of Tweets')\n\nplt.title(f'Sentiment Analysis of Tweets containing {hashtag}')\n\nplt.show()\n\n```\n\nThat should do it for a basic script. Now, let's think about improving it.\n\nFirst, error handling. I should add try-except blocks to handle potential errors, such as authentication issues or rate limits.\n\nSecond, preprocessing the tweet text. I can create a function to clean the text by removing URLs, mentions, special characters, etc.\n\nThird, handling larger volumes of data. If I want to analyze more tweets, I need to make sure the script can handle it efficiently.\n\nFourth, saving the results to a file, perhaps for further analysis.\n\nFifth, making the script more interactive, allowing the user to choose different hashtags or parameters.\n\nLet me start by adding some basic error handling.\n\nWrap the authentication and tweet fetching in try-except blocks:\n\n```python\n\ntry:\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth)\n\nexcept tweepy.TweepError as e:\n\nprint(f\"Authentication error: {e}\")\n\nexit()\n\ntry:\n\ncursor = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\").items(100)\n\ntweets = [tweet.text for tweet in cursor]\n\nexcept tweepy.TweepError as e:\n\nprint(f\"Error fetching tweets: {e}\")\n\nexit()\n\n```\n\nNext, create a function to clean the tweet text:\n\n```python\n\nimport re\n\ndef clean_tweet(tweet):\n\n# Remove URLs\n\ntweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n\n# Remove mentions\n\ntweet = re.sub(r'\\@\\w+', '', tweet)\n\n# Remove hashtags\n\ntweet = re.sub(r'\\#\\w+', '', tweet)\n\n# Remove punctuation and special characters\n\ntweet = re.sub(r'[^\\w\\s]', '', tweet)\n\n# Convert to lowercase\n\ntweet = tweet.lower()\n\nreturn tweet\n\n```\n\nThen, apply this function to each tweet before analyzing its sentiment:\n\n```python\n\ncleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n\nsentiments = []\n\nfor tweet in cleaned_tweets:\n\nscore = sia.polarity_scores(tweet)\n\ncompound = score['compound']\n\nif compound >= 0.05:\n\nsentiments.append('positive')\n\nelif compound <= -0.05:\n\nsentiments.append('negative')\n\nelse:\n\nsentiments.append('neutral')\n\n```\n\nThis should improve the accuracy of the sentiment analysis by removing noise from the tweet text.\n\nAlso, I can add some logging or print statements to show the progress of the script.\n\nFor example, after fetching the tweets:\n\n```python\n\nprint(f\"Fetched {len(tweets)} tweets containing {hashtag}\")\n\n```\n\nAnd after analyzing the sentiments:\n\n```python\n\nprint(\"Sentiment analysis complete.\")\n\n```\n\nAdditionally, I can allow the user to specify the number of tweets to fetch, rather than hardcoding it to 100.\n\nModify the script to accept user input for the number of tweets:\n\n```python\n\nnum_tweets = int(input(\"Enter the number of tweets to fetch: \"))\n\ncursor = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\").items(num_tweets)\n\n```\n\nBut, I need to make sure that the number doesn't exceed Twitter's rate limits.\n\nAlternatively, I can set a maximum limit to prevent exceeding rate limits.\n\nAlso, I should consider sleeping between requests to avoid hitting the rate limits.\n\nTweepy's Cursor handles pagination, but I can add a delay using time.sleep.\n\nImport the time module:\n\n```python\n\nimport time\n\n```\n\nThen, after fetching a batch of tweets, pause for a bit:\n\n```python\n\ntweets = []\n\nfor tweet in cursor:\n\ntweets.append(tweet.text)\n\ntime.sleep(1)  # Pause for 1 second between requests\n\n```\n\nThis can help prevent hitting rate limits, but it will slow down the script.\n\nAnother improvement could be to handle retweets differently. Maybe exclude them or handle their text appropriately.\n\nI can check if a tweet is a retweet and handle it accordingly.\n\nModify the tweet fetching loop:\n\n```python\n\ntweets = []\n\nfor tweet in cursor:\n\nif not tweet.retweeted and ('RT @' not in tweet.text):\n\ntweets.append(tweet.text)\n\ntime.sleep(1)\n\n```\n\nThis should exclude retweets from the analysis.\n\nAlso, I can handle quotes differently if needed.\n\nFurthermore, I can expand the script to analyze multiple hashtags or compare sentiments across different hashtags.\n\nBut for now, let's stick to a single hashtag.\n\nLet me think about saving the results to a file. Perhaps save the tweet text and its sentiment category to a CSV file.\n\nFirst, import the csv module:\n\n```python\n\nimport csv\n\n```\n\nThen, after analyzing the sentiments, write the results to a CSV:\n\n```python\n\nwith open('tweets_sentiment.csv', mode='w', newline='', encoding='utf-8') as file:\n\nwriter = csv.writer(file)\n\nwriter.writerow(['Tweet', 'Sentiment'])\n\nfor i in range(len(cleaned_tweets)):\n\nwriter.writerow([cleaned_tweets[i], sentiments[i]])\n\n```\n\nThis way, the user can review the individual tweets and their sentiment classifications.\n\nAlso, I can read the hashtag from a file or a list, allowing for batch processing of multiple hashtags.\n\nBut for this script, I'll keep it simple and focus on a single hashtag.\n\nLastly, I can add a command-line interface using argparse to make the script more user-friendly.\n\nBut for now, using input functions should suffice.\n\nLet me summarize the final script structure:\n\n1. Import necessary libraries\n\n2. Download NLTK data\n\n3. Set up Twitter API credentials from environment variables\n\n4. Authenticate with Twitter\n\n5. Get user input for hashtag and number of tweets\n\n6. Fetch tweets containing the hashtag, excluding retweets\n\n7. Clean the tweet text\n\n8. Analyze sentiment of each cleaned tweet\n\n9. Categorize sentiments into positive, negative, or neutral\n\n10. Count tweets in each category\n\n11. Plot a bar chart using Matplotlib\n\n12. Save the results to a CSV file\n\nI should also add comments to the script for clarity and include a docstring at the beginning explaining what the script does.\n\nLet me write a docstring:\n\n```python\n\n\"\"\"\n\nThis script interacts with the Twitter API to retrieve tweets containing a specific hashtag,\n\nprocesses the text using NLTK for sentiment analysis, and visualizes the results using Matplotlib.\n\nMake sure to set up Twitter API credentials as environment variables:\n\n- TWITTER_CONSUMER_KEY\n\n- TWITTER_CONSUMER_SECRET\n\n- TWITTER_ACCESS_TOKEN\n\n- TWITTER_ACCESS_TOKEN_SECRET\n\nAlso, install required libraries:\n\n- tweepy\n\n- nltk\n\n- matplotlib\n\n\"\"\"\n\n```\n\nNow, putting it all together, here's the complete script:\n\n```python\n\n\"\"\"\n\nThis script interacts with the Twitter API to retrieve tweets containing a specific hashtag,\n\nprocesses the text using NLTK for sentiment analysis, and visualizes the results using Matplotlib.\n\nMake sure to set up Twitter API credentials as environment variables:\n\n- TWITTER_CONSUMER_KEY\n\n- TWITTER_CONSUMER_SECRET\n\n- TWITTER_ACCESS_TOKEN\n\n- TWITTER_ACCESS_TOKEN_SECRET\n\nAlso, install required libraries:\n\n- tweepy\n\n- nltk\n\n- matplotlib\n\n\"\"\"\n\nimport tweepy\n\nimport nltk\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport matplotlib.pyplot as plt\n\nimport os\n\nimport time\n\nimport csv\n\nimport re\n\n# Download necessary NLTK data\n\nnltk.download('vader_lexicon')\n\n# Set up Twitter API credentials from environment variables\n\nconsumer_key = os.environ.get('TWITTER_CONSUMER_KEY')\n\nconsumer_secret = os.environ.get('TWITTER_CONSUMER_SECRET')\n\naccess_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n\naccess_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n\n# Authenticate with Twitter\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth)\n\n# Function to clean tweet text\n\ndef clean_tweet(tweet):\n\n# Remove URLs\n\ntweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n\n# Remove mentions\n\ntweet = re.sub(r'\\@\\w+', '', tweet)\n\n# Remove hashtags\n\ntweet = re.sub(r'\\#\\w+', '', tweet)\n\n# Remove punctuation and special characters\n\ntweet = re.sub(r'[^\\w\\s]', '', tweet)\n\n# Convert to lowercase\n\ntweet = tweet.lower()\n\nreturn tweet\n\n# Get user input for hashtag and number of tweets\n\nhashtag = input(\"Enter the hashtag to search for: \")\n\nnum_tweets = int(input(\"Enter the number of tweets to fetch: \"))\n\n# Fetch tweets containing the hashtag, excluding retweets\n\ntweets = []\n\nfor tweet in tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\").items(num_tweets):\n\nif not tweet.retweeted and ('RT @' not in tweet.text):\n\ntweets.append(tweet.text)\n\ntime.sleep(1)  # Pause to avoid rate limits\n\nprint(f\"Fetched {len(tweets)} tweets containing {hashtag}\")\n\n# Clean the tweet text\n\ncleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n\n# Initialize SentimentIntensityAnalyzer\n\nsia = SentimentIntensityAnalyzer()\n\n# Analyze sentiment of each tweet\n\nsentiments = []\n\nfor tweet in cleaned_tweets:\n\nscore = sia.polarity_scores(tweet)\n\ncompound = score['compound']\n\nif compound >= 0.05:\n\nsentiments.append('positive')\n\nelif compound <= -0.05:\n\nsentiments.append('negative')\n\nelse:\n\nsentiments.append('neutral')\n\nprint(\"Sentiment analysis complete.\")\n\n# Count tweets in each category\n\nfrom collections import Counter\n\ncounts = Counter(sentiments)\n\n# Plotting the results\n\nlabels = ['Positive', 'Negative', 'Neutral']\n\nsizes = [counts['positive'], counts['negative'], counts['neutral']]\n\ncolors = ['green', 'red', 'blue']\n\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n\nplt.axis('equal')\n\nplt.title(f'Sentiment Analysis of Tweets containing {hashtag}')\n\nplt.show()\n\n# Save results to a CSV file\n\nwith open('tweets_sentiment.csv', mode='w', newline='', encoding='utf-8') as file:\n\nwriter = csv.writer(file)\n\nwriter.writerow(['Tweet', 'Sentiment'])\n\nfor i in range(len(cleaned_tweets)):\n\nwriter.writerow([cleaned_tweets[i], sentiments[i]])\n\nprint(\"Results saved to tweets_sentiment.csv\")\n\n```\n\nThis script should accomplish the task as described. It interacts with the Twitter API to fetch tweets containing a specific hashtag, processes the text for sentiment analysis using NLTK, visualizes the results with Matplotlib, and saves the detailed results to a CSV file.\n\n## Final Solution\nTo create a Python script that interacts with the Twitter API to retrieve tweets containing a specific hashtag, processes the text using NLTK for sentiment analysis, and visualizes the results using Matplotlib, follow the steps below.\n\n### Prerequisites\n1. **Twitter Developer Account**: Set up a Twitter developer account and create an app to get API keys and access tokens.\n2. **Python Libraries**: Install the required libraries using pip:\n   ```bash\n   pip install tweepy nltk matplotlib\n   ```\n3. **Environment Variables**: Set environment variables for Twitter API credentials:\n   - `TWITTER_CONSUMER_KEY`\n   - `TWITTER_CONSUMER_SECRET`\n   - `TWITTER_ACCESS_TOKEN`\n   - `TWITTER_ACCESS_TOKEN_SECRET`\n\n### Solution Code\n\n```python\n\"\"\"\nThis script interacts with the Twitter API to retrieve tweets containing a specific hashtag,\nprocesses the text using NLTK for sentiment analysis, and visualizes the results using Matplotlib.\n\nMake sure to set up Twitter API credentials as environment variables:\n- TWITTER_CONSUMER_KEY\n- TWITTER_CONSUMER_SECRET\n- TWITTER_ACCESS_TOKEN\n- TWITTER_ACCESS_TOKEN_SECRET\n\nAlso, install required libraries:\n- tweepy\n- nltk\n- matplotlib\n\"\"\"\n\nimport tweepy\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport csv\nimport re\n\n# Download necessary NLTK data\nnltk.download('vader_lexicon')\n\n# Set up Twitter API credentials from environment variables\nconsumer_key = os.environ.get('TWITTER_CONSUMER_KEY')\nconsumer_secret = os.environ.get('TWITTER_CONSUMER_SECRET')\naccess_token = os.environ.get('TWITTER_ACCESS_TOKEN')\naccess_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n\n# Authenticate with Twitter\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)\n\n# Function to clean tweet text\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove mentions\n    tweet = re.sub(r'\\@\\w+', '', tweet)\n    # Remove hashtags\n    tweet = re.sub(r'\\#\\w+', '', tweet)\n    # Remove punctuation and special characters\n    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n    # Convert to lowercase\n    tweet = tweet.lower()\n    return tweet\n\n# Get user input for hashtag and number of tweets\nhashtag = input(\"Enter the hashtag to search for: \")\nnum_tweets = int(input(\"Enter the number of tweets to fetch: \"))\n\n# Fetch tweets containing the hashtag, excluding retweets\ntweets = []\nfor tweet in tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\").items(num_tweets):\n    if not tweet.retweeted and ('RT @' not in tweet.text):\n        tweets.append(tweet.text)\n    time.sleep(1)  # Pause to avoid rate limits\nprint(f\"Fetched {len(tweets)} tweets containing {hashtag}\")\n\n# Clean the tweet text\ncleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n\n# Initialize SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\n# Analyze sentiment of each tweet\nsentiments = []\nfor tweet in cleaned_tweets:\n    score = sia.polarity_scores(tweet)\n    compound = score['compound']\n    if compound >= 0.05:\n        sentiments.append('positive')\n    elif compound <= -0.05:\n        sentiments.append('negative')\n    else:\n        sentiments.append('neutral')\nprint(\"Sentiment analysis complete.\")\n\n# Count tweets in each category\nfrom collections import Counter\ncounts = Counter(sentiments)\n\n# Plotting the results\nlabels = ['Positive', 'Negative', 'Neutral']\nsizes = [counts['positive'], counts['negative'], counts['neutral']]\ncolors = ['green', 'red', 'blue']\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')\nplt.title(f'Sentiment Analysis of Tweets containing {hashtag}')\nplt.show()\n\n# Save results to a CSV file\nwith open('tweets_sentiment.csv', mode='w', newline='', encoding='utf-8') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Tweet', 'Sentiment'])\n    for i in range(len(cleaned_tweets)):\n        writer.writerow([cleaned_tweets[i], sentiments[i]])\nprint(\"Results saved to tweets_sentiment.csv\")\n```\n\n### Explanation\n1. **Twitter API Setup**: Authenticate using Tweepy with credentials from environment variables.\n2. **Fetch Tweets**: Use Tweepy\u2019s Cursor to fetch tweets containing the specified hashtag, excluding retweets.\n3. **Text Cleaning**: Preprocess tweet text by removing URLs, mentions, hashtags, and special characters.\n4. **Sentiment Analysis**: Use NLTK\u2019s SentimentIntensityAnalyzer to categorize tweet sentiments.\n5. **Visualization**: Use Matplotlib to plot a pie chart showing the distribution of sentiments.\n6. **Save Results**: Store tweet text and sentiment categories in a CSV file for further analysis."
    }
]