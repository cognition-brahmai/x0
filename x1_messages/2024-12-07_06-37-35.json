[
    {
        "role": "user",
        "content": "Implement a simple LRU (Least Recently Used) cache in Python with a fixed size. The cache should support get and put operations, evicting the least recently used item when the cache is full."
    },
    {
        "role": "assistant",
        "content": "Sure, I can help you with that! Implementing an LRU cache in Python is a great way to understand both data structures and how to manage memory efficiently. An LRU cache is a type of cache that removes the least recently used items first when it reaches its capacity. This is particularly useful in scenarios where you have a limited amount of fast memory and you need to decide which data to keep in it based on usage patterns.\n\nTo implement this, we'll use a combination of a dictionary and a doubly linked list. The dictionary will provide us with O(1) access time for get operations, and the doubly linked list will allow us to efficiently move elements to the front (for recently used items) and remove elements from the end (for least recently used items), both in O(1) time.\n\nHere's a step-by-step plan:\n\n1. **Node Class**: Create a class to represent nodes in the doubly linked list. Each node will hold a key and a value, and pointers to the next and previous nodes.\n\n2. **Doubly Linked List**: Implement methods to add nodes to the head and remove nodes from the tail of the list. Also, implement a method to move a node to the head, which indicates it's been recently used.\n\n3. **LRUCache Class**: This will be the main class that uses the dictionary and the doubly linked list.\n   - **Constructor**: Initialize the cache with a given capacity and set up the head and tail of the linked list as dummy nodes to simplify operations.\n   - **Get Operation**: If the key exists in the dictionary, return the value and move the corresponding node to the head of the list.\n   - **Put Operation**: If the key already exists, update the value and move the node to the head. If the key doesn't exist, create a new node, add it to the head, and add it to the dictionary. If the cache is full, remove the least recently used item from the tail of the list and delete it from the dictionary.\n\nLet's dive into the code. I'll write it step by step, explaining each part.\n\nFirst, we define the Node class:\n\n```python\nclass Node:\n    def __init__(self, key, value):\n        self.key = key\n        self.value = value\n        self.prev = None\n        self.next = None\n```\n\nThis is straightforward. Each node has a key, a value, and pointers to the previous and next nodes.\n\nNext, we'll implement the LRUCache class:\n\n```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}  # dictionary to store key -> Node mappings\n        self.head = Node(0, 0)  # dummy head node\n        self.tail = Node(0, 0)  # dummy tail node\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _add_node(self, node):\n        # Add a new node right after head\n        node.prev = self.head\n        node.next = self.head.next\n        self.head.next.prev = node\n        self.head.next = node\n\n    def _remove_node(self, node):\n        # Remove an existing node from the list\n        prev = node.prev\n        new = node.next\n        prev.next = new\n        new.prev = prev\n\n    def _move_to_head(self, node):\n        # Move a node to the head\n        self._remove_node(node)\n        self._add_node(node)\n\n    def _pop_tail(self):\n        # Pop the current tail\n        res = self.tail.prev\n        self._remove_node(res)\n        return res\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            node = self.cache[key]\n            self._move_to_head(node)\n            return node.value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            node = self.cache[key]\n            self._move_to_head(node)\n            node.value = value\n        else:\n            if len(self.cache) >= self.capacity:\n                # Pop the tail\n                tail = self._pop_tail()\n                del self.cache[tail.key]\n            new_node = Node(key, value)\n            self._add_node(new_node)\n            self.cache[key] = new_node\n```\n\nLet's walk through the methods:\n\n- **__init__**: Initializes the cache with a given capacity and sets up dummy head and tail nodes to simplify adding and removing nodes.\n\n- **_add_node**: Adds a new node right after the head. This is where recently used items are placed.\n\n- **_remove_node**: Removes an existing node from the list.\n\n- **_move_to_head**: Moves a node to the head by first removing it and then adding it to the head.\n\n- **_pop_tail**: Removes the current tail (least recently used item) from the list.\n\n- **get**: If the key exists, it moves the corresponding node to the head (indicating it's now recently used) and returns its value. If the key doesn't exist, it returns -1.\n\n- **put**: If the key already exists, it updates the value and moves the node to the head. If the key doesn't exist, it creates a new node, adds it to the head, and adds it to the dictionary. If the cache is full, it removes the least recently used item from the tail and deletes it from the dictionary.\n\nThis implementation ensures that both get and put operations run in constant time, O(1), which is the goal for an efficient cache.\n\nI hope this helps! Let me know if you have any questions or need further explanation on any part of the implementation."
    }
]