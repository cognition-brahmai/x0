[
    {
        "role": "user",
        "content": "Implement a simple Least Recently Used (LRU) cache in Python with the following methods: put(key, value), get(key), and delete(key). The cache should have a maximum size specified during initialization.\nPLAINFORMAT"
    },
    {
        "role": "assistant",
        "content": "Sure, I can help you with that! Implementing an LRU cache in Python is a great way to manage a fixed-size cache efficiently. We'll use a combination of a dictionary and a doubly-linked list to achieve this. The dictionary will provide fast access to the cache entries, while the doubly-linked list will help us keep track of the order in which items were used.\n\nFirst, let's understand what an LRU cache is. LRU stands for Least Recently Used, which is a caching strategy where the least recently accessed items are removed first when the cache reaches its capacity. This approach assumes that if data has not been used for a while, it may not be used in the near future.\n\n### Approach\n\n1. **Dictionary**: We'll use a dictionary to store the key-value pairs for O(1) access time.\n2. **Doubly-Linked List**: This will help us maintain the order of elements based on their usage. Each node in the list will represent a cache entry and will contain a key and a value.\n   - The most recently used item will be at the head of the list.\n   - The least recently used item will be at the tail of the list.\n\n### Solution Code\n\n```python\nclass Node:\n    def __init__(self, key, value):\n        self.key = key\n        self.value = value\n        self.prev = None\n        self.next = None\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.head = Node(0, 0)  # Dummy head\n        self.tail = Node(0, 0)  # Dummy tail\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _remove(self, node):\n        prev_node = node.prev\n        next_node = node.next\n        prev_node.next = next_node\n        next_node.prev = prev_node\n\n    def _add(self, node):\n        node.prev = self.head\n        node.next = self.head.next\n        self.head.next.prev = node\n        self.head.next = node\n\n    def get(self, key):\n        if key in self.cache:\n            node = self.cache[key]\n            self._remove(node)\n            self._add(node)\n            return node.value\n        return -1\n\n    def put(self, key, value):\n        if key in self.cache:\n            self._remove(self.cache[key])\n        node = Node(key, value)\n        self.cache[key] = node\n        self._add(node)\n        if len(self.cache) > self.capacity:\n            lru_node = self.tail.prev\n            self._remove(lru_node)\n            del self.cache[lru_node.key]\n\n    def delete(self, key):\n        if key in self.cache:\n            node = self.cache[key]\n            self._remove(node)\n            del self.cache[key]\n```\n\n### Explanation\n\n1. **Initialization**:\n   - `capacity`: Maximum size of the cache.\n   - `cache`: Dictionary to store key-node pairs.\n   - `head` and `tail`: Dummy nodes to simplify adding and removing nodes.\n\n2. **Helper Methods**:\n   - `_remove(node)`: Removes a node from the linked list.\n   - `_add(node)`: Adds a node right after the head.\n\n3. **get(key)**:\n   - If the key exists, move the corresponding node to the head (most recently used) and return its value.\n   - If the key doesn't exist, return -1.\n\n4. **put(key, value)**:\n   - If the key already exists, remove the old node.\n   - Create a new node and add it to the head.\n   - If the cache exceeds capacity, remove the least recently used node (tail.prev).\n\n5. **delete(key)**:\n   - If the key exists, remove the corresponding node from the linked list and delete it from the dictionary.\n\nThis implementation ensures that all operations (`get`, `put`, `delete`) are performed in O(1) time complexity, making it efficient for caching purposes."
    }
]