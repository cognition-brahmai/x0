[
    {
        "role": "user",
        "content": "I am using XGBoost to generate predictions for a multi-class classifier. However, I want to get not only the predicted class but also the probability of the prediction. For example, return Medium - 88%. How can I achieve this? In my current implementation, I am using the following parameters:\n\n```python\nparams = {\n    'max_depth': 3,\n    'objective': 'multi:softmax',  # error evaluation for multiclass training\n    'num_class': 3,\n    'n_gpus': 0\n}\n```\nI am predicting with `pred = model.predict(D_test)`, and the results are in this format `array([2., 2., 1., ..., 1., 2., 2.], dtype=float32)`. After label encoding, the results are like `array(['Medium', 'Medium', 'Low', 'Low', 'Medium'], dtype=object)`. In the edit, it was suggested to use `predict_proba` but I am not instantiating `XGBClassifer()`. Should I be? How would I modify my pipeline to use `predict_proba`, if so? My model training looks like this:\n\n```python\nparams = {\n    'max_depth': 3,\n    'objective': 'multi:softmax',  # error evaluation for multiclass training\n    'num_class': 3,\n    'n_gpus': 0\n}\n\nsteps = 20  # The number of training iterations\n\nmodel = xgb.train(params, D_train, steps)\n```"
    },
    {
        "role": "assistant",
        "content": "To get the predicted class probabilities using XGBoost, you need to change the `objective` parameter from `'multi:softmax'` to `'multi:softprob'`. The `'multi:softmax'` objective directly outputs the class with the highest predicted probability, while `'multi:softprob'` outputs the probabilities for each class.\n\nHere's a step-by-step guide on how to modify your pipeline:\n\n1. **Update Parameters:**\n   Change the `objective` parameter to `'multi:softprob'`.\n\n   ```python\n   params = {\n       'max_depth': 3,\n       'objective': 'multi:softprob',  # Output class probabilities\n       'num_class': 3,\n       'n_gpus': 0\n   }\n   ```\n\n2. **Train the Model:**\n   The training part remains the same.\n\n   ```python\n   steps = 20  # The number of training iterations\n\n   model = xgb.train(params, D_train, steps)\n   ```\n\n3. **Predict Probabilities:**\n   Use the `predict` method to get the class probabilities.\n\n   ```python\n   pred_probs = model.predict(D_test)\n   ```\n\n   The `pred_probs` will be an array of shape `(n_samples, num_class)`, where each row contains the probabilities of each class.\n\n4. **Get Predicted Class and Probability:**\n   You can then determine the predicted class and its associated probability for each sample like this:\n\n   ```python\n   import numpy as np\n\n   # Get the index of the predicted class (highest probability)\n   pred_classes = np.argmax(pred_probs, axis=1)\n\n   # Get the probability of the predicted class\n   pred_probabilities = np.max(pred_probs, axis=1)\n\n   # Map predicted class indices to actual class labels\n   class_labels = ['Low', 'Medium', 'High']  # Assuming these are your class labels\n\n   # Combine class labels with their probabilities\n   predictions = [(class_labels[int(pred_class)], round(prob * 100, 2))\n                  for pred_class, prob in zip(pred_classes, pred_probabilities)]\n\n   # Example output\n   for label, prob in predictions:\n       print(f\"{label} - {prob}%\")\n   ```\n\nBy following these steps, you can obtain the predicted class along with the probability of that prediction for each sample."
    }
]