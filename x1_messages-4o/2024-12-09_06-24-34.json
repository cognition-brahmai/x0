[
    {
        "role": "user",
        "content": "Does TensorFlow automatically utilize all available GPUs in a system by default? I have a machine with 3 GTX Titan GPUs and after running the Cifar10 example with cifar10_train.py, I noticed output that seems to suggest that TensorFlow is attempting to initialize itself on two devices (gpu0 and gpu1). Why is this the case and is there a way to limit TensorFlow to only use a single GPU?\n\nOutput: \n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:60] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:60] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:694] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:694] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN, pci bus id: 0000:84:00.0)\n```"
    },
    {
        "role": "assistant",
        "content": "By default, TensorFlow will attempt to utilize all available GPUs on a system. This is part of its design to leverage the computational power of multiple GPUs if they are available. The output you're seeing indicates that TensorFlow is initializing itself on two of the GPUs, which is consistent with its default behavior.\n\nIf you want to limit TensorFlow to use only a single GPU, you can do so by setting the `CUDA_VISIBLE_DEVICES` environment variable before running your script. This environment variable controls which GPUs are visible to TensorFlow and other CUDA applications. Here's how you can set it to limit TensorFlow to use just one GPU:\n\n1. **Set the Environment Variable:**\n\n   For example, if you want to use only the first GPU (`gpu0`), you can set the environment variable like this in your terminal:\n\n   ```bash\n   export CUDA_VISIBLE_DEVICES=0\n   ```\n\n   This command makes only the first GPU visible to TensorFlow.\n\n2. **Run Your Script:**\n\n   After setting the environment variable, you can run your script as usual. TensorFlow will only see the GPU(s) specified by `CUDA_VISIBLE_DEVICES`.\n\n3. **Python Script Alternative:**\n\n   If you prefer setting the environment variable within your Python script, you can do the following at the top of your script:\n\n   ```python\n   import os\n   os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only the first GPU\n   ```\n\n4. **Check GPU Usage:**\n\n   You can verify which GPUs are being used by observing the output logs or using monitoring tools like `nvidia-smi`.\n\nBy using the `CUDA_VISIBLE_DEVICES` environment variable, you have fine-grained control over which GPUs TensorFlow uses, enabling you to manage your resources effectively based on your specific requirements."
    }
]