[
    {
        "role": "user",
        "content": "How can I fetch multiple urls using aiohttp and asyncio in Python 3.5 and handle errors when one of the requests fails? I am currently using the new async with syntax but when a session.get(url) request breaks, the error is not handled and the whole program breaks. I have tried adding error checks such as try...except and checking if the response status is not 200, but I am not sure how to do this with the async with, await and other objects. My goal is to inspect the results and quickly see whether the url failed (and why, including status code or exception name) or worked, along with the response object."
    },
    {
        "role": "assistant",
        "content": "To fetch multiple URLs using `aiohttp` and `asyncio` in Python 3.5, you need to ensure that you handle errors properly so that one failed request does not crash your entire program. You can achieve this by using `try...except` blocks within your coroutine functions and collecting results in a structured way. Below is an example of how you can do this:\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch_url(session, url):\n    \"\"\"Fetch a URL and handle errors.\"\"\"\n    try:\n        async with session.get(url) as response:\n            # Check if the HTTP status code is 200\n            if response.status == 200:\n                data = await response.text()\n                return {'url': url, 'status': response.status, 'data': data, 'error': None}\n            else:\n                return {'url': url, 'status': response.status, 'data': None, 'error': f'HTTP Error: {response.status}'}\n    except Exception as e:\n        return {'url': url, 'status': None, 'data': None, 'error': str(e)}\n\nasync def fetch_all_urls(urls):\n    \"\"\"Fetch multiple URLs concurrently.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        return await asyncio.gather(*tasks)\n\ndef main():\n    urls = [\n        'https://example.com',\n        'https://httpbin.org/status/404',  # A URL that will return a 404 error\n        'https://invalid-url',  # An invalid URL to trigger an exception\n        # Add more URLs as needed\n    ]\n\n    loop = asyncio.get_event_loop()\n    results = loop.run_until_complete(fetch_all_urls(urls))\n\n    for result in results:\n        if result['error']:\n            print(f\"Failed to fetch {result['url']}: {result['error']}\")\n        else:\n            print(f\"Successfully fetched {result['url']}: {len(result['data'])} bytes\")\n\nif __name__ == '__main__':\n    main()\n```\n\n### Key Points:\n\n1. **Async Context Manager**: Use `async with` to open an `aiohttp.ClientSession`. This ensures that resources are properly cleaned up after use.\n\n2. **Error Handling**: Wrap the request in a `try...except` block. This will allow you to catch exceptions such as connection errors and handle them gracefully.\n\n3. **Status Code Check**: Always check the HTTP status code of the response. If it\u2019s not 200, handle it as an error.\n\n4. **Structured Results**: Return a dictionary from the `fetch_url` function that includes the URL, status, data, and any error. This makes it easier to inspect results after fetching.\n\n5. **Concurrency**: Use `asyncio.gather` to run multiple fetch operations concurrently.\n\nThis approach ensures that you can handle both HTTP errors and exceptions without crashing your program, and you have a clear way of inspecting each result."
    }
]