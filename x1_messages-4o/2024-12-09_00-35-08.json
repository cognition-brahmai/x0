[
    {
        "role": "user",
        "content": "How can I properly parse the heatmap output for the pose estimation TFLite model in order to obtain the coordinates and confidences of each body part? I've been trying to use the model to get keypoints on humans, and I've been able to fit a single image or a person and invoke the model. However, I'm having trouble interpreting the model's results to get the keypoints back to the original image. I'm also experiencing an issue where some of the coordinates I'm getting are negative, which I know can't be correct. Can anyone help me with a Python example where they successfully interpreted this model's results?\n\nHere is the code I've been using:\n\n```python\nimg = cv.imread('photos\\standing\\\\3.jpg')\nimg = tf.reshape(tf.image.resize(img, [257,257]), [1,257,257,3])\nmodel = tf.lite.Interpreter('models\\posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite')\nmodel.allocate_tensors()\ninput_details = model.get_input_details()\noutput_details = model.get_output_details()\nfloating_model = input_details[0]['dtype'] == np.float32\nif floating_model:\n    img = (np.float32(img) - 127.5) / 127.5\nmodel.set_tensor(input_details[0]['index'], img)\nmodel.invoke()\noutput_data = model.get_tensor(output_details[0]['index'])\noffset_data = model.get_tensor(output_details[1]['index'])\nresults = np.squeeze(output_data)\noffsets_results = np.squeeze(offset_data)\nprint(\"output shape: {}\".format(output_data.shape))\nnp.savez('sample3.npz', results, offsets_results)\n\ndef get_keypoints(self, data):\n    height, width, num_keypoints = data.shape\n    keypoints = []\n    for keypoint in range(0, num_keypoints):\n        maxval = data[0][0][keypoint]\n        maxrow = 0\n        maxcol = 0\n        for row in range(0, width):\n            for col in range(0,height):\n                if data[row][col][keypoint] > maxval:\n                    maxrow = row\n                    maxcol = col\n                    maxval = data[row][col][keypoint]\n        keypoints.append(KeyPoint(keypoint, maxrow, maxcol, maxval))\n    return keypoints\n\ndef get_image_coordinates_from_keypoints(self, offsets):\n    height, width, depth = (257,257,3)\n    coords = [{'point': k.body_part,\n               'location': (k.x / (width - 1)*width + offsets[k.y][k.x][k.index],\n              k.y / (height - 1)*height + offsets[k.y][k.x][k.index]),\n               'confidence': k.confidence} for k in self.keypoints]\n    return coords\n```"
    },
    {
        "role": "assistant",
        "content": "To properly parse the heatmap output of a pose estimation TFLite model and obtain the coordinates and confidences of each body part, you need to understand how the model's output is structured. Typically, models like PoseNet produce two main outputs: heatmaps and offsets. The heatmaps indicate the presence of keypoints, while the offsets provide additional information to refine the position of each keypoint.\n\nHere's a step-by-step explanation and improved code to help you achieve this:\n\n1. **Heatmap Interpretation**: The heatmap output is a 3D array with dimensions `[height, width, num_keypoints]`. Each slice along the third dimension corresponds to the heatmap for a specific keypoint.\n\n2. **Finding Keypoints**: For each keypoint, find the location with the highest confidence value in the heatmap. This is the most likely position of the keypoint within the heatmap.\n\n3. **Applying Offsets**: Use the offset output to refine the position of each keypoint. Offsets adjust the coarse positions from the heatmaps to provide more accurate keypoint locations.\n\n4. **Scaling to Original Image**: Scale the keypoint coordinates back to the original image dimensions.\n\nHere's a refactored version of your code with comments explaining each step:\n\n```python\nimport cv2 as cv\nimport tensorflow as tf\nimport numpy as np\n\n# Load and preprocess image\nimg = cv.imread('photos/standing/3.jpg')\nimg_resized = tf.image.resize(img, [257, 257])\nimg_input = tf.reshape(img_resized, [1, 257, 257, 3])\n\n# Load TFLite model\nmodel = tf.lite.Interpreter('models/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite')\nmodel.allocate_tensors()\n\n# Get input and output details\ninput_details = model.get_input_details()\noutput_details = model.get_output_details()\n\n# Normalize input if needed\nif input_details[0]['dtype'] == np.float32:\n    img_input = (np.float32(img_input) - 127.5) / 127.5\n\n# Set input tensor and invoke the model\nmodel.set_tensor(input_details[0]['index'], img_input)\nmodel.invoke()\n\n# Get model outputs\nheatmap_data = model.get_tensor(output_details[0]['index'])\noffset_data = model.get_tensor(output_details[1]['index'])\n\n# Squeeze to remove batch dimension\nheatmaps = np.squeeze(heatmap_data)\noffsets = np.squeeze(offset_data)\nheight, width, num_keypoints = heatmaps.shape\n\ndef get_keypoints_from_heatmaps(heatmaps, offsets):\n    keypoints = []\n    for k in range(num_keypoints):\n        # Find the maximum value's location for each keypoint heatmap\n        y, x = np.unravel_index(np.argmax(heatmaps[:, :, k]), (height, width))\n        \n        # Get offset adjustments\n        offset_y = offsets[y, x, k]\n        offset_x = offsets[y, x, k + num_keypoints]\n        \n        # Adjust keypoint location using offsets\n        keypoint_y = y + offset_y\n        keypoint_x = x + offset_x\n        \n        # Store keypoint with confidence\n        max_confidence = heatmaps[y, x, k]\n        keypoints.append({'id': k, 'position': (keypoint_x, keypoint_y), 'confidence': max_confidence})\n    \n    return keypoints\n\ndef scale_keypoints_to_original_image(keypoints, original_height, original_width):\n    scaled_keypoints = []\n    for kp in keypoints:\n        x, y = kp['position']\n        # Scale the coordinates\n        scaled_x = x / width * original_width\n        scaled_y = y / height * original_height\n        scaled_keypoints.append({'id': kp['id'], 'position': (scaled_x, scaled_y), 'confidence': kp['confidence']})\n    return scaled_keypoints\n\n# Obtain keypoints\nkeypoints = get_keypoints_from_heatmaps(heatmaps, offsets)\n# Scale keypoints to original image size\noriginal_height, original_width, _ = img.shape\nscaled_keypoints = scale_keypoints_to_original_image(keypoints, original_height, original_width)\n\n# Print keypoints\nfor kp in scaled_keypoints:\n    print(f\"Keypoint ID: {kp['id']}, Position: {kp['position']}, Confidence: {kp['confidence']}\")\n```\n\n### Key Points:\n- Ensure that you handle the offsets correctly by accessing the appropriate slices of the offset tensor.\n- Keypoints are adjusted using the offsets to provide more accurate positions.\n- Always scale the final coordinates back to the size of the original image to ensure accuracy of the keypoint positions in the context of the original input image.\n- Pay attention to the data types and shapes to prevent negative or incorrect values.\n\nThis code should help you parse the heatmap and offset outputs effectively to get accurate keypoint coordinates and confidences."
    }
]