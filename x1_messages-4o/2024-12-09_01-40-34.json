[
    {
        "role": "user",
        "content": "I have a TensorFlow SavedModel that I have exported and I need to load it back in and make a prediction. The model was trained with the following features and labels: F1, F2, F3 and L1, all of which are float32. I want to feed in the values 20.9, 1.8, 0.9 and get a single float32 prediction. I have successfully loaded the model, but I am unsure how to make the prediction call. The code I have so far is:\n\n```python\nwith tf.Session(graph=tf.Graph()) as sess:\n    tf.saved_model.loader.load(\n        sess,\n        [tf.saved_model.tag_constants.SERVING],\n        \"/job/export/Servo/1503723455\"\n    )\n\n    # How can I predict from here?\n    # I want to do something like prediction = model.predict([20.9, 1.8, 0.9])\n```\n\nThis is not a duplicate of the question posted here. My question focuses on a minimal example of performing inference on a SavedModel of any model class (not just limited to tf.estimator) and the syntax of specifying input and output node names."
    },
    {
        "role": "assistant",
        "content": "To make predictions using a TensorFlow SavedModel, you'll need to follow a few key steps to identify the input and output tensor names and then run the session with your input data. Here's a step-by-step guide to achieve this:\n\n1. **Identify Input and Output Tensor Names**: When you saved the model, it was associated with specific input and output tensor names. If you're unsure of these names, you can inspect the graph's operations to find out:\n\n   ```python\n   with tf.Session(graph=tf.Graph()) as sess:\n       tf.saved_model.loader.load(\n           sess,\n           [tf.saved_model.tag_constants.SERVING],\n           \"/job/export/Servo/1503723455\"\n       )\n       \n       for op in sess.graph.get_operations():\n           print(op.name)  # This will list all operations and you can find inputs/outputs\n   ```\n\n   Look for operations related to inputs and outputs. Often, input tensors are named something like `serving_default_input`, and output tensors might be named `StatefulPartitionedCall` or similar, but this can vary.\n\n2. **Run the Session with Input Data**: Once you have the names of the input and output tensors, you can feed your data into the graph and fetch the prediction.\n\n   Here is a code snippet to illustrate this:\n\n   ```python\n   import tensorflow as tf\n\n   with tf.Session(graph=tf.Graph()) as sess:\n       tf.saved_model.loader.load(\n           sess,\n           [tf.saved_model.tag_constants.SERVING],\n           \"/job/export/Servo/1503723455\"\n       )\n\n       # Assume 'input_tensor_name' and 'output_tensor_name' are placeholders\n       # identified from the graph operations.\n       input_tensor = sess.graph.get_tensor_by_name('input_tensor_name:0')\n       output_tensor = sess.graph.get_tensor_by_name('output_tensor_name:0')\n\n       # Prepare your input data\n       input_data = [[20.9, 1.8, 0.9]]  # Wrap in a list to match expected batch input shape\n\n       # Run the session to get predictions\n       prediction = sess.run(output_tensor, feed_dict={input_tensor: input_data})\n\n       print(\"Prediction:\", prediction)\n   ```\n\n3. **Adjust Based on Your Model**: The exact names and input shapes might differ based on how your model was defined and exported. Adjust the `input_tensor_name` and `output_tensor_name` accordingly and ensure the input data matches the expected shape.\n\nBy using this approach, you should be able to make predictions using your exported TensorFlow SavedModel with the provided input values."
    }
]