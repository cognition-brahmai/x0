[
    {
        "role": "user",
        "content": "I am facing an issue when I try to concatenate two parquet files using pandas in Python and then try to write and save the DataFrame to a parquet file. The error message is: \"ArrowInvalid: Casting from timestamp[ns] to timestamp[ms] would lose data\". From the pandas documentation, I understand that it defaults the timestamp syntax in milliseconds when writing the parquet file. How can I write the parquet file using the existing schema after concatenation? Here is the code that I am using:\n\n```python\nimport pandas as pd\n\ntable1 = pd.read_parquet(path= ('path.parquet'),engine='pyarrow')\ntable2 = pd.read_parquet(path= ('path.parquet'),engine='pyarrow')\n\ntable = pd.concat([table1, table2], ignore_index=True) \ntable.to_parquet('./file.gzip', compression='gzip')\n```"
    },
    {
        "role": "assistant",
        "content": "The error you're encountering is related to the precision of timestamp data types. When you read Parquet files with `pyarrow`, timestamps are often read with nanosecond precision (`timestamp[ns]`), but when writing, the default precision might be milliseconds (`timestamp[ms]`), which can lead to data loss if the timestamps have nanosecond precision.\n\nTo resolve this, you can ensure that the timestamps are in the same resolution during both reading and writing. You can explicitly control the timestamp resolution when writing the Parquet file by using the `coerce_timestamps` parameter available in `to_parquet`. Here's how you can modify your code:\n\n```python\nimport pandas as pd\n\n# Read parquet files\ntable1 = pd.read_parquet('path1.parquet', engine='pyarrow')\ntable2 = pd.read_parquet('path2.parquet', engine='pyarrow')\n\n# Concatenate tables\ntable = pd.concat([table1, table2], ignore_index=True)\n\n# Write to parquet file with explicit timestamp resolution\ntable.to_parquet('./file.gzip', compression='gzip', coerce_timestamps='us')  # Using microseconds\n\n# Alternative: using nanoseconds if you want to retain full precision\n# table.to_parquet('./file.gzip', compression='gzip', coerce_timestamps='ns')\n```\n\nHere, `coerce_timestamps='us'` converts timestamps to microsecond precision, which should usually be sufficient for most use cases. If you need to preserve nanosecond precision, you can use `coerce_timestamps='ns'`, but make sure that the reading environment also supports this precision.\n\nAdjust the `path1.parquet` and `path2.parquet` to the correct file paths you are using. This should help you avoid the `ArrowInvalid` error by ensuring consistent timestamp precision."
    }
]