[
    {
        "role": "user",
        "content": "In my Natural Language Processing project, I am trying to compute the similarity between two text documents. What would be the best way to accomplish this in Python?"
    },
    {
        "role": "assistant",
        "content": "To compute the similarity between two text documents in Python, there are several approaches you can take, depending on the nature of your documents and the level of sophistication you desire. Here are a few popular methods:\n\n1. **Cosine Similarity with TF-IDF**:\n   - **TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus).\n   - **Cosine Similarity** measures the cosine of the angle between two vectors in a multi-dimensional space, which in this case are the TF-IDF vectors of your documents.\n\n   ```python\n   from sklearn.feature_extraction.text import TfidfVectorizer\n   from sklearn.metrics.pairwise import cosine_similarity\n\n   documents = [\"Document 1 text goes here\", \"Document 2 text goes here\"]\n   vectorizer = TfidfVectorizer()\n   tfidf_matrix = vectorizer.fit_transform(documents)\n   similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n   print(similarity_matrix)\n   ```\n\n2. **Word Embeddings**:\n   - Use pre-trained word embeddings like **Word2Vec**, **GloVe**, or **fastText** to convert words into dense vector representations. You can then average the word vectors to get a document vector and compute similarity using cosine similarity.\n\n   ```python\n   from gensim.models import Word2Vec, KeyedVectors\n   from sklearn.metrics.pairwise import cosine_similarity\n   import numpy as np\n\n   # Load pre-trained word vectors (e.g., GloVe)\n   word_vectors = KeyedVectors.load_word2vec_format('path_to_vectors', binary=False)\n\n   def document_vector(doc):\n       # Remove out-of-vocabulary words\n       doc = [word for word in doc if word in word_vectors]\n       return np.mean(word_vectors[doc], axis=0)\n\n   doc1 = \"Document 1 text goes here\".split()\n   doc2 = \"Document 2 text goes here\".split()\n   similarity = cosine_similarity([document_vector(doc1)], [document_vector(doc2)])\n   print(similarity)\n   ```\n\n3. **Sentence Transformers**:\n   - Use models like **BERT**, **RoBERTa**, or the more efficient **Sentence-BERT (SBERT)** to compute embeddings for sentences or documents. These embeddings can then be compared using cosine similarity.\n\n   ```python\n   from sentence_transformers import SentenceTransformer, util\n\n   model = SentenceTransformer('all-MiniLM-L6-v2')\n   embeddings = model.encode([\"Document 1 text goes here\", \"Document 2 text goes here\"])\n   similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n   print(similarity)\n   ```\n\n4. **Jaccard Similarity**:\n   - This is a simpler method that calculates the similarity between two sets by dividing the size of the intersection by the size of the union of the sets.\n\n   ```python\n   def jaccard_similarity(doc1, doc2):\n       a = set(doc1.split())\n       b = set(doc2.split())\n       intersection = a.intersection(b)\n       union = a.union(b)\n       return float(len(intersection)) / len(union)\n\n   similarity = jaccard_similarity(\"Document 1 text goes here\", \"Document 2 text goes here\")\n   print(similarity)\n   ```\n\nChoose the method that best fits your use case. For more nuanced and context-aware similarity, leveraging transformer-based models like Sentence-BERT can be highly effective. If you are dealing with a large number of documents and computational efficiency is a concern, TF-IDF with cosine similarity might be more appropriate."
    }
]