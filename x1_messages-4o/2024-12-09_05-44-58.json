[
    {
        "role": "user",
        "content": "I am new to using Pytorch's DistributedDataParallel() and I have noticed from tutorials, such as the ImageNet tutorial, that the model saved is the local rank 0 model during training. This results in multiple models if I'm using multiple machines with multiple GPUs. For instance, if I have 3 machines with 4 GPUs each, I end up with 3 models. The code used to save the model is as follows:\n\n```python\nif not args.multiprocessing_distributed or (args.multiprocessing_distributed\n            and args.rank % ngpus_per_node == 0):\n    save_checkpoint({...})\n```\n\nThis means the model is saved if `rank % ngpus_per_node == 0`. Since DistributedDataParallel() automatically reduces the loss on the backend, and every process can sync the loss automatically, the models on each process should only have slight differences by the end of the process. Therefore, I'm assuming that saving one model should be enough. So, why is the model saved on `rank % ngpus_per_node == 0` instead of `rank == 0`? Also, if I end up with multiple models, which one should I use? If saving the model in distributed learning this way is correct, should I merge them, use one of them, or inference the result based on all three models? Please correct me if I'm wrong."
    },
    {
        "role": "assistant",
        "content": "In PyTorch's distributed training using `DistributedDataParallel` (DDP), the model weights are synchronized across all processes during the backward pass. Therefore, ideally, the models should be identical across different processes after every training step, assuming they start from the same initial state and there is no non-deterministic behavior.\n\nHere's an explanation of why the model is saved with `rank % ngpus_per_node == 0` and the implications of having multiple models:\n\n1. **Saving Model with `rank % ngpus_per_node == 0`:**\n   - The condition `rank % ngpus_per_node == 0` is typically used when you have a multi-node setup with multiple GPUs per node. It ensures that only one model per node is saved, reducing redundant writes to disk. This is particularly useful in a distributed environment where saving models from every process could lead to unnecessary I/O overhead.\n\n2. **Why Not Just `rank == 0`?:**\n   - In a multi-node setup, `rank == 0` corresponds to only one process across all nodes. Saving from just this one process might be enough, but the `rank % ngpus_per_node == 0` condition provides a balance by saving from the first GPU of each node. This can be useful for debugging or ensuring redundancy.\n\n3. **Which Model to Use?:**\n   - Since the models are synchronized, any model saved (whether from rank 0 or the first rank on each node) should be identical. You can choose any one of them for inference.\n\n4. **Merging Models:**\n   - There is no need to merge models since they are supposed to be identical. If they are not, it might indicate some synchronization issue or non-deterministic behavior in your training loop.\n\n5. **Inference Using Multiple Models:**\n   - In practice, you typically use one model for inference. Using multiple models saved from different processes of the same training run should yield the same results, so there's no need to use them all.\n\nIn conclusion, the condition `rank % ngpus_per_node == 0` is just an optimization to avoid saving too many redundant models. As long as you ensure that the models are synchronized (which DDP handles during training), you can confidently use any of the saved models for inference. If you're saving checkpoints frequently and are concerned about storage, you might want to save less frequently or only save the final model."
    }
]