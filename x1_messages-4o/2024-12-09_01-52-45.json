[
    {
        "role": "user",
        "content": "What is the most efficient method to append rows to a DataFrame in Python? I am working with a large dataset with over 1 million rows and 29 columns which I need to convert to a .csv format. As the dataset gets larger, appending rows to it becomes increasingly time-consuming. I'm currently using the following code:\n\n```python\ndf = DataFrame()\n\nfor startID in range(0, 100000, 1000):\n    s1 = time.time()\n    tempdf = DataFrame()\n    url = f'https://******/products?startId={startID}&amp;size=1000'\n\n    r = requests.get(url, headers={'****-Token': 'xxxxxx', 'Merchant-Id': '****'})\n    jsonList = r.json()  \n\n    normalized = json_normalize(jsonList)\n    print(startID / 1000) # status indicator\n    for series in normalized.iterrows():  \n        series = series[1] # iterrows returns tuple (index, series)\n        offers = series['offers']\n        series = series.drop(columns='offers')\n        length = len(offers)\n\n        for offer in offers:\n            n = json_normalize(offer).squeeze()  # squeeze() casts DataFrame into Series\n            concatinated = concat([series, n]).to_frame().transpose()\n            tempdf = tempdf.append(concatinated, ignore_index=True)\n\n    del normalized\n    df = df.append(tempdf)\n    f1 = time.time()\n    print(f1 - s1, ' seconds')\n\ndf.to_csv('out.csv')\n```\nIs there a faster way to accomplish this? Any recommendations are welcome."
    },
    {
        "role": "assistant",
        "content": "Certainly! Appending rows to a DataFrame using the `append` method in a loop can be inefficient, especially with large datasets, as it creates a new DataFrame each time. Instead, you can utilize a list to collect DataFrames and then concatenate them all at once. Here's an improved version of your code:\n\n```python\nimport requests\nimport pandas as pd\nfrom pandas import json_normalize\n\n# Initialize an empty list to collect DataFrames\ndf_list = []\n\nfor startID in range(0, 100000, 1000):\n    s1 = time.time()\n    url = f'https://******/products?startId={startID}&amp;size=1000'\n\n    r = requests.get(url, headers={'****-Token': 'xxxxxx', 'Merchant-Id': '****'})\n    jsonList = r.json()  \n\n    normalized = json_normalize(jsonList)\n    print(startID / 1000)  # status indicator\n\n    temp_list = []  # Temporary list to store rows for this batch\n    for _, series in normalized.iterrows():  \n        offers = series['offers']\n        series = series.drop(columns='offers')\n        \n        for offer in offers:\n            n = json_normalize(offer).squeeze()\n            concatenated = pd.concat([series, n]).to_frame().transpose()\n            temp_list.append(concatenated)\n\n    # Concatenate all rows in temp_list into a single DataFrame and add to df_list\n    if temp_list:\n        tempdf = pd.concat(temp_list, ignore_index=True)\n        df_list.append(tempdf)\n\n    f1 = time.time()\n    print(f1 - s1, ' seconds')\n\n# Concatenate all DataFrames in df_list into a single DataFrame\ndf = pd.concat(df_list, ignore_index=True)\n\ndf.to_csv('out.csv', index=False)\n```\n\n### Key Improvements:\n\n1. **Use Lists for Accumulation**: By accumulating DataFrames in a list and concatenating them all at once, you avoid the overhead of repeatedly copying DataFrames with `append`.\n\n2. **Efficient Concatenation**: Use `pd.concat` to efficiently concatenate the entire list of DataFrames once all data is gathered.\n\n3. **Avoid Redundant Operations**: Always try to minimize operations within loops, especially when dealing with large data sets. Here, `concat` and `squeeze` are minimized by collecting rows in a list first.\n\nThis approach should significantly reduce the time complexity and improve the performance of your data processing pipeline."
    }
]