[
    {
        "role": "user",
        "content": "I am working on a binary classification problem with the Tensorflow BERT language model. When I try to make predictions after saving and loading the trained model, I encounter an error. The error seems to be related to the 'batch_size' parameter, which is present in the estimator but not in the loaded model parameters. How can I fix this issue?\n\nHere is the code for saving the model:\n\n```python\ndef serving_input_receiver_fn():\n  feature_spec = {\n      \"input_ids\" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n      \"input_mask\" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n      \"segment_ids\" : tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n      \"label_ids\" :  tf.FixedLenFeature([], tf.int64)\n  }\n  serialized_tf_example = tf.placeholder(dtype=tf.string,\n                                         shape=[None],\n                                         name='input_example_tensor')\n  print(serialized_tf_example.shape)\n  receiver_tensors = {'example': serialized_tf_example}\n  features = tf.parse_example(serialized_tf_example, feature_spec)\n  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\nexport_path = '/content/drive/My Drive/binary_class/bert/'\nestimator._export_to_tpu = False  # this is important\nestimator.export_saved_model(export_dir_base=export_path,serving_input_receiver_fn=serving_input_receiver_fn)\n```\n\nAnd here is the code for predicting with dummy text:\n\n```python\npred_sentences = [\n  \"A novel, simple method to get insights from reviews\"\n]\n\ndef getPrediction1(in_sentences):\n  labels = [\"Irrelevant\", \"Relevant\"]\n  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n  predictions = est.predict(predict_input_fn)\n  print(predictions)\n  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n\nest = tf.contrib.estimator.SavedModelEstimator(MODEL_FILE_PATH)\npredictions = getPrediction1(pred_sentences[0])\npredictions\n```\n\nThis is the error I encounter:\n\n```python\nW0702 05:44:17.551325 139812812932992 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpzeiaa6q8\nW0702 05:44:17.605536 139812812932992 saved_model_estimator.py:170] train mode not found in SavedModel.\nW0702 05:44:17.608479 139812812932992 saved_model_estimator.py:170] eval mode not found in SavedModel.\n<generator object Estimator.predict at 0x7f27fa721eb8>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-28-56ea95428bf4> in <module>()\n     21 # Relevant \"Nanoparticulate drug delivery is a promising drug delivery system to a range of molecules to desired site specific action in the body. In this present work nanoparticles are prepared with positive group of amino group of chitosan with varying concentration based nanoparticles are loaded with anastrazole were prepared by with negative group of sodium tripolyphosphate by ionotropic gelation method. All these formulated nanoparticles are characterized for its particle size ,zeta potential ,drug entrapment efficacy and in-vitro release kinetics .The particle size of all these formulations were found to be 200,365,420,428 And 483.zeta potential of all formulations are-16.3\u00b12.1 ,28.2\u00b14.3,-10.38\u00b13.6,-24.31\u00b13.2 and 21.38\u00b15.2.respectively. FT-IR studies indicated that there was no chemical interaction between drug and polymer and stability of drug. The in-vitro release behaviour from all the drug loaded batches was found to be zero order and provided sustained release over a period of 12 h by diffusion and swelling mechanism and The values of n and r 2 for coated batch was 0.731 and 0.979.Since the values of slope (n) lies in between 0.5 and 1 it was concluded that the mechanism by which drug is being released is a Non-Fickian anomalous solute diffusion mechanism, \"\n     22 \n---> 23 predictions = getPrediction1(pred_sentences[0:2])\n     24 predictions\n     25 \n\n5 frames\n<ipython-input-28-56ea95428bf4> in getPrediction1(in_sentences)\n     14   predictions = est.predict(predict_input_fn)\n     15   print(predictions)\n---> 16   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n     17 \n     18 \n\n<ipython-input-28-56ea95428bf4> in <listcomp>(.0)\n     14   predictions = est.predict(predict_input_fn)\n     15   print(predictions)\n---> 16   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n     17 \n     18 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in predict(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\n    615         self._create_and_assert_global_step(g)\n    616         features, input_hooks = self._get_features_from_input_fn(\n---> 617             input_fn, ModeKeys.PREDICT)\n    618         estimator_spec = self._call_model_fn(\n    619             features, None, ModeKeys.PREDICT, self.config)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _get_features_from_input_fn(self, input_fn, mode)\n    991   def _get_features_from_input_fn(self, input_fn, mode):\n    992     \"\"\"Extracts the `features` from return values of `input_fn`.\"\"\"\n---> 993     result = self._call_input_fn(input_fn, mode)\n    994     result, _, hooks = estimator_util.parse_input_fn_result(result)\n    995     self._validate_features_in_predict_input(result)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _call_input_fn(self, input_fn, mode, input_context)\n   1111       kwargs['input_context'] = input_context\n   1112     with ops.device('/cpu:0'):\n---> 1113       return input_fn(**kwargs)\n   1114 \n   1115   def _call_model_fn(self, features, labels, mode, config):\n\n/usr/local/lib/python3.6/dist-packages/bert/run_classifier.py in input_fn(params)\n    727   def input_fn(params):\n    728     \"\"\"The actual input function.\"\"\"\n---> 729     batch_size = params[\"batch_size\"]\n    730 \n    731     num_examples = len(features)\n\nKeyError: 'batch_size'\n```\n\nWhen I check the 'batch_size' parameter in the estimator and the loaded model, I see that it is present in the former but not in the latter:\n\n```python\nestimator.params['batch_size'] # 32\n\nest.params['batch_size'] # KeyError: 'batch_size'\n```"
    },
    {
        "role": "assistant",
        "content": "The issue you're facing is related to the fact that when you export a saved model from a `tf.estimator.Estimator`, some parameters like `batch_size` may not be automatically preserved in the saved model. This is a common issue when working with TensorFlow Estimators and SavedModels.\n\nHere's how you can address this issue:\n\n1. **Pass `batch_size` explicitly in your prediction function**: When you are using the `SavedModelEstimator` to make predictions, you need to explicitly pass the `batch_size` parameter. This is because the saved model doesn't inherently retain the batch size information.\n\n2. **Modify the Input Function**: You need to ensure that the `batch_size` is set correctly in your `input_fn`.\n\nHere's how you can modify the `getPrediction1` function to handle this:\n\n```python\ndef getPrediction1(in_sentences, batch_size=32):\n    labels = [\"Irrelevant\", \"Relevant\"]\n    input_examples = [run_classifier.InputExample(guid=\"\", text_a=x, text_b=None, label=0) for x in in_sentences]  # Dummy label\n    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\n    def predict_input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices(dict(input_features))\n        dataset = dataset.batch(batch_size)\n        return dataset\n\n    predictions = est.predict(input_fn=predict_input_fn)\n    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n\n# Use the function to get predictions\npredictions = getPrediction1(pred_sentences, batch_size=32)\nprint(predictions)\n```\n\n### Key Changes:\n\n- **Custom `predict_input_fn`:** Instead of using `run_classifier.input_fn_builder`, define a custom function that creates a dataset and batches it with the desired `batch_size`. This ensures that the batch size is correctly applied when making predictions.\n  \n- **Pass `batch_size`:** Make sure to explicitly pass the `batch_size` parameter to the input function.\n\n### Additional Considerations:\n\n- **Check TensorFlow Version:** Ensure that your TensorFlow version is compatible with the code. Some functions might have different implementations based on the version.\n  \n- **Debugging:** If the issue persists, add print statements to check the structure and types of the input features to ensure everything is as expected.\n\nBy making these adjustments, you should be able to resolve the `KeyError: 'batch_size'` and successfully make predictions with your saved BERT model."
    }
]