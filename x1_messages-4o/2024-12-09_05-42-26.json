[
    {
        "role": "user",
        "content": "I am working on a multi-classification problem using a neural network in scikit-learn. I am trying to optimize my hyperparameters such as the number of layers, perceptrons and others. I've discovered that I can use GridSearchCV for this, but the output gives me the mean accuracy, while I want to obtain the F1-score. How can I adjust my code to return the average F1-score instead of accuracy?\n\nHere's the code I'm using:\n\n```python\nmlp = MLPClassifier(max_iter=600)\nclf = GridSearchCV(mlp, parameter_space, n_jobs= -1, cv = 3)\nclf.fit(X_train, y_train.values.ravel())\n\nparameter_space = {\n    'hidden_layer_sizes': [(1), (2), (3)],\n}\n\nprint('Best parameters found:\\n', clf.best_params_)\n\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n```\n\nThe output:\n\n```python\nBest parameters found:\n {'hidden_layer_sizes': 3}\n0.842 (+/-0.089) for {'hidden_layer_sizes': 1}\n0.882 (+/-0.031) for {'hidden_layer_sizes': 2}\n0.922 (+/-0.059) for {'hidden_layer_sizes': 3}\n```"
    },
    {
        "role": "assistant",
        "content": "To optimize your hyperparameters using the F1-score instead of accuracy, you can modify your `GridSearchCV` object to use a custom scoring function. Scikit-learn provides several built-in scoring functions, including the F1-score. Here\u2019s how you can adjust your code to use the F1-score as the scoring metric:\n\n```python\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import make_scorer, f1_score\n\n# Define your parameter space\nparameter_space = {\n    'hidden_layer_sizes': [(1,), (2,), (3,)],\n}\n\n# Create the MLPClassifier\nmlp = MLPClassifier(max_iter=600)\n\n# Create a custom scorer using F1-score\nf1_scorer = make_scorer(f1_score, average='weighted')  # Use 'weighted' for multi-class\n\n# Use GridSearchCV with the custom scorer\nclf = GridSearchCV(mlp, parameter_space, scoring=f1_scorer, n_jobs=-1, cv=3)\nclf.fit(X_train, y_train.values.ravel())\n\n# Print the best parameters\nprint('Best parameters found:\\n', clf.best_params_)\n\n# Print the mean test F1-scores and standard deviations\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n```\n\n### Key Changes:\n1. **Scoring Parameter:** I changed the `scoring` parameter in `GridSearchCV` to use `make_scorer(f1_score, average='weighted')`. The `average='weighted'` is used for multi-class classification to account for class imbalance.\n\n2. **Parameter Space Definition:** Ensure that you define `hidden_layer_sizes` as tuples (e.g., `(1,)`, `(2,)`, `(3,)`) because it expects a tuple for each layer configuration.\n\n3. **Other Considerations:** If your classification problem is binary, you can use `average='binary'` instead and adjust the `f1_scorer` accordingly.\n\nThis modification will allow you to optimize your model's hyperparameters based on the F1-score, which is often more informative than accuracy, especially in imbalanced datasets."
    }
]