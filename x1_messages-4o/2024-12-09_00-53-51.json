[
    {
        "role": "user",
        "content": "I've noticed that in Torchtext 0.7, Field is being deprecated and will be moved to legacy in 0.8. I am having difficulty finding an example of the new paradigm for custom datasets, not the ones included in torch.datasets, that doesn't use Field. Can anyone provide an up-to-date example to replace the deprecated Field, Examples and BucketIterator paradigm?"
    },
    {
        "role": "assistant",
        "content": "With the deprecation of `Field`, `Example`, and `BucketIterator` in Torchtext, the library is moving towards a more modular and flexible approach by utilizing PyTorch's native data handling utilities. Here's how you can work with custom datasets using the new paradigm in Torchtext:\n\n### Step-by-Step Guide\n\n1. **Define Your Dataset**: Use PyTorch's `Dataset` class to define your own dataset. This involves implementing the `__len__` and `__getitem__` methods.\n\n2. **Tokenization and Vocabulary**: Use Torchtext's `Transforms` for tokenization and `Vocab` for vocabulary management.\n\n3. **Data Iterators**: Use PyTorch's `DataLoader` for batching, shuffling, and loading your data.\n\nHere is an example showing how to set up a custom dataset without using `Field`:\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.transforms import ToTensor, VocabTransform\nfrom torchtext.data.utils import get_tokenizer\n\n# Step 1: Define your custom dataset\nclass CustomTextDataset(Dataset):\n    def __init__(self, text_data, labels, tokenizer):\n        self.text_data = text_data\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        text = self.text_data[idx]\n        label = self.labels[idx]\n        tokens = self.tokenizer(text)\n        return tokens, label\n\n# Example data\ntexts = [\"This is a sentence.\", \"Another sentence.\", \"More text data.\"]\nlabels = [0, 1, 0]\n\n# Step 2: Tokenization\ntokenizer = get_tokenizer(\"basic_english\")\n\n# Create dataset instance\ndataset = CustomTextDataset(texts, labels, tokenizer)\n\n# Step 3: Build vocabulary\ndef yield_tokens(data_iter):\n    for text, _ in data_iter:\n        yield text\n\nvocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# Transform dataset to include vocab indices\ntext_transform = VocabTransform(vocab)\nlabel_transform = ToTensor(dtype=torch.long)\n\n# Data processing function\ndef collate_batch(batch):\n    text_list, label_list = [], []\n    for (_text, _label) in batch:\n        text_list.append(torch.tensor(text_transform(_text), dtype=torch.long))\n        label_list.append(label_transform([_label]))\n    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n    labels = torch.cat(label_list)\n    return text_list, labels\n\n# Step 4: DataLoader\nbatch_size = 2\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n\n# Example iteration over the DataLoader\nfor text_batch, label_batch in dataloader:\n    print(text_batch)\n    print(label_batch)\n```\n\n### Key Points:\n- **Tokenizer**: Use a tokenizer from `torchtext.data.utils`.\n- **Vocabulary**: Build a vocabulary using `build_vocab_from_iterator`.\n- **DataLoader**: Use PyTorch's `DataLoader` with a custom `collate_fn` to handle variable sequence lengths and conversion to indices.\n\nThis new approach provides more flexibility and adheres to the PyTorch ecosystem's design principles. You can further customize the tokenization, vocabulary, and batching process as per your needs."
    }
]