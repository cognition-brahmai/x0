[
    {
        "role": "user",
        "content": "I am currently using PyTorch for fine-tuning a ResNet50 model and I need to set different learning rates for different layers. Specifically, I want the learning rate of the last fully connected layer to be 10^-3, while the learning rate of all other layers should be set to 10^-6. I'm aware that I can follow the method documented in PyTorch's official documentation, as shown below:\n```python\noptim.SGD([{'params': model.base.parameters()},\n           {'params': model.classifier.parameters(), 'lr': 1e-3}], \n          lr=1e-2, momentum=0.9)\n```\nHowever, I'm wondering if there is a way to achieve this without having to set the parameters for each layer individually?"
    },
    {
        "role": "assistant",
        "content": "Yes, you can achieve this by using parameter groups in PyTorch's optimizer without explicitly specifying each layer individually. The idea is to separate the parameters of the last fully connected layer from the rest and assign them to different parameter groups with different learning rates. Here's how you can do it with a ResNet50 model:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# Load a pre-trained ResNet50 model\nmodel = models.resnet50(pretrained=True)\n\n# Assuming the fully connected layer is the last layer of the model\n# and is named 'fc' in ResNet\nfc_params = list(model.fc.parameters())\nother_params = [param for name, param in model.named_parameters() if 'fc' not in name]\n\n# Define the optimizer with different learning rates for different parameter groups\noptimizer = optim.SGD([\n    {'params': other_params, 'lr': 1e-6},\n    {'params': fc_params, 'lr': 1e-3}\n], momentum=0.9)\n\n# Now you can proceed with your training loop\n```\n\nIn this approach, `model.named_parameters()` is used to iterate over model parameters with their names. This allows you to separate the parameters of the last fully connected layer (`fc`) from the rest. By creating two parameter groups, one for `other_params` with a learning rate of `1e-6` and one for `fc_params` with a learning rate of `1e-3`, you can effectively control the learning rate for different parts of the model without needing to set the parameters for each layer individually."
    }
]