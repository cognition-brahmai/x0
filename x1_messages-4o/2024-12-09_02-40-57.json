[
    {
        "role": "user",
        "content": "I am trying to send a large pandas.DataFrame to a remote server running MS SQL. I am currently converting the DataFrame to a list of tuples and using the `executemany()` function from pyODBC to send the data. I tried making my code more readable by using the `to_sql()` method from pandas, but this has resulted in my upload being at least 150 times slower. Is there a way to enable `fast_executemany` when using SQLAlchemy? I am using pandas-0.20.3, pyODBC-4.0.21 and sqlalchemy-1.1.13.\n\nHere is my code:\n\n```python\nimport pyodbc as pdb\n\nlist_of_tuples = convert_df(data_frame)\n\nconnection = pdb.connect(cnxn_str)\n\ncursor = connection.cursor()\ncursor.fast_executemany = True\ncursor.executemany(sql_statement, list_of_tuples)\nconnection.commit()\n\ncursor.close()\nconnection.close()\n```\n\nAnd my attempt at using `to_sql()`:\n\n```python\nimport sqlalchemy as sa\n\nengine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % cnxn_str)\ndata_frame.to_sql(table_name, engine, index=False)\n```"
    },
    {
        "role": "assistant",
        "content": "To improve the performance of the `to_sql()` method when using SQLAlchemy with MS SQL Server and enable `fast_executemany`, you'll need to modify the connection setup to utilize the `fast_executemany` feature available in pyODBC. This feature is not directly exposed through SQLAlchemy, but you can configure it by customizing the connection creation function.\n\nHere's how you can do it:\n\n1. Define a custom function to set `fast_executemany` on the pyODBC connection.\n2. Use that function when creating the SQLAlchemy engine.\n\nHere's an example of how you can achieve this:\n\n```python\nimport pandas as pd\nimport sqlalchemy as sa\nimport pyodbc\n\n# Define a function to set fast_executemany\ndef create_engine_with_fast_executemany(cnxn_str):\n    # Create a pyodbc connection with fast_executemany enabled\n    def connect():\n        connection = pyodbc.connect(cnxn_str)\n        connection.fast_executemany = True\n        return connection\n\n    # Create a SQLAlchemy engine using the custom connect function\n    engine = sa.create_engine(\"mssql+pyodbc://\", creator=connect)\n    return engine\n\n# Your connection string\ncnxn_str = \"your_connection_string_here\"\n\n# Create the engine with fast_executemany enabled\nengine = create_engine_with_fast_executemany(cnxn_str)\n\n# Use to_sql with the customized engine\ndata_frame.to_sql(table_name, engine, index=False, if_exists='append')\n```\n\n### Key Points:\n\n- **Custom Connection Function**: By creating a custom connection function, you directly enable `fast_executemany` on the pyODBC connection.\n- **SQLAlchemy Engine**: Use the `creator` parameter of `create_engine` to pass this custom function, allowing SQLAlchemy to use the optimized connection.\n- **`if_exists='append'`**: Ensure that you use `if_exists='append'` or another appropriate option based on whether you want to replace or append data in the table.\n\nThis approach should significantly improve the performance of the `to_sql()` operation when inserting large datasets into a SQL Server database."
    }
]