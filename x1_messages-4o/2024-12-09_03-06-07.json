[
    {
        "role": "user",
        "content": "I am trying to implement a custom inference pipeline for feature preprocessing in AWS Sagemaker using the python sklearn sdk. I am following the tutorial given here: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb. \n\nThe tutorial uses a script for custom preprocessing pipeline like this:\n\n```python\nfrom sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n```\n\nHowever, I am unable to find a way to send multiple files. I need to send multiple files as I have a custom class that needs to be imported from a custom module for the sklearn pipeline. Without importing, it throws an error `AttributeError: module '__main__' has no attribute 'CustomClassName'` when I try to include the custom class in the same preprocessing.py file, which I believe is related to the way pickle works. Is there a way to send multiple files in AWS Sagemaker SKlearn?"
    },
    {
        "role": "assistant",
        "content": "To address your situation, you want to ensure that all dependencies, including custom classes and modules, are available to your script when it runs in the SageMaker environment. In AWS SageMaker, you can package multiple files and modules together to be used in your training or inference script. Here's a step-by-step guide to achieve this:\n\n### Packaging Your Code\n\n1. **Organize Your Code:**\n   - Create a directory structure where your main script (e.g., `preprocessing.py`) and any other modules or files are organized as a package. For example:\n\n     ```\n     my_package/\n     \u251c\u2500\u2500 __init__.py\n     \u251c\u2500\u2500 preprocessing.py\n     \u2514\u2500\u2500 custom_module.py\n     ```\n\n2. **Create a tar.gz Archive:**\n   - Navigate to the directory containing `my_package`.\n   - Use the following command to create a tar.gz archive:\n\n     ```bash\n     tar -czvf my_package.tar.gz my_package/\n     ```\n\n3. **Upload the tar.gz to S3:**\n   - Upload this archive to an S3 bucket.\n\n### Using the Archive with SKLearn Estimator\n\n4. **Modify Your SKLearn Estimator:**\n   - Specify the `source_dir` parameter in the `SKLearn` estimator to point to your tar.gz archive in S3.\n\n   ```python\n   from sagemaker.sklearn.estimator import SKLearn\n\n   sklearn_preprocessor = SKLearn(\n       entry_point='preprocessing.py',\n       source_dir='s3://your-bucket/my_package.tar.gz',\n       role=role,\n       train_instance_type=\"ml.c4.xlarge\",\n       sagemaker_session=sagemaker_session\n   )\n   ```\n\n### Considerations\n\n- **Ensure All Imports are Correct:** Make sure your `preprocessing.py` correctly imports your custom modules, e.g., `from custom_module import CustomClassName`.\n  \n- **Dependencies:** If you have any Python package dependencies, you may need to provide a `requirements.txt` file in your `my_package` directory. SageMaker will automatically install these dependencies.\n\n- **Entry Point Script:** The `entry_point` file, here `preprocessing.py`, should be able to locate and import your custom modules. The `source_dir` will be the root directory during execution, so all imports should be relative to this directory.\n\nBy packaging your custom modules and scripts into an archive and specifying it in the `source_dir`, you ensure that all necessary files are available during the execution of your script in SageMaker."
    }
]