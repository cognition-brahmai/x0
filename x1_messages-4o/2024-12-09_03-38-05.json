[
    {
        "role": "user",
        "content": "I recently built Apache Spark 1.4 from the GH development master and the build process was successful. However, whenever I execute `bin/pyspark`, it defaults to Python 2.7.9. How can I configure it to use Python 3 instead?"
    },
    {
        "role": "assistant",
        "content": "To configure Apache Spark to use Python 3 instead of the default Python 2.7.9, you'll need to set the `PYSPARK_PYTHON` environment variable to point to your Python 3 interpreter. Here\u2019s how you can do it:\n\n1. **Find the path to your Python 3 interpreter.** You can do this by running the following command in your terminal:\n\n   ```bash\n   which python3\n   ```\n\n   This will return the path to the Python 3 executable, for example, `/usr/bin/python3`.\n\n2. **Set the `PYSPARK_PYTHON` environment variable.** You can set this environment variable in your shell configuration file (like `.bashrc`, `.bash_profile`, or `.zshrc`), or you can set it temporarily in your terminal session. Here\u2019s how to set it temporarily:\n\n   ```bash\n   export PYSPARK_PYTHON=/usr/bin/python3\n   ```\n\n   Replace `/usr/bin/python3` with the path you found in step 1.\n\n3. **Run PySpark with the updated configuration.** Now, when you run `bin/pyspark`, it should use Python 3.\n\nIf you want this change to be permanent, you should add the `export` line to your shell configuration file (like `.bashrc`, `.bash_profile`, or `.zshrc`). After adding it to the file, don't forget to source the file or restart your terminal session to apply the changes:\n\n```bash\nsource ~/.bashrc\n```\n\nor\n\n```bash\nsource ~/.bash_profile\n```\n\nor\n\n```bash\nsource ~/.zshrc\n```\n\nBy following these steps, PySpark should now use Python 3 as the interpreter."
    }
]