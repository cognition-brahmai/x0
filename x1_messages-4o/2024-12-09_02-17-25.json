[
    {
        "role": "user",
        "content": "I'm working on a Natural Language Processing task and I need to calculate the co-occurrence matrix over documents. I have a matrix of shape `(n, length)`, where each row represents a sentence composed by `length` words. So there are `n` sentences with the same length in all. With a defined context size, for instance, `window_size = 5`, I want to calculate the co-occurrence matrix `D`, where the entry in the `cth` row and `wth` column is `#(w,c)`, that indicates the number of times that a context word `c` appears in `w`'s context. I understand that it can be calculated by stacking loops, but I want to know if there exists a simple way or a simple function? I have found some answers but they can't work with a window sliding through the sentence. Can anyone tell me if there's any function in Python that can handle this problem concisely? I believe this task is quite common in NLP tasks."
    },
    {
        "role": "assistant",
        "content": "Calculating a co-occurrence matrix using a sliding window is a common task in NLP, and while there may not be a single built-in function in Python that meets all your needs out-of-the-box, you can achieve this effectively using libraries such as NumPy or by leveraging the `scipy.sparse` module for efficiency, especially if you are dealing with a large vocabulary.\n\nHere\u2019s a concise way to compute a co-occurrence matrix using Python:\n\n1. **Using `collections.Counter` and `itertools.combinations`:**\n\n   This approach uses a dictionary to store co-occurrence counts.\n\n   ```python\n   import numpy as np\n   from collections import Counter\n   from itertools import combinations\n\n   def calculate_co_occurrence_matrix(sentences, window_size):\n       vocab = {word: idx for idx, word in enumerate(set(word for sentence in sentences for word in sentence))}\n       co_occurrence = Counter()\n\n       for sentence in sentences:\n           for i, word in enumerate(sentence):\n               window_start = max(0, i - window_size)\n               window_end = min(len(sentence), i + window_size + 1)\n               context = sentence[window_start:i] + sentence[i+1:window_end]\n               word_idx = vocab[word]\n               for context_word in context:\n                   context_word_idx = vocab[context_word]\n                   if word_idx != context_word_idx:\n                       co_occurrence[(word_idx, context_word_idx)] += 1\n\n       matrix_size = len(vocab)\n       co_occurrence_matrix = np.zeros((matrix_size, matrix_size), dtype=int)\n\n       for (word_idx, context_word_idx), count in co_occurrence.items():\n           co_occurrence_matrix[word_idx, context_word_idx] = count\n\n       return co_occurrence_matrix, vocab\n\n   # Example usage:\n   sentences = [\n       ['word1', 'word2', 'word3', 'word4'],\n       ['word2', 'word3', 'word4', 'word5']\n   ]\n\n   window_size = 5\n   co_occurrence_matrix, vocab = calculate_co_occurrence_matrix(sentences, window_size)\n   print(co_occurrence_matrix)\n   ```\n\n2. **Using `scipy.sparse` for Memory Efficiency:**\n\n   If you're dealing with large vocabularies, using sparse matrices will save memory.\n\n   ```python\n   import numpy as np\n   from scipy.sparse import lil_matrix\n   from collections import defaultdict\n\n   def calculate_co_occurrence_matrix_sparse(sentences, window_size):\n       vocab = {word: idx for idx, word in enumerate(set(word for sentence in sentences for word in sentence))}\n       matrix_size = len(vocab)\n       co_occurrence = lil_matrix((matrix_size, matrix_size), dtype=int)\n\n       for sentence in sentences:\n           for i, word in enumerate(sentence):\n               window_start = max(0, i - window_size)\n               window_end = min(len(sentence), i + window_size + 1)\n               context = sentence[window_start:i] + sentence[i+1:window_end]\n               word_idx = vocab[word]\n               for context_word in context:\n                   context_word_idx = vocab[context_word]\n                   if word_idx != context_word_idx:\n                       co_occurrence[word_idx, context_word_idx] += 1\n\n       return co_occurrence, vocab\n\n   # Example usage:\n   sentences = [\n       ['word1', 'word2', 'word3', 'word4'],\n       ['word2', 'word3', 'word4', 'word5']\n   ]\n\n   window_size = 5\n   co_occurrence_matrix_sparse, vocab = calculate_co_occurrence_matrix_sparse(sentences, window_size)\n   print(co_occurrence_matrix_sparse.toarray())\n   ```\n\nThese examples show how to compute the co-occurrence matrix using a sliding window approach. You can choose between using dense or sparse matrices depending on your specific memory and computational requirements."
    }
]