[
    {
        "role": "user",
        "content": "Why are the validation loss (val_loss) and validation accuracy (val_acc) only displayed at the end of training? I am using Keras for a machine learning model, and when the training starts, only the loss and accuracy (acc) are displayed in each line of the run window. I want to see the validation loss and validation accuracy in each line too. Here is the code I am using for the model:\n\n```python\nmodel.add(Flatten())\nmodel.add(Dense(512, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation=\"softmax\"))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=\"adam\",\n    metrics=['accuracy']\n)\n\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32, \n    epochs=1, \n    validation_data=(x_test, y_test),\n    shuffle=True\n)\n```\n\nWhen the training starts, it looks like this:\n\n```python\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/1\n\n   32/50000 [..............................] - ETA: 34:53 - loss: 2.3528 - acc: 0.0938\n   64/50000 [..............................] - ETA: 18:56 - loss: 2.3131 - acc: 0.0938\n   96/50000 [..............................] - ETA: 13:45 - loss: 2.3398 - acc: 0.1146\n```\n\nAnd when it finishes, it looks like this:\n\n```python\n49984/50000 [============================>.] - ETA: 0s - loss: 1.5317 - acc: 0.4377\n50000/50000 [==============================] - 231s 5ms/step - loss: 1.5317 - acc: 0.4378 - val_loss: 1.1503 - val_acc: 0.5951\n```"
    },
    {
        "role": "assistant",
        "content": "In Keras, by default, the training output you see during the training process shows only the training loss and accuracy for each batch. The validation loss and accuracy are computed at the end of each epoch, which is why you only see them at the end.\n\nIf you want to see the validation loss and accuracy more frequently, you can use the `verbose` parameter of the `fit` function. However, this will still only show the validation metrics at the end of each epoch, not after each batch. Here's how you can use the `verbose` parameter:\n\n```python\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32, \n    epochs=1, \n    validation_data=(x_test, y_test),\n    shuffle=True,\n    verbose=1  # Use 1 for progress bar logging, 2 for one log line per epoch.\n)\n```\n\nIf you want to monitor validation metrics more frequently, you can implement a custom callback. Here's an example of how to do that:\n\n```python\nfrom keras.callbacks import Callback\nimport numpy as np\n\nclass ValidationMetrics(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        x_val, y_val = self.validation_data[:2]\n        val_loss, val_acc = self.model.evaluate(x_val, y_val, verbose=0)\n        print(f'\\nEpoch {epoch+1} - val_loss: {val_loss} - val_acc: {val_acc}\\n')\n\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32, \n    epochs=1, \n    validation_data=(x_test, y_test),\n    shuffle=True,\n    callbacks=[ValidationMetrics()]\n)\n```\n\nThis custom callback will output the validation loss and accuracy at the end of each epoch, allowing you to see it immediately after the epoch output. Keep in mind that evaluating on the validation set after each batch is computationally expensive and generally not recommended unless necessary."
    }
]