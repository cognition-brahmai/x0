[
    {
        "role": "user",
        "content": "I am working with a Keras model where I need to compute my own metric (perplexity), which requires the unnormalized probabilities/logits. The existing model only returns softmax probabilities. The model I'm currently using is as follows:\n\n```python\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(n_hidden, return_sequences=False))\nmodel.add(Dropout(dropout_keep_prob))\nmodel.add(Dense(vocab_size))\nmodel.add(Activation('softmax'))\noptimizer = RMSprop(lr=self.lr)\n\nmodel.compile(optimizer=optimizer, \nloss='sparse_categorical_crossentropy')\n```\n\nFrom my understanding, the Keras FAQ suggests getting the output of intermediate layers. However, the solutions provided store the intermediate outputs in a different model, which is not what I require. I need to use the logits for my custom metric, and have this custom metric included in the `model.compile()` function so it's evaluated and displayed during training. Essentially, I don't require the output of the Dense layer separated in a different model, but as part of my original model.\n\nTherefore, my questions are as follows:\n\n1. If I define a custom metric as outlined in the Keras documentation using `def custom_metric(y_true, y_pred)`, does `y_pred` contain logits or normalized probabilities?\n2. If `y_pred` contains normalized probabilities, how can I get the unnormalized probabilities, i.e., the logits output by the Dense layer?"
    },
    {
        "role": "assistant",
        "content": "To address your questions about computing a custom metric like perplexity using logits in your Keras model, let's break down the problem and offer a solution:\n\n1. **Understanding `y_pred` in Custom Metrics**:\n   - When you define a custom metric in Keras using `def custom_metric(y_true, y_pred)`, the `y_pred` parameter typically contains the output of the last layer of your model. In your case, since the last layer is a `softmax` activation, `y_pred` will contain the normalized probabilities, not the logits.\n\n2. **Accessing Logits for Custom Metrics**:\n   - Since `y_pred` is post-softmax, you need to access the logits (i.e., the output of the Dense layer before applying the softmax activation) to compute perplexity or any other metric that requires them.\n\nHere is a strategy to achieve this:\n\n### Modify the Model to Access Logits\n\nYou can modify your model to output logits by altering the architecture slightly. Instead of using the `Activation('softmax')` layer in your Sequential model, compute the softmax in the loss function and within your custom metric. This way, you can access logits directly:\n\n```python\nfrom keras.models import Model\nimport keras.backend as K\n\n# Define the model as usual\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(n_hidden, return_sequences=False))\nmodel.add(Dropout(dropout_keep_prob))\ndense_output = Dense(vocab_size)(model.output)  # Get logits\n# Do not add the softmax activation here\n\n# Define a custom loss function using logits\ndef custom_loss(y_true, logits):\n    return K.sparse_categorical_crossentropy(y_true, logits, from_logits=True)\n\n# Define a custom metric using logits\ndef perplexity(y_true, logits):\n    # Compute softmax probabilities from logits\n    probs = K.softmax(logits)\n    # Calculate perplexity (you can define it as per your requirement)\n    cross_entropy = K.sparse_categorical_crossentropy(y_true, logits, from_logits=True)\n    perplexity = K.exp(K.mean(cross_entropy))\n    return perplexity\n\n# Compile the model with custom loss and metric\nmodel = Model(inputs=model.input, outputs=dense_output)  # Create a new Model with logits as output\nmodel.compile(optimizer=optimizer, \n              loss=custom_loss,\n              metrics=[perplexity])\n```\n\n### Key Points\n- **Logits Output**: The model now outputs logits directly, and you compute the softmax as part of the loss and metric functions.\n- **Custom Loss and Metric**: Both the custom loss and metric functions take `logits` as input, allowing you to calculate necessary values like cross-entropy and perplexity with logits.\n\nThis setup allows you to train your model while having direct access to logits within your custom metric and loss functions, ensuring you can compute perplexity or any other metric that requires logits."
    }
]