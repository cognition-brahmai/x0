[
    {
        "role": "user",
        "content": "I'm having trouble understanding the setup of my binary classification problem in scikit-learn. I've labeled the positive class as 1 and the negative class as 0, but I believe scikit-learn defaults to using class 0 as the positive class in its confusion matrix. This is causing some confusion. Is the top row in scikit-learn's default setting the positive or negative class? Given a confusion matrix output like this:\n\n```python\nconfusion_matrix(y_test, preds)\n [ [30  5]\n    [2 42] ]\n```\n\nWhat would the confusion matrix look like? Are the actual instances represented by the rows or the columns in scikit-learn?\n\n```python\n          prediction                        prediction\n           0       1                          1       0\n         -----   -----                      -----   -----\n      0 | TN   |  FP        (OR)         1 |  TP  |  FP\nactual   -----   -----             actual   -----   -----\n      1 | FN   |  TP                     0 |  FN  |  TN\n```"
    },
    {
        "role": "assistant",
        "content": "In scikit-learn, the confusion matrix uses the convention where rows represent the true classes (actual labels) and columns represent the predicted classes. By default, scikit-learn orders the classes in ascending order based on their labels, which means that class `0` is typically considered first, followed by class `1`.\n\nGiven your confusion matrix output:\n\n```\n[ [30  5]\n  [2 42] ]\n```\n\nThis matrix is interpreted as follows:\n\n- The first row `[30, 5]` corresponds to the true class `0`.\n- The second row `[2, 42]` corresponds to the true class `1`.\n\nSo, the confusion matrix structure is:\n\n```\n          Prediction\n           0       1\n         -----   -----\n      0 |  30  |  5  |  True Negative (TN) and False Positive (FP)\nActual   -----   -----\n      1 |  2   |  42 |  False Negative (FN) and True Positive (TP)\n```\n\nTherefore:\n- `30` is the number of true negatives (TN): the model correctly predicted class `0`.\n- `5` is the number of false positives (FP): the model incorrectly predicted class `1` when the true class was `0`.\n- `2` is the number of false negatives (FN): the model incorrectly predicted class `0` when the true class was `1`.\n- `42` is the number of true positives (TP): the model correctly predicted class `1`.\n\nIn terms of the confusion matrix layout you showed, it aligns with the first layout:\n\n```\n          Prediction\n           0       1\n         -----   -----\n      0 |  TN  |  FP\nActual   -----   -----\n      1 |  FN  |  TP\n```\n\nIn this case, the top row is for the negative class (`0`), and the bottom row is for the positive class (`1`)."
    }
]