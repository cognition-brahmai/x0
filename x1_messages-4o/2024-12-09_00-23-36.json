[
    {
        "role": "user",
        "content": "How can I add confidence intervals to the ROC curve using scikit-learn? I have successfully plotted an ROC curve using the following code: `fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=1)`, where `y_true` is a list of true labels (0 for negative cases and 1 for positive cases), and `y_pred` is a corresponding list of predicted scores. However, I haven't found an easy way to add confidence intervals to this curve."
    },
    {
        "role": "assistant",
        "content": "Adding confidence intervals to an ROC curve can be a bit challenging with scikit-learn alone, as it does not directly provide a built-in function to calculate these intervals. However, you can achieve this by using bootstrapping to estimate the variability of the ROC curve. Here's a step-by-step guide on how to do it:\n\n1. **Bootstrap Sampling**: Generate multiple bootstrap samples of your data. For each sample, compute the ROC curve.\n\n2. **Calculate ROC for Each Sample**: For each bootstrap sample, calculate the true positive rate (TPR) and false positive rate (FPR) using `roc_curve`. Save these results.\n\n3. **Compute Confidence Intervals**: For each point on the ROC curve, calculate the confidence intervals for the TPR across all bootstrap samples.\n\nHere's some sample code to help you implement this:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\n\n# Your true labels and predicted scores\ny_true = [...]\ny_pred = [...]\n\n# Number of bootstrap samples\nn_bootstraps = 1000\nrng_seed = 42  # For reproducibility\nrng = np.random.RandomState(rng_seed)\n\n# Arrays to store the true positive rates for each bootstrap sample\ntprs = []\nbase_fpr = np.linspace(0, 1, 101)\n\nfor i in range(n_bootstraps):\n    # Resample with replacement\n    indices = rng.randint(0, len(y_pred), len(y_pred))\n    if len(np.unique(y_true[indices])) < 2:\n        # Skip if the bootstrap sample doesn't contain both classes\n        continue\n\n    y_true_bootstrap = y_true[indices]\n    y_pred_bootstrap = y_pred[indices]\n\n    # Compute the ROC curve\n    fpr, tpr, _ = roc_curve(y_true_bootstrap, y_pred_bootstrap, pos_label=1)\n\n    # Interpolate the tpr for the base fpr\n    tpr_interpolated = np.interp(base_fpr, fpr, tpr)\n    tpr_interpolated[0] = 0.0\n    tprs.append(tpr_interpolated)\n\ntprs = np.array(tprs)\n\n# Calculate the mean and standard deviation of the true positive rates\nmean_tpr = tprs.mean(axis=0)\nstd_tpr = tprs.std(axis=0)\n\n# Calculate the confidence intervals\ntpr_upper = np.minimum(mean_tpr + std_tpr, 1)\ntpr_lower = np.maximum(mean_tpr - std_tpr, 0)\n\n# Plot the ROC curve with confidence intervals\nplt.figure(figsize=(8, 6))\nplt.plot(base_fpr, mean_tpr, color='b', label='Mean ROC')\nplt.fill_between(base_fpr, tpr_lower, tpr_upper, color='grey', alpha=0.3, label='\u00b1 1 std. dev.')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve with Confidence Intervals')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n### Explanation:\n- **Bootstrap Sampling**: We create multiple resampled datasets and calculate the ROC curve for each. This allows us to estimate the variability of the ROC curve.\n- **Interpolation**: Since the FPR varies for each sample, we interpolate the TPR to a common FPR scale.\n- **Mean and Standard Deviation**: Compute these for the interpolated TPRs to estimate the confidence intervals.\n- **Plotting**: Plot the mean ROC curve and shade the area between the lower and upper bounds of the confidence interval.\n\nThis approach gives you a visual indication of the variability in your ROC curve estimates."
    }
]