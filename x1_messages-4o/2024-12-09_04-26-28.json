[
    {
        "role": "user",
        "content": "How can I correctly use the official Batch Normalization layer in TensorFlow? I tried to use batch normalization to train my Neural Networks using TensorFlow, but it was unclear to me how to use the official layer implementation of Batch Normalization. After some research and debugging, I found that one needs a 'tf.cond' to use it properly and also a 'reuse=True' flag so that the BN shift and scale variables are properly reused. I have written a short script to test it, but I am not sure how to test it. The code runs with no error messages but returns 'NaNs' unexpectedly. Here is the code:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    is_training=True,\n    reuse=None,\n    trainable=True,\n    scope=scope_bn)\n\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    is_training=False,\n    reuse=True,\n    trainable=True,\n    scope=scope_bn)\n\n    z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return z\n\ndef get_NN_layer(x, input_dim, output_dim, scope, train_phase):\n    with tf.name_scope(scope+'vars'):\n        W = tf.Variable(tf.truncated_normal(shape=[input_dim, output_dim], mean=0.0, stddev=0.1))\n        b = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    with tf.name_scope(scope+'Z'):\n        z = tf.matmul(x,W) + b\n    with tf.name_scope(scope+'BN'):\n        if train_phase is not None:\n            z = batch_norm_layer(z,train_phase,scope+'BN_unit')\n    with tf.name_scope(scope+'A'):\n        a = tf.nn.relu(z)\n    return a\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nx = tf.placeholder(tf.float32, [None, 784])\ntrain_phase = tf.placeholder(tf.bool, name='phase_train')\nhiden_units = 25\nlayer1 = get_NN_layer(x, input_dim=784, output_dim=hiden_units, scope='layer1', train_phase=train_phase)\nW_final = tf.Variable(tf.truncated_normal(shape=[hiden_units, 10], mean=0.0, stddev=0.1))\nb_final = tf.Variable(tf.constant(0.1, shape=[10]))\ny = tf.nn.softmax(tf.matmul(layer1, W_final) + b_final)\n\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.reduce_mean( -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]) )\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    steps = 3000\n    for iter_step in xrange(steps):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        if iter_step%1000 == 0:\n            batch_xstrain, batch_xstrain = batch_xs, batch_ys\n            batch_xcv, batch_ycv = mnist.test.next_batch(5000)\n            batch_xtest, batch_ytest = mnist.test.next_batch(5000)\n            train_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xs, y_:batch_ys, train_phase: False})\n            cv_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xcv, y_:batch_ycv, train_phase: False})\n            test_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xtest, y_:batch_ytest, train_phase: False})\n\n            def do_stuff_with_errors(*args):\n                print args\n            do_stuff_with_errors(train_error, cv_error, test_error)\n        sess.run(fetches=train_step, feed_dict={x: batch_xs, y_:batch_ys, train_phase: True})\n    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, train_phase: False}))\n```\n\nWhen I run this code, I get:\n\n```python\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n(2.3474066, 2.3498712, 2.3461707)\n(0.49414295, 0.88536006, 0.91152304)\n(0.51632041, 0.393666, nan)\n0.9296\n```\n\nI used to get all the last numbers as 'NaN' and now only a few of them. Is everything fine or am I missing something?"
    },
    {
        "role": "assistant",
        "content": "It looks like you're encountering issues with NaNs in your TensorFlow model, potentially due to numerical instability in your computations. Here are some suggestions and improvements that might help you resolve these issues:\n\n1. **Avoid Log of Zero:** When computing the cross-entropy, ensure that the input to `tf.log` is clipped to avoid taking the log of zero, which causes NaNs. You can use a small epsilon value for clipping:\n   ```python\n   epsilon = 1e-10\n   y_clipped = tf.clip_by_value(y, epsilon, 1.0 - epsilon)\n   cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_clipped), reduction_indices=[1]))\n   ```\n\n2. **Use `tf.compat.v1` for TensorFlow 1.x Compatibility:** Since TensorFlow 2.x has some differences, ensure compatibility by using `tf.compat.v1` functions for TensorFlow 1.x code:\n   ```python\n   import tensorflow.compat.v1 as tf\n   tf.disable_v2_behavior()\n   ```\n\n3. **Use `tf.global_variables_initializer()`:** Instead of `tf.initialize_all_variables()`, use `tf.global_variables_initializer()` to initialize variables:\n   ```python\n   sess.run(tf.global_variables_initializer())\n   ```\n\n4. **Batch Normalization in TensorFlow 2.x:** Consider using the official `tf.keras.layers.BatchNormalization` if you switch to TensorFlow 2.x. This layer handles many of the complexities for you.\n\n5. **Update the Batch Normalization Layer:** Ensure that the batch normalization layer is implemented correctly. The `reuse` flag is not needed in TensorFlow 1.14+ if you're using a non-deprecated batch normalization way. Here's a simplified approach:\n   ```python\n   def batch_norm_layer(x, train_phase, scope_bn):\n       return tf.layers.batch_normalization(\n           x,\n           training=train_phase,\n           momentum=0.999,\n           name=scope_bn\n       )\n   ```\n\n6. **Check for Exploding Gradients:** If NaNs persist, consider adding gradient clipping to prevent exploding gradients:\n   ```python\n   optimizer = tf.train.GradientDescentOptimizer(0.5)\n   gvs = optimizer.compute_gradients(cross_entropy)\n   capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n   train_step = optimizer.apply_gradients(capped_gvs)\n   ```\n\n7. **Use Consistent Data for Evaluation:** Ensure that you are using a consistent dataset for evaluation. Sometimes using different splits of the test dataset between evaluations can cause confusion.\n\nHere's a refined version of your code with these improvements integrated:\n\n```python\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    return tf.layers.batch_normalization(\n        x,\n        training=train_phase,\n        momentum=0.999,\n        name=scope_bn\n    )\n\ndef get_NN_layer(x, input_dim, output_dim, scope, train_phase):\n    with tf.name_scope(scope+'vars'):\n        W = tf.Variable(tf.truncated_normal(shape=[input_dim, output_dim], mean=0.0, stddev=0.1))\n        b = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    with tf.name_scope(scope+'Z'):\n        z = tf.matmul(x,W) + b\n    with tf.name_scope(scope+'BN'):\n        if train_phase is not None:\n            z = batch_norm_layer(z, train_phase, scope+'BN_unit')\n    with tf.name_scope(scope+'A'):\n        a = tf.nn.relu(z)\n    return a\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nx = tf.placeholder(tf.float32, [None, 784])\ntrain_phase = tf.placeholder(tf.bool, name='phase_train')\nhidden_units = 25\nlayer1 = get_NN_layer(x, input_dim=784, output_dim=hidden_units, scope='layer1', train_phase=train_phase)\nW_final = tf.Variable(tf.truncated_normal(shape=[hidden_units, 10], mean=0.0, stddev=0.1))\nb_final = tf.Variable(tf.constant(0.1, shape=[10]))\ny = tf.nn.softmax(tf.matmul(layer1, W_final) + b_final)\n\ny_ = tf.placeholder(tf.float32, [None, 10])\nepsilon = 1e-10\ny_clipped = tf.clip_by_value(y, epsilon, 1.0 - epsilon)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_clipped), reduction_indices=[1]))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ngvs = optimizer.compute_gradients(cross_entropy)\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\ntrain_step = optimizer.apply_gradients(capped_gvs)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    steps = 3000\n    for iter_step in range(steps):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        if iter_step % 1000 == 0:\n            batch_xcv, batch_ycv = mnist.test.next_batch(5000)\n            batch_xtest, batch_ytest = mnist.test.next_batch(5000)\n            train_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xs, y_: batch_ys, train_phase: False})\n            cv_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xcv, y_: batch_ycv, train_phase: False})\n            test_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xtest, y_: batch_ytest, train_phase: False})\n\n            def do_stuff_with_errors(*args):\n                print(args)\n            do_stuff_with_errors(train_error, cv_error, test_error)\n\n        sess.run(fetches=train_step, feed_dict={x: batch_xs, y_: batch_ys, train_phase: True})\n    \n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, train_phase: False}))\n```\n\nMake sure you're using a consistent dataset for testing and evaluation, and observe the changes. These adjustments should help stabilize your training and reduce the occurrence of NaNs."
    }
]