[
    {
        "role": "user",
        "content": "How can I print the Learning Rate at each epoch with Adam optimizer in Keras? I am trying to manually set the learning rate because online learning does not work well with Keras when using an adaptive optimizer. This is because the learning rate schedule resets when calling .fit(). However, to manually set the learning rate, I need to know what the learning rate was at the end of the last epoch. I think I can achieve this through a callback but it seems that you have to recalculate it each time and I'm not sure how to do that with Adam. I found a solution for SGD, but I need one for Adam optimizer. The code for SGD is:\n\n```python\nclass SGDLearningRateTracker(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        optimizer = self.model.optimizer\n        lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n        print('\\nLR: {:.6f}\\n'.format(lr))\n```"
    },
    {
        "role": "assistant",
        "content": "To print the learning rate at each epoch when using the Adam optimizer in Keras, you can create a custom callback similar to the one you found for SGD. However, the Adam optimizer has a different mechanism for updating the learning rate, involving parameters such as `beta_1`, `beta_2`, and `epsilon`. The effective learning rate in Adam is influenced by these parameters and the adaptive moment estimates.\n\nHere's how you can create a custom callback to print the learning rate for the Adam optimizer:\n\n```python\nfrom keras.callbacks import Callback\nimport keras.backend as K\n\nclass AdamLearningRateTracker(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        optimizer = self.model.optimizer\n        \n        # Get the learning rate\n        lr = float(K.get_value(optimizer.learning_rate))\n        \n        # Get the decay, if any\n        decay = float(K.get_value(optimizer.decay))\n        \n        # Get beta_1 and beta_2\n        beta_1 = float(K.get_value(optimizer.beta_1))\n        beta_2 = float(K.get_value(optimizer.beta_2))\n        \n        # Get epsilon\n        epsilon = float(K.get_value(optimizer.epsilon))\n        \n        # Calculate the effective learning rate\n        t = K.get_value(optimizer.iterations) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(beta_2, t)) / (1. - K.pow(beta_1, t)))\n        \n        print(f'\\nEpoch {epoch + 1}: learning rate = {lr_t:.6f}, base lr = {lr:.6f}, decay = {decay:.6f}')\n\n# Usage\n# model.fit(..., callbacks=[AdamLearningRateTracker()])\n```\n\nThis custom callback calculates the effective learning rate `lr_t` using the formula for Adam, based on the iteration count `t`, and prints it at the end of each epoch. You can use this callback in your `model.fit()` function to track the learning rate throughout the training process. \n\nNote that when using adaptive learning rate methods like Adam, the learning rate printed is the effective one, considering the running averages and corrections, not just the initial `learning_rate` parameter."
    }
]