[
    {
        "role": "user",
        "content": "How can I read a list of parquet files from S3 as a pandas dataframe using pyarrow? I have tried a couple of methods using boto3, pyarrow, and pandas but they seem inefficient. For instance, I am able to read a single parquet file and a directory of parquet files locally. However, when I try to read from an S3 bucket, I encounter an OSError: \"Passed non-file path: s3n://dsn/to/my/bucket\". I have managed to create a workaround for reading a single file from S3 and getting a pandas dataframe, and a similar workaround to create a pandas dataframe from an S3 folder path. I am looking for a better, more efficient method to achieve this. Is there a connector for pandas using pyarrow? I would prefer not to use pyspark unless it is the only solution available."
    },
    {
        "role": "assistant",
        "content": "You can efficiently read parquet files from S3 into a pandas DataFrame using `pyarrow` and `boto3` without involving `pyspark`. The key is to use `pyarrow`'s S3 filesystem capabilities to read the files directly from the bucket. Here's a step-by-step guide to help you achieve this:\n\n1. **Setup your environment**: Make sure you have the necessary libraries installed. You can install them using pip if you haven't already:\n\n   ```bash\n   pip install boto3 pyarrow pandas\n   ```\n\n2. **Configure your AWS credentials**: Ensure your AWS credentials are set up correctly, either in your environment variables or using the AWS CLI.\n\n3. **Read parquet files from S3**: Use `pyarrow` to interact with S3 and read the parquet files into a DataFrame. Here's a sample code snippet:\n\n   ```python\n   import pyarrow.parquet as pq\n   import pyarrow.fs as fs\n   import pandas as pd\n\n   # Specify your S3 bucket and folder\n   bucket_name = 'your-bucket-name'\n   folder_path = 'path/to/parquet/files/'\n\n   # Create a PyArrow S3 filesystem\n   s3 = fs.S3FileSystem()\n\n   # List the files in the directory\n   file_info = s3.get_file_info(fs.FileSelector(f'{bucket_name}/{folder_path}'))\n   files = [info.path for info in file_info if info.is_file]\n\n   # Read the parquet files into a single DataFrame\n   tables = [pq.read_table(f's3://{file}', filesystem=s3) for file in files]\n   combined_table = pq.concat_tables(tables)\n   df = combined_table.to_pandas()\n\n   # Now you can work with the DataFrame `df`\n   print(df.head())\n   ```\n\nThis method uses `pyarrow`'s S3FileSystem to handle the S3 interactions, which is generally more efficient and cleaner than manually downloading files with `boto3` and then reading them.\n\nMake sure to replace `'your-bucket-name'` and `'path/to/parquet/files/'` with your actual S3 bucket name and directory path where the parquet files are stored. This method loads all files into memory, so ensure you have sufficient memory for your data size."
    }
]