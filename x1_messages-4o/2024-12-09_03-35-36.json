[
    {
        "role": "user",
        "content": "I'm trying to pass two loss functions to a model as Keras allows. However, when I try to train, I'm encountering the following error: \"NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array.\" Can you help me understand and solve this issue? Here are the loss functions and the model I'm using:\n\nLoss functions:\n```python\ndef l_2nd(beta):\n    def loss_2nd(y_true, y_pred):\n        ...\n        return K.mean(t)\n    return loss_2nd\n\ndef l_1st(alpha):\n    def loss_1st(y_true, y_pred):\n        ...\n        return alpha * 2 * tf.linalg.trace(tf.matmul(tf.matmul(Y, L, transpose_a=True), Y)) / batch_size\n    return loss_1st\n```\n\nModel:\n```python\nl2 = K.eval(l_2nd(self.beta))\nl1 = K.eval(l_1st(self.alpha))\nself.model.compile(opt, [l2, l1])\n```\n\nError during training:\n```python\n1.15.0-rc3 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630:\ncalling BaseResourceVariable.__init__ (from\ntensorflow.python.ops.resource_variable_ops) with constraint is\ndeprecated and will be removed in a future version. Instructions for\nupdating: If using Keras pass *_constraint arguments to layers.\n--------------------------------------------------------------------------- \nNotImplementedError                       Traceback (most recent call\nlast) &lt;ipython-input-20-298384dd95ab&gt; in &lt;module&gt;()\n     47                          create_using=nx.DiGraph(), nodetype=None, data=[('weight', int)])\n     48 \n---&gt; 49     model = SDNE(G, hidden_size=[256, 128],)\n     50     model.train(batch_size=100, epochs=40, verbose=2)\n     51     embeddings = model.get_embeddings()\n```"
    },
    {
        "role": "assistant",
        "content": "The issue you're encountering is related to the incorrect way of passing custom loss functions to your model. The error message \"NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array\" typically arises when trying to evaluate symbolic tensors (like those used in TensorFlow/Keras operations) outside of the computational graph, which is not allowed.\n\nHere's a step-by-step guide to fix this issue:\n\n1. **Custom Loss Functions**: Ensure that your custom loss functions are defined correctly and do not use K.eval() when passing them to the model. `K.eval()` is used to evaluate a tensor or variable, not to define a function. Instead, just pass the function itself.\n\n2. **Loss Function Handling**: You should return the loss function directly without evaluating it. Here's how you can define your loss functions:\n\n   ```python\n   def l_2nd(beta):\n       def loss_2nd(y_true, y_pred):\n           # Your computation logic here\n           return K.mean(t)\n       return loss_2nd\n\n   def l_1st(alpha):\n       def loss_1st(y_true, y_pred):\n           # Your computation logic here\n           return alpha * 2 * tf.linalg.trace(tf.matmul(tf.matmul(Y, L, transpose_a=True), Y)) / batch_size\n       return loss_1st\n   ```\n\n3. **Model Compilation**: When compiling the model, directly pass the loss functions returned by `l_2nd()` and `l_1st()` by calling them with the parameters `beta` and `alpha` respectively. You do not need to evaluate these functions using `K.eval()`.\n\n   ```python\n   self.model.compile(optimizer=opt, loss=[l_2nd(self.beta), l_1st(self.alpha)])\n   ```\n\n4. **Ensure Symbolic Operations**: Make sure any operations inside the loss functions are compatible with TensorFlow/Keras symbolic operations. Whenever possible, use TensorFlow/Keras backend functions to maintain compatibility with symbolic tensors.\n\n5. **Debugging**: If the problem persists, try simplifying the loss functions to rule out any specific operation causing the issue. Gradually add complexity to pinpoint the exact cause.\n\nBy following these steps, you should be able to resolve the error and successfully train your model with two custom loss functions."
    }
]