[
    {
        "role": "user",
        "content": "How can I load the MNIST dataset in PyTorch and Torchvision, and divide it into training, validation, and test parts? I have a code snippet to load the dataset but I am unsure about how I can split the training dataset into training and validation subsets using DataLoader. I wish to use the last 10000 examples from the training dataset as a validation dataset.\n\nMy current code is:\n```python\ndef load_dataset():\n    train_loader = torch.utils.data.DataLoader(\n        torchvision.datasets.MNIST(\n            '/data/', train=True, download=True,\n            transform=torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor()])),\n        batch_size=batch_size_train, shuffle=True)\n\n    test_loader = torch.utils.data.DataLoader(\n        torchvision.datasets.MNIST(\n            '/data/', train=False, download=True,\n            transform=torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor()])),\n        batch_size=batch_size_test, shuffle=True)\n```"
    },
    {
        "role": "assistant",
        "content": "To split the MNIST training dataset into training and validation subsets, you can use `torch.utils.data.random_split`. This function allows you to specify how to partition the dataset. Given that you want the last 10,000 examples for validation, you can achieve this by defining the lengths of the training and validation sets accordingly.\n\nHere's how you can modify your code to include a validation split:\n\n```python\nimport torch\nimport torchvision\n\n# Set the batch sizes\nbatch_size_train = 64\nbatch_size_test = 1000\n\ndef load_dataset():\n    # Load the entire MNIST training dataset\n    full_train_dataset = torchvision.datasets.MNIST(\n        '/data/', train=True, download=True,\n        transform=torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor()]))\n    \n    # Define the split sizes\n    train_size = len(full_train_dataset) - 10000\n    val_size = 10000\n\n    # Split the dataset into training and validation sets\n    train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n\n    # Create DataLoader for the training dataset\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size_train, shuffle=True)\n    \n    # Create DataLoader for the validation dataset\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=batch_size_train, shuffle=False)\n\n    # Load the test dataset\n    test_loader = torch.utils.data.DataLoader(\n        torchvision.datasets.MNIST(\n            '/data/', train=False, download=True,\n            transform=torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor()])),\n        batch_size=batch_size_test, shuffle=False)\n    \n    return train_loader, val_loader, test_loader\n\n# Example usage\ntrain_loader, val_loader, test_loader = load_dataset()\n```\n\n### Key Points:\n1. **Data Splitting**: The `random_split` function is used to partition the dataset into training and validation sets. You specify the sizes of each split.\n2. **DataLoader**: Separate `DataLoader`s are created for the training, validation, and test datasets. The validation and test loaders often use `shuffle=False` to ensure consistent evaluation order.\n3. **Batch Sizes**: You can adjust the batch sizes (`batch_size_train` and `batch_size_test`) as needed for your training and evaluation.\n\nThis code now provides loaders for training, validation, and test datasets using the last 10,000 samples from the original training set as the validation set."
    }
]