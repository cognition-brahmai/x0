[
    {
        "role": "user",
        "content": "How can I retrieve the names of subfolders located in an S3 bucket using boto3 in Python? Currently, I'm able to access my S3 bucket using boto3 as shown below:\n\n```python\ns3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket-name')\n```\n\nThe bucket contains a folder `first-level`, which itself contains several sub-folders named with a timestamp, for instance `1456753904534`. I tried to retrieve these sub-folders with the following code:\n\n```python\nobjs = bucket.meta.client.list_objects(Bucket='my-bucket-name')\n```\n\nHowever, this gives me a dictionary, where the key 'Contents' provides all the third-level files instead of the second-level timestamp directories. For example:\n\n```python\n{u'ETag': '\"etag\"', u'Key': 'first-level/1456753904534/part-00014', u'LastModified': datetime.datetime(2016, 2, 29, 13, 52, 24, tzinfo=tzutc()), u'Owner': {u'DisplayName': 'owner', u'ID': 'id'}, u'Size': size, u'StorageClass': 'storageclass'}\n```\n\nHere, I only need the directory name, but this method is giving me the specific files, such as `part-00014`. Although I could strip out the directory name from all the paths, it seems inefficient to retrieve everything at the third level to get the second level.\n\nI've also tried another method suggested in a GitHub issue thread:\n\n```python\nfor o in bucket.objects.filter(Delimiter='/'):\n    print(o.key)\n```\n\nHowever, this doesn't give me the folders at the desired level. Is there a more efficient way to retrieve the names of the subfolders?"
    },
    {
        "role": "assistant",
        "content": "To efficiently retrieve the names of subfolders located at a specific level in an S3 bucket using `boto3`, you can use the `list_objects_v2` method with a `Prefix` and `Delimiter`. This approach allows you to list the common prefixes (which act like folders) within a specific path in your S3 bucket.\n\nHere's how you can modify your code to achieve this:\n\n```python\nimport boto3\n\ndef list_subfolders(bucket_name, prefix):\n    s3_client = boto3.client('s3')\n    response = s3_client.list_objects_v2(\n        Bucket=bucket_name,\n        Prefix=prefix,\n        Delimiter='/'\n    )\n\n    # Extract 'CommonPrefixes', which represent the \"directories\"\n    subfolders = []\n    if 'CommonPrefixes' in response:\n        for cp in response['CommonPrefixes']:\n            subfolders.append(cp['Prefix'].split('/')[-2])  # Get the folder name without trailing slash\n\n    return subfolders\n\nbucket_name = 'my-bucket-name'\nprefix = 'first-level/'  # Specify the prefix to the level you want to list subfolders from\n\nsubfolders = list_subfolders(bucket_name, prefix)\nprint(subfolders)\n```\n\n### Explanation:\n\n1. **Prefix**: You specify the `Prefix` as `'first-level/'`, which tells S3 to list objects starting with this prefix. This ensures you are only looking at the contents within the `'first-level/'` directory.\n\n2. **Delimiter**: By specifying the `Delimiter` as `'/'`, S3 will return the \"subfolder\" names (common prefixes) that appear after the specified prefix. This effectively simulates the directory structure.\n\n3. **CommonPrefixes**: The `list_objects_v2` response will include a `CommonPrefixes` key if there are any prefixes (subfolders) under the specified prefix. You can extract these to get the subfolder names.\n\nThis approach should efficiently give you the names of the subfolders without having to list all files within each subfolder."
    }
]