[
    {
        "role": "user",
        "content": "In the context of Google's BERT (Bidirectional Encoder Representations from Transformers), can someone explain what makes a transformer bidirectional by nature? I understand that a transformer takes keys, values, and queries as input; uses an encoder-decoder architecture; and applies attention to these keys, queries, and values. I also understand that we need to pass tokens explicitly rather than the transformer understanding this by nature. However, I'm confused about the bidirectional nature of Transformers and would like to know why masks are applied to make it unidirectional."
    },
    {
        "role": "assistant",
        "content": "To understand the bidirectional nature of Transformers, particularly in the context of models like BERT (Bidirectional Encoder Representations from Transformers), it's important to look at how attention is applied in these architectures.\n\n1. **Self-Attention Mechanism**: In a transformer model, self-attention allows each word (or token) in a sentence to consider the entire sentence when computing its representation. This means that the model can take into account both the left and right context simultaneously for any given word. This is what makes the transformer inherently \"bidirectional.\"\n\n2. **Encoder-Only Architecture in BERT**: BERT specifically uses an encoder-only architecture, which means it doesn't use the decoder part that is typically used for tasks like translation. Each layer in BERT's encoder processes the input sentence in both directions\u2014left-to-right and right-to-left\u2014simultaneously. This is different from models like GPT (Generative Pre-trained Transformer) which process the input in a unidirectional manner (left-to-right).\n\n3. **Use of Masking**: In the context of transformers, masking is used to control what parts of the input sequence are visible to each token:\n   - **BERT** uses masking during its training phase (Masked Language Modeling, MLM) where random tokens in a sentence are masked, and the model learns to predict these masked tokens by considering the context from both directions. This approach allows BERT to leverage the full context around a word for accurate predictions.\n   - **Unidirectional Models** like GPT mask future tokens in a sequence so that the model can only attend to previous tokens, thus making it inherently unidirectional.\n\n4. **Bidirectional vs. Unidirectional**: The term \"bidirectional\" in BERT refers to its ability to understand context from both sides of a token. In contrast, a unidirectional model only considers past tokens (left context) when predicting the next token, which is suitable for tasks like language generation.\n\nIn summary, the bidirectional nature of transformers like BERT comes from their use of self-attention mechanisms that allow for the simultaneous consideration of both left and right contexts, and the absence of constraints (such as masking future tokens) during training, which is typical in unidirectional models. This bidirectional ability is crucial for tasks that require understanding the full context, such as sentence classification or question answering."
    }
]