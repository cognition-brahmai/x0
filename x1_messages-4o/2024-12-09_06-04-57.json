[
    {
        "role": "user",
        "content": "I have a PySpark DataFrame as illustrated below:\n\n```python\nrdd = sc.parallelize(\n    [\n        (0, \"A\", 223,\"201603\", \"PORT\"), \n        (0, \"A\", 22,\"201602\", \"PORT\"), \n        (0, \"A\", 422,\"201601\", \"DOCK\"), \n        (1,\"B\", 3213,\"201602\", \"DOCK\"), \n        (1,\"B\", 3213,\"201601\", \"PORT\"), \n        (2,\"C\", 2321,\"201601\", \"DOCK\")\n    ]\n)\n\ndf_data = sqlContext.createDataFrame(rdd, [\"id\",\"type\", \"cost\", \"date\", \"ship\"])\n\ndf_data.show()\n```\n\nThis produces:\n\n```python\n+---+----+----+------+----+\n| id|type|cost|  date|ship|\n+---+----+----+------+----+\n|  0|   A| 223|201603|PORT|\n|  0|   A|  22|201602|PORT|\n|  0|   A| 422|201601|DOCK|\n|  1|   B|3213|201602|DOCK|\n|  1|   B|3213|201601|PORT|\n|  2|   C|2321|201601|DOCK|\n+---+----+----+------+----+\n```\n\nI have successfully pivoted this DataFrame based on the 'date' column obtaining the average of 'cost', like this:\n\n```python\ndf_data.groupby(df_data.id, df_data.type).pivot(\"date\").avg(\"cost\").show()\n```\n\nThis gives me:\n\n```python\n+---+----+------+------+------+\n| id|type|201601|201602|201603|\n+---+----+------+------+------+\n|  2|   C|2321.0|  null|  null|\n|  0|   A| 422.0|  22.0| 223.0|\n|  1|   B|3213.0|3213.0|  null|\n+---+----+------+------+------+\n```\n\nHowever, when I attempt to pivot the non-numeric column 'ship', I get an AnalysisException. How can I pivot the DataFrame to get a non-numeric column like 'ship' in PySpark?"
    },
    {
        "role": "assistant",
        "content": "Pivoting non-numeric columns in PySpark can indeed be a bit tricky because operations like average, sum, etc., are not applicable to non-numeric data. Therefore, you need to use an aggregation function that makes sense for non-numeric data, such as collecting the values into a list or concatenating them.\n\nTo pivot the 'ship' column, you can use the `collect_list` or `first` function with `groupBy` and `pivot`. Here's how you can achieve this:\n\n```python\nfrom pyspark.sql.functions import first\n\n# Pivoting the DataFrame using the 'ship' column\npivoted_ship_df = df_data.groupBy(\"id\", \"type\").pivot(\"date\").agg(first(\"ship\"))\n\npivoted_ship_df.show()\n```\n\nIn this example, `first(\"ship\")` is used to get the first occurrence of the 'ship' value for each 'date'. This works well if you have unique combinations of 'id', 'type', and 'date'. If there are multiple values for 'ship' for the same 'id', 'type', and 'date', you might want to use `collect_list` and then handle the list according to your needs.\n\nHere's how you could do it with `collect_list`:\n\n```python\nfrom pyspark.sql.functions import collect_list\n\n# Pivoting the DataFrame using the 'ship' column\npivoted_ship_df = df_data.groupBy(\"id\", \"type\").pivot(\"date\").agg(collect_list(\"ship\"))\n\npivoted_ship_df.show()\n```\n\nThis will give you the 'ship' values as lists, and you can further manipulate these lists if needed. Choose the method that best suits your data and requirements."
    }
]