[
    {
        "role": "user",
        "content": "Why does TensorFlow's documentation refer to the input of a softmax as \"logits\"? According to the TensorFlow documentation, they define the inputs/logits of the softmax as \"Unscaled log probabilities.\" However, other sources such as Wikipedia define a logit as the log of the odds, and the inverse of the sigmoid/logistic function. Is there a mathematical or conventional reason for TensorFlow to use the term \"logits\" for the inputs to a softmax function? Is it not more accurate to refer to them as \"unscaled log probabilities\"? Could it be that TensorFlow is maintaining the same variable name for both binary logistic regression and categorical logistic regression, where the term logit is more applicable?"
    },
    {
        "role": "assistant",
        "content": "The use of the term \"logits\" in TensorFlow's documentation when referring to the inputs of the softmax function can indeed be a bit confusing, especially when considering the traditional definition of a \"logit\" as used in statistics.\n\nIn statistical terms, a \"logit\" usually refers to the log of the odds of a binary event, often used in the context of logistic regression. The logit function is the natural logarithm of the odds of the probability and serves as the inverse of the logistic (sigmoid) function.\n\nIn the context of machine learning frameworks like TensorFlow, however, the term \"logits\" has been adopted more broadly to refer to the raw, unnormalized scores output by a model before they are passed through an activation function like softmax. These scores can indeed be thought of as \"unscaled log probabilities\" because, after applying the softmax function, they are transformed into normalized probabilities.\n\nThe use of \"logits\" in this context is likely a result of several factors:\n\n1. **Consistency Across Models**: In both binary logistic regression and multiclass classification (using softmax), the model outputs a set of scores (logits). By using the same term, TensorFlow maintains consistency in terminology across different types of models.\n\n2. **Historical and Conventional Usage**: Over time, the term \"logits\" has become conventionally used in the machine learning community to refer to these pre-softmax scores, even if it slightly diverges from the traditional statistical definition. It serves as shorthand in many machine learning libraries.\n\n3. **Emphasis on Raw Scores**: By using the term \"logits,\" TensorFlow emphasizes that these are raw scores that need to be transformed into probabilities through a softmax (for multiclass) or sigmoid (for binary) function.\n\nWhile it might be more precise to call them \"unscaled log probabilities,\" the term \"logits\" is now widely accepted and understood within the context of deep learning and neural network frameworks. This conventional usage helps practitioners and developers quickly grasp the idea that these are the intermediate scores produced by the network before normalization into probabilities."
    }
]