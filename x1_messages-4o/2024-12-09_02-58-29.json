[
    {
        "role": "user",
        "content": "I am attempting to implement Principal Coordinate Analysis with different distance metrics using both `skbio` and `sklearn` libraries. However, I noticed that the implementation of `sklearn` produces different results each time while `skbio` produces consistent results. Is there an inherent randomness in Multidimensional Scaling and, specifically, Principal Coordinate Analysis? Is my implementation correct?\n\nHere is the code I used for `skbio`:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nimport skbio\nfrom scipy.spatial import distance\n\nnp.random.seed(0)\n\nDF_data = pd.DataFrame(load_iris().data, \n                       index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\n                       columns = load_iris().feature_names)\n\nSe_targets = pd.Series(load_iris().target, \n                       index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])], \n                       name = \"Species\")\n\nDF_standard = pd.DataFrame(StandardScaler().fit_transform(DF_data), \n                           index = DF_data.index,\n                           columns = DF_data.columns)\n\nAr_dist = distance.squareform(distance.pdist(DF_data, metric=\"braycurtis\"))\nDM_dist = skbio.stats.distance.DistanceMatrix(Ar_dist, ids=DF_standard.index)\nPCoA = skbio.stats.ordination.pcoa(DM_dist)\n```\n\nAnd this is the code for `sklearn`:\n\n```python\nfrom sklearn.manifold import MDS\nimport matplotlib.pyplot as plt\n\nfig, ax=plt.subplots(ncols=5, figsize=(12,3))\nfor rs in range(5):\n    M = MDS(n_components=2, metric=True, random_state=rs, dissimilarity='precomputed')\n    A = M.fit(Ar_dist).embedding_\n    ax[rs].scatter(A[:,0],A[:,1], c=[{0:\"b\", 1:\"g\", 2:\"r\"}[t] for t in Se_targets])\n```"
    },
    {
        "role": "assistant",
        "content": "The difference in results between `skbio` and `sklearn` can be attributed to the inherent randomness in the Multidimensional Scaling (MDS) algorithm used by `sklearn`. Principal Coordinate Analysis (PCoA), as implemented in `skbio`, is a deterministic method based on eigen decomposition, which does not involve randomness and thus yields consistent results across multiple runs. On the other hand, `sklearn`'s MDS involves an iterative optimization procedure that can be influenced by the initial configuration, which is controlled by the `random_state` parameter.\n\n### Key Points:\n\n1. **Deterministic vs. Stochastic**:\n   - **`skbio` PCoA**: This method performs eigen decomposition on the distance matrix, leading to deterministic and reproducible results.\n   - **`sklearn` MDS**: It uses a stress-minimization approach, which is iterative and can be influenced by the initial conditions, leading to potentially different results across runs unless a fixed `random_state` is set.\n\n2. **Random State**:\n   - When using `sklearn`'s MDS, setting the `random_state` parameter ensures that the random initialization is consistent across runs. This is why you're seeing different results when you vary `random_state` in your loop. Each different `random_state` provides a different starting configuration, leading to different final embeddings.\n\n3. **Implementation**:\n   - Your implementation seems correct for both `skbio` and `sklearn`. However, remember that `sklearn`'s MDS will give you different results unless you fix the `random_state` parameter, as you've done in your loop.\n\n### Recommendations:\n\n- If you need reproducible results with MDS, always set the `random_state` to a fixed number. This will ensure that the initial configuration is the same across runs.\n\n- If you want to compare results from `skbio` and `sklearn`, note that they are inherently different methods and may not yield directly comparable results, especially since MDS is solving a different optimization problem.\n\n- Consider using `n_init` parameter in `sklearn`'s MDS for averaging over multiple initializations if you seek a more stable solution.\n\nHere is an updated example using a fixed `random_state` for reproducibility:\n\n```python\nfrom sklearn.manifold import MDS\nimport matplotlib.pyplot as plt\n\n# Use a fixed random_state for reproducibility\nrandom_state = 42\n\nM = MDS(n_components=2, metric=True, random_state=random_state, dissimilarity='precomputed')\nA = M.fit(Ar_dist).embedding_\n\nplt.scatter(A[:,0], A[:,1], c=[{0:\"b\", 1:\"g\", 2:\"r\"}[t] for t in Se_targets])\nplt.title('MDS with fixed random_state')\nplt.show()\n```\n\nThis setup will ensure that the results are consistent across multiple runs."
    }
]