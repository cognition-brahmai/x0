[
    {
        "role": "user",
        "content": "How to pivot multiple columns in Spark SQL using PySpark? I have a PySpark dataframe that I need to pivot more than one column on. I currently have a solution where I create a new dataframe for each column I want to pivot and then join them all together, but I'm not sure if this is the best practice. Is there a better way to do this? Here is my sample dataframe and the current solution I'm using:\n\n```python\nd = [(100,1,23,10),(100,2,45,11),(100,3,67,12),(100,4,78,13),(101,1,23,10),(101,2,45,13),(101,3,67,14),(101,4,78,15),(102,1,23,10),(102,2,45,11),(102,3,67,16),(102,4,78,18)]\nmydf = spark.createDataFrame(d,['id','day','price','units'])\n```\n\nPivot method for price column:\n\n```python\npvtdf = mydf.withColumn('combcol',F.concat(F.lit('price_'),mydf['day'])).groupby('id').pivot('combcol').agg(F.first('price'))\n```\n\nFunction to pivot multiple columns:\n\n```python\ndef pivot_udf(df,*cols):\n    mydf = df.select('id').drop_duplicates()\n    for c in cols:\n       mydf = mydf.join(df.withColumn('combcol',F.concat(F.lit('{}_'.format(c)),df['day'])).groupby('id').pivot('combcol').agg(F.first(c)),'id')\n    return mydf\n\npivot_udf(mydf,'price','units')\n```"
    },
    {
        "role": "assistant",
        "content": "Pivoting multiple columns in a PySpark DataFrame can be a bit tricky, but your current solution using a UDF to pivot each column separately and then joining them seems reasonable. However, there are ways to make it more efficient or concise. Here's a refined approach that avoids creating multiple DataFrames and joins, while utilizing a more advanced handling of column operations:\n\n1. **Use a Single Pivot Operation with Structs**: You can use structs to pivot multiple columns together. This approach reduces the need for multiple joins and keeps the DataFrame operations concise.\n\n2. **Flatten the Result**: After pivoting, you may need to flatten the resulting DataFrame to get the desired output format.\n\nHere's how you can implement this:\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\n# Sample DataFrame\nd = [(100,1,23,10),(100,2,45,11),(100,3,67,12),(100,4,78,13),\n     (101,1,23,10),(101,2,45,13),(101,3,67,14),(101,4,78,15),\n     (102,1,23,10),(102,2,45,11),(102,3,67,16),(102,4,78,18)]\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"day\", IntegerType(), True),\n    StructField(\"price\", IntegerType(), True),\n    StructField(\"units\", IntegerType(), True)\n])\nspark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\nmydf = spark.createDataFrame(data=d, schema=schema)\n\n# Create a struct of the columns to pivot\nmydf = mydf.withColumn(\"price_units\", F.struct(\"price\", \"units\"))\n\n# Pivot on the struct\npivoted_df = mydf.groupBy(\"id\").pivot(\"day\").agg(\n    F.first(\"price_units\").alias(\"price_units\")\n)\n\n# Flatten the struct columns\nfor day in [1, 2, 3, 4]:\n    pivoted_df = pivoted_df.withColumn(f\"price_{day}\", F.col(f\"{day}.price_units.price\")) \\\n                           .withColumn(f\"units_{day}\", F.col(f\"{day}.price_units.units\"))\n\n# Drop the temporary struct columns\npivoted_df = pivoted_df.drop(\"1\", \"2\", \"3\", \"4\")\n\npivoted_df.show()\n```\n\n### Explanation:\n\n- **Struct Creation**: First, create a struct column combining the columns you want to pivot (e.g., 'price' and 'units'). This allows you to pivot them together as a single unit.\n- **Pivoting**: Use the `pivot` method on the combined struct to handle multiple columns simultaneously.\n- **Flattening**: Extract the individual fields from the pivoted struct columns and rename them to the desired output format.\n- **Dropping Temporary Columns**: Finally, remove the intermediate struct columns to clean up the DataFrame.\n\nThis method provides a cleaner and potentially more efficient way to pivot multiple columns without having to perform multiple joins."
    }
]