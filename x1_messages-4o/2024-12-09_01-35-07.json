[
    {
        "role": "user",
        "content": "How can I use BertForMaskedLM or BertForMaskedLM to calculate the perplexity of a sentence? I've written some code for this, but I have also noticed BertForMaskedLM's parameter `masked_lm_labels`. Could I use this to calculate the perplexity of a sentence more easily? I understand that the `input_ids` argument is the masked input and the `masked_lm_labels` argument is the desired output, but I am struggling to understand the actual meaning of the output loss.\n\nMy code is as follows:\n\n```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForMaskedLM\n# Load pre-trained model (weights)\nwith torch.no_grad():\n    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')\n    model.eval()\n    # Load pre-trained model tokenizer (vocabulary)\n    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n    sentence = \"\u6211\u4e0d\u4f1a\u5fd8\u8bb0\u548c\u4f60\u4e00\u8d77\u594b\u6597\u7684\u65f6\u5149\u3002\"\n    tokenize_input = tokenizer.tokenize(sentence)\n    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n    sen_len = len(tokenize_input)\n    sentence_loss = 0.\n\n    for i, word in enumerate(tokenize_input):\n        # add mask to i-th character of the sentence\n        tokenize_input[i] = '[MASK]'\n        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n\n        output = model(mask_input)\n\n        prediction_scores = output[0]\n        softmax = nn.Softmax(dim=0)\n        ps = softmax(prediction_scores[0, i]).log()\n        word_loss = ps[tensor_input[0, i]]\n        sentence_loss += word_loss.item()\n\n        tokenize_input[i] = word\n    ppl = np.exp(-sentence_loss/sen_len)\n    print(ppl)\n```\n\nThe loss output is as follows:\n\n```python\nif masked_lm_labels is not None:\n    loss_fct = CrossEntropyLoss()  # -100 index = padding token\n    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), \n    masked_lm_labels.view(-1))\n    outputs = (masked_lm_loss,) + outputs\n```"
    },
    {
        "role": "assistant",
        "content": "To calculate the perplexity of a sentence using `BertForMaskedLM`, you're on the right track with your approach. Perplexity is a measure of how well a probability distribution or probability model predicts a sample. In the context of language models, it is used to evaluate how well the model predicts a sequence of words.\n\n### Understanding Masked Language Modeling with BERT\n\nBERT is a bi-directional transformer model that is trained on a masked language modeling (MLM) task. This means it predicts a masked word in a sentence given its context. Perplexity isn't straightforward to calculate for BERT because it's not a generative model like GPT that predicts the next word in a sequence.\n\n### Calculating Perplexity with BERT\n\nPerplexity is defined as the exponential of the average negative log-likelihood of a sequence. You can calculate it by summing the negative log probabilities of each word in the sentence being correctly predicted and then exponentiating the average of these values.\n\nHere's how you can interpret and modify your code to leverage the output loss calculation for perplexity:\n\n#### Key Points:\n\n1. **Masked LM Loss**: The `masked_lm_labels` parameter is used internally to calculate the cross-entropy loss between the predicted token distributions and the true tokens. For perplexity, this isn't directly useful unless you're modifying the model to predict the entire sequence rather than masked tokens.\n\n2. **Softmax and Log Probability**: You correctly apply softmax to convert logits to probabilities and log to get log probabilities.\n\n3. **Avoid Masked LM Loss for Perplexity**: Since BERT isn't designed for generative tasks, the loss computed using `masked_lm_labels` wouldn't directly give you perplexity. Instead, calculate the log probability of each word as you've done, without using `masked_lm_labels`.\n\n4. **Iterate Over Each Token**: For each token, mask it, predict, and compute the negative log probability of the true word.\n\nHere's a refined version of your code that correctly calculates perplexity:\n\n```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForMaskedLM\n\n# Load pre-trained model and tokenizer\nmodel = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')\nmodel.eval()\ntokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n\nsentence = \"\u6211\u4e0d\u4f1a\u5fd8\u8bb0\u548c\u4f60\u4e00\u8d77\u594b\u6597\u7684\u65f6\u5149\u3002\"\ntokenize_input = tokenizer.tokenize(sentence)\ntensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\nsen_len = len(tokenize_input)\nsentence_loss = 0.\n\n# Calculate perplexity\nwith torch.no_grad():\n    for i, word in enumerate(tokenize_input):\n        # Mask the i-th token\n        original_token_id = tensor_input[0, i].item()\n        input_ids = tensor_input.clone()\n        input_ids[0, i] = tokenizer.mask_token_id\n\n        # Predict the masked token\n        outputs = model(input_ids)\n        prediction_scores = outputs.logits\n\n        # Calculate log probability of the true word\n        softmax = nn.Softmax(dim=-1)\n        word_prob = softmax(prediction_scores[0, i])\n        word_log_prob = torch.log(word_prob[original_token_id])\n        \n        # Accumulate sentence loss\n        sentence_loss += word_log_prob.item()\n\n    # Calculate perplexity\n    ppl = np.exp(-sentence_loss / sen_len)\n    print(ppl)\n```\n\n### Explanation:\n\n- **Masking**: You replace each token with `[MASK]` one at a time and calculate how well the model predicts the masked token.\n- **Log Probabilities**: Calculate the log probability of the actual token at the masked index.\n- **Perplexity Calculation**: Compute the exponential of the negative average log probability.\n\nThis approach gives an approximation of perplexity using BERT, acknowledging its limitations for this task due to its masked language model nature."
    }
]