[
    {
        "role": "user",
        "content": "I am working on multiplying integer matrices using NumPy/SciPy in a multi-threaded environment. I noticed that when I use 64-bit floats, the program uses multiple cores and performs optimally. However, when I use 8-bit integer arrays, the code doesn't utilize multiple cores and runs much slower. Here are the codes that I have tried and the performance results.\n\nCode 1:\n```python\nimport numpy as np\na = np.random.rand(10**4, 10**4)\nb = np.dot(a, a)\n```\n\nCode 2:\n```python\na = np.random.randint(2, size=(n, n)).astype(np.int8)\n```\n\nPerformance Results:\n```python\narray: np.random.randint(2, size=shape).astype(dtype)\n\ndtype    shape          %time (average)\n\nfloat32 (2000, 2000)    62.5 ms\nfloat32 (3000, 3000)    219 ms\nfloat32 (4000, 4000)    328 ms\nfloat32 (10000, 10000)  4.09 s\n\nint8    (2000, 2000)    13 seconds\nint8    (3000, 3000)    3min 26s\nint8    (4000, 4000)    12min 20s\nint8    (10000, 10000)  It didn't finish in 6 hours\n\nfloat16 (2000, 2000)    2min 25s\nfloat16 (3000, 3000)    Not tested\nfloat16 (4000, 4000)    Not tested\nfloat16 (10000, 10000)  Not tested\n```\n\nI have also tried using the SciPy BLAS wrappers, which enables multi-threaded computation, but it converts everything to `float32` and outputs floats, which is not what I want. \n\nCode 3:\n```python\nimport scipy.linalg.blas as blas\na = np.random.randint(2, size=(n, n)).astype(np.int8)\nb = blas.sgemm(alpha=1.0, a=a, b=a)\n```\n\nGiven these four options, I am looking for a way to do integer matrix multiplication:\n\n1. Use NumPy's slow `np.dot` and keep the 8-bit integers.\n2. Use SciPy's `sgemm` and use up 4x memory.\n3. Use Numpy's `np.float16` and only use up 2x memory, with the caveat that `np.dot` is much slower on float16 arrays than on float32 arrays, more so than int8.\n4. Find an optimized library for multi-threaded integer matrix multiplication, ideally supporting 1-bit arrays, although 8-bit arrays is also fine.\n\nCan I go with option 4? Is there a library that supports multi-threaded integer matrix multiplication?"
    },
    {
        "role": "assistant",
        "content": "Yes, exploring option 4 is a good approach for achieving efficient multi-threaded integer matrix multiplication. There are several libraries that support optimized integer matrix operations, especially for 8-bit integers. Here are some options you might consider:\n\n1. **Intel Math Kernel Library (MKL)**: Intel MKL provides optimized routines for various data types, including integers. It is highly optimized for Intel architectures and can utilize multiple cores effectively. It supports `gemm` operations for integer types.\n\n2. **OpenBLAS**: OpenBLAS is an open-source implementation of the Basic Linear Algebra Subprograms (BLAS) and is optimized for various architectures. It may not natively support 8-bit integer matrix multiplication, but you can convert your matrices to 32-bit integers for computation if necessary.\n\n3. **cuBLAS (NVIDIA CUDA)**: If you have access to NVIDIA GPUs, cuBLAS provides highly optimized routines for matrix multiplications, including support for integer types. This can significantly speed up your computations if you have a compatible GPU.\n\n4. **Torch or TensorFlow**: Both PyTorch and TensorFlow offer tensor operations that can be executed on multiple threads or GPUs. They provide support for integer types and can leverage hardware accelerations.\n\n5. **TBB (Threading Building Blocks)**: While this is more of a threading library, you can integrate it with custom C++ routines that are optimized for integer matrix multiplication.\n\n6. **Eigen**: Eigen is a popular C++ template library for linear algebra. It supports multi-threaded operations and can be a good choice if you don't mind writing a bit of C++ code and wrapping it for Python.\n\nTo integrate one of these libraries into your Python workflow, you may need to write some C/C++ code or use a library like Cython or Pybind11 to create Python bindings. This can be a bit more involved but would give you the performance you need without converting to higher precision and using more memory.\n\nIf you are looking for a Python-only solution, you might need to balance between memory overhead and performance until a more straightforward Python library with support for your specific requirements becomes available."
    }
]