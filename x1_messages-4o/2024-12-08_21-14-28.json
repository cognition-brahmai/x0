[
    {
        "role": "user",
        "content": "What traditional statistical methods can be used to develop a model that predicts which customers are most likely to leave a telecom company? The given dataset contains features like age, gender, monthly charges, and tenure. The final model should achieve an accuracy of at least 80% on a test set of customer data without using any neural network-based approach. Can you provide a Python code that uses regression or decision trees to create the model and outputs a probability of churn for new customers? Also, can you provide a code to test the model using a set of 100 new customer records?"
    },
    {
        "role": "assistant",
        "content": "To develop a model that predicts customer churn in a telecom company using traditional statistical methods, you can use logistic regression or decision trees. Both methods are well-suited for binary classification problems like predicting customer churn. Below, I will provide a Python code example that uses logistic regression with the `scikit-learn` library to create the model and output a probability of churn for new customers. Additionally, I'll include a code snippet for testing the model with new customer data.\n\n### Step 1: Model Development with Logistic Regression\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your dataset\n# Replace 'your_dataset.csv' with the path to your dataset\ndata = pd.read_csv('your_dataset.csv')\n\n# Assuming the dataset has columns: 'age', 'gender', 'monthly_charges', 'tenure', and 'churn'\n# Convert categorical data to numeric (e.g., 'gender' to 0 and 1)\ndata['gender'] = data['gender'].map({'Male': 0, 'Female': 1})\n\n# Define features and target variable\nX = data[['age', 'gender', 'monthly_charges', 'tenure']]\ny = data['churn']  # Assuming 'churn' is the target column with 0 for staying and 1 for leaving\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize and train the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# Make predictions and evaluate the model\ny_pred = model.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Model Accuracy: {accuracy:.2f}')\n\n# Output classification report\nprint(classification_report(y_test, y_pred))\n\n```\n\n### Step 2: Predict Churn Probability for New Customers\n\n```python\n# New customer data (example)\nnew_customers = pd.DataFrame({\n    'age': [30, 45, 22, 35, 29],\n    'gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n    'monthly_charges': [70.0, 89.0, 56.0, 99.0, 75.0],\n    'tenure': [12, 24, 6, 36, 18]\n})\n\n# Convert 'gender' to numeric\nnew_customers['gender'] = new_customers['gender'].map({'Male': 0, 'Female': 1})\n\n# Scale the features using the same scaler as the training data\nnew_customers_scaled = scaler.transform(new_customers)\n\n# Predict churn probabilities\nchurn_probabilities = model.predict_proba(new_customers_scaled)[:, 1]\nnew_customers['churn_probability'] = churn_probabilities\n\n# Display the churn probabilities for the new customers\nprint(new_customers)\n```\n\n### Step 3: Testing the Model with a Set of 100 New Customer Records\n\nYou can replace the `new_customers` DataFrame with your own dataset containing 100 new customer records. Make sure to preprocess the data similarly to how the training data was prepared (e.g., converting categorical variables and scaling features).\n\nThis approach uses logistic regression for simplicity, but you can also explore decision trees or ensemble methods like Random Forests for potentially better performance. Always ensure to validate your model using cross-validation and tune hyperparameters as needed to achieve the desired accuracy."
    }
]