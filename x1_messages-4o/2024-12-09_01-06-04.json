[
    {
        "role": "user",
        "content": "Why does PyTorch Dataloader hang when num_workers is greater than 0? The code hangs with only about 500 M GPU memory usage. I am running this on system with NVIDIA-SMI 418.56 Driver Version, CUDA Version: 10.1. This issue appears when I run my script in terminal as python main.py, but it runs fine when I debug the same code on Pycharm or VScode, or when I run the same code (in terminal) on other machines. Why is this happening?\n\nHere is the trace when I ctrl c the code in terminal:\n```\nFile \"train.py\", line 226, in main\ntrain_domain_adaptation(model, source_loader, target_loader, val_loader,\nFile \"/home/zhangyu/codes/person_seg/IntraDA/ADVENT/advent/domain_adaptation/train_UDA.py\", line 326, in train_domain_adaptation\ntrain_advent(model, trainloader, targetloader, val_loader, cfg, group=group, fk_loader=fk_loader)\nFile \"/home/zhangyu/codes/person_seg/IntraDA/ADVENT/advent/domain_adaptation/train_UDA.py\", line 114, in train_advent\n_, (images_source, labels, src_names, voc_ids, _) = trainloader_iter.__next__()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\ndata = self._next_data()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 974, in _next_data\nidx, data = self._get_data()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 931, in _get_data\nsuccess, data = self._try_get_data()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 779, in _try_get_data\ndata = self._data_queue.get(timeout=timeout)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/queue.py\", line 179, in get\nself.not_empty.wait(remaining)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py\", line 306, in wait\ngotit = waiter.acquire(True, timeout)\nKeyboardInterrupt\nException in thread Thread-2:\nTraceback (most recent call last):\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\nself.run()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py\", line 870, in run\nself._target(*self._args, **self._kwargs)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\nr = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/queues.py\", line 116, in get\nreturn _ForkingPickler.loads(res)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\nfd = df.detach()\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\nwith _resource_sharer.get_connection(self._id) as conn:\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\nc = Client(address, authkey=process.current_process().authkey)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py\", line 508, in Client\nanswer_challenge(c, authkey)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py\", line 752, in answer_challenge\nmessage = connection.recv_bytes(256)         # reject large message\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\nbuf = self._recv_bytes(maxlength)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\nbuf = self._recv(4)\nFile \"/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\nchunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\n```"
    },
    {
        "role": "assistant",
        "content": "The issue you are encountering with the PyTorch DataLoader hanging when `num_workers` is greater than 0 can be due to several reasons related to multiprocessing. Here are some common causes and potential solutions:\n\n1. **Python Multiprocessing and Forking**:\n   - Some systems, particularly those with older Linux distributions or specific configurations, may have issues with the default 'fork' start method used by Python's multiprocessing library. \n   - To address this, you can try setting the start method to 'spawn', which is more robust across different platforms. This can be done at the beginning of your script:\n     ```python\n     import multiprocessing\n     multiprocessing.set_start_method('spawn', force=True)\n     ```\n\n2. **Shared Memory Issues**:\n   - With multiprocessing, data is transferred between processes using shared memory, and if there isn't enough shared memory available, it can cause hangs or crashes.\n   - Check your system's shared memory limits with `df -h /dev/shm` and increase it if necessary.\n\n3. **Driver and CUDA Version Compatibility**:\n   - Ensure that your NVIDIA driver and CUDA versions are compatible with the PyTorch version you are using. For CUDA 10.1, make sure your PyTorch version supports this CUDA version.\n\n4. **Environment and Package Conflicts**:\n   - Sometimes, package conflicts or issues in the Python environment can cause unexpected behavior. Consider creating a fresh virtual environment and reinstalling PyTorch and other dependencies.\n\n5. **Debugging DataLoader**:\n   - Try running your DataLoader with `num_workers=0` to rule out issues with the data processing code itself. If it works fine, the issue is likely related to multiprocessing.\n   - You can also try using a smaller dataset or batch size to see if the problem persists.\n\n6. **Resource Limits**:\n   - Check your system's resource usage. High CPU or memory usage can cause DataLoader processes to hang.\n   - Limit the number of workers based on your CPU cores and available memory.\n\nSince the issue doesn't occur when running the script in PyCharm or VSCode, it could also be related to the terminal environment. Ensure that the terminal environment matches the IDE's environment, particularly in terms of environment variables and Python paths.\n\nBy investigating these areas, you should be able to pinpoint the cause of the hang and address it accordingly."
    }
]