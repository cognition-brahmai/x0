[
    {
        "role": "user",
        "content": "I'm looking for a serverless key:value store in Python that can handle possibly 100 GB of data. The solutions I've tried like json.loads/json.dumps, pickle, shelve, ujson, and sqlite all rewrite the entire data when closing or serializing which is not feasible for this amount of data. I'm not interested in redis because it uses a client/server scheme. What key:value stores are frequently used in Python for handling large data? Ideally, I would like it to have a standard Pythonic syntax like d[key] = value. Here is an example of the code I'm looking to implement:\n\n```python\nimport mydb\nd = mydb.mydb('myfile.db')\nd['hello'] = 17          # able to use string or int or float as key\nd[183] = [12, 14, 24]    # able to store lists as values (will probably internally jsonify it?)\nd.flush()                # easy to flush on disk \n```\nI've seen BsdDB (BerkeleyDB) and LevelDB for Python but they seem to be deprecated or not well-known, and I haven't found a version which is ready to use on Windows. What are the most common ones used for this purpose?"
    },
    {
        "role": "assistant",
        "content": "For handling large key-value stores in Python without rewriting the entire dataset, you have a few options that are more efficient for large-scale data management:\n\n1. **LMDB (Lightning Memory-Mapped Database):**\n   - LMDB is a fast memory-mapped key-value database that is efficient and supports ACID transactions.\n   - It handles large datasets well and provides efficient random access.\n   - You can use the `lmdb` Python package to interact with LMDB databases.\n\n   Example usage:\n   ```python\n   import lmdb\n\n   env = lmdb.open('myfile.db', map_size=1099511627776)  # 1 TB map size for large datasets\n   with env.begin(write=True) as txn:\n       txn.put(b'hello', b'17')\n       txn.put(b'183', b'[12, 14, 24]')\n\n   with env.begin() as txn:\n       value = txn.get(b'hello')\n       print(value)\n   ```\n\n2. **ZODB (Zope Object Database):**\n   - ZODB is an object-oriented database for Python that allows you to persist native Python objects transparently.\n   - It is a robust solution for storing complex data structures efficiently.\n\n   Example usage:\n   ```python\n   import ZODB, ZODB.FileStorage\n\n   storage = ZODB.FileStorage.FileStorage('myfile.fs')\n   db = ZODB.DB(storage)\n   connection = db.open()\n   root = connection.root\n\n   root['hello'] = 17\n   root['183'] = [12, 14, 24]\n\n   transaction.commit()  # Commit changes to disk\n\n   print(root['hello'])\n   connection.close()\n   ```\n\n3. **HDF5 with h5py:**\n   - HDF5 is a file format and set of tools for managing complex data.\n   - The `h5py` library allows you to store large datasets in a hierarchical structure, similar to a filesystem.\n\n   Example usage:\n   ```python\n   import h5py\n\n   with h5py.File('myfile.h5', 'w') as f:\n       f.create_dataset('hello', data=17)\n       f.create_dataset('183', data=[12, 14, 24])\n\n   with h5py.File('myfile.h5', 'r') as f:\n       print(f['hello'][()])\n   ```\n\n4. **UnQLite:**\n   - UnQLite is a document store and key/value database engine that can be embedded into applications.\n   - It is lightweight and does not require a separate server process.\n\n   You can use the `unqlite` Python package to interact with UnQLite databases.\n\n   Example usage:\n   ```python\n   import unqlite\n\n   db = unqlite.UnQLite('myfile.db')\n   db['hello'] = 17\n   db['183'] = [12, 14, 24]\n\n   print(db['hello'])\n   db.commit()  # Persist changes to disk\n   db.close()\n   ```\n\nThese options give you different trade-offs in terms of performance, complexity, and feature set, so you can choose the one that best fits your needs."
    }
]