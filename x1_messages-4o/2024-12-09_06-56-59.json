[
    {
        "role": "user",
        "content": "I'm trying to compute the cross entropy using PyTorch. I'm aware that the cross entropy formula should yield 0 when `1*log(1)` is computed. However, when I run the following code:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\noutput = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)\ntarget = Variable(torch.LongTensor([3]))\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(output, target)\nprint(loss)\n```\n\nI get a loss of `0.7437` instead of `0`. Why is this the case?"
    },
    {
        "role": "assistant",
        "content": "The issue you're encountering is due to how PyTorch's `nn.CrossEntropyLoss` function interprets the inputs. The `nn.CrossEntropyLoss` expects the `output` (also known as the logits) to be raw, unnormalized scores for each class, not probabilities. In other words, the values in `output` should not be passed through a softmax before being passed to the criterion.\n\nHere's a breakdown of why you see a non-zero result:\n\n1. **Logits vs Probabilities**: The `output` should be raw scores (logits), not probabilities. The `CrossEntropyLoss` combines `log_softmax` and `nll_loss` in a single function, which means it expects logits and not probabilities.\n\n2. **Softmax and Log Transformation**: When you provide `[0, 0, 0, 1]` as the logits, it does an internal softmax to convert these to probabilities, and then computes the negative log-likelihood. The softmax of `[0, 0, 0, 1]` results in a non-zero probability distribution that does not simply map the 1 to 1 and the others to 0.\n\n3. **Correct Usage**: If you want to compute the cross-entropy loss using probabilities, you should directly use `nn.NLLLoss` with the log of the probabilities.\n\nTo get the intended behavior, provide the logits instead of probabilities. Here's an example of how you might set this up correctly:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# Correct logits that will become [0, 0, 0, 1] after softmax is applied internally\noutput = Variable(torch.FloatTensor([1, 0, 0, 0])).view(1, -1)  # Changing to logits\ntarget = Variable(torch.LongTensor([0]))  # Index should be 0 for the first element\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(output, target)\nprint(loss)\n```\n\nThis should give you a loss close to zero, as the model correctly predicts the target class with high confidence. Adjust the logits accordingly based on the index of the target class to get the desired results."
    }
]