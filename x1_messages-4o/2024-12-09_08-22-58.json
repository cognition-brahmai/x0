[
    {
        "role": "user",
        "content": "How can I utilize Nvidia Multi-process Service (MPS) to concurrently run multiple non-MPI CUDA applications on NVIDIA Kepler GPUs? I am asking this because my applications are not fully utilizing the GPU and I want them to operate together. Is there an example code to achieve this?"
    },
    {
        "role": "assistant",
        "content": "To utilize Nvidia Multi-process Service (MPS) for running multiple non-MPI CUDA applications concurrently on NVIDIA Kepler GPUs, follow these steps:\n\n### Overview of MPS\nNvidia MPS allows multiple CUDA applications to run simultaneously on a single GPU by sharing the GPU's resources. This is particularly useful when individual applications do not fully utilize the GPU, allowing you to maximize resource usage.\n\n### Steps to Use MPS\n\n1. **Enable MPS**: First, ensure that MPS is enabled on your system. You can check this by executing:\n   ```bash\n   nvidia-smi -q | grep \"MPS\"\n   ```\n   If MPS is not enabled, you'll need to start the MPS control daemon.\n\n2. **Start MPS Control Daemon**: Run the following command to start the MPS control daemon:\n   ```bash\n   nvidia-cuda-mps-control -d\n   ```\n\n3. **Configure MPS Environment**: Set the CUDA_MPS_PIPE_DIRECTORY and CUDA_MPS_LOG_DIRECTORY environment variables. These directories are used by MPS to exchange messages and store logs, respectively.\n   ```bash\n   export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\n   export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n   ```\n\n4. **Launch Applications**: Once MPS is running, you can launch your CUDA applications as usual. The MPS server will automatically manage the GPU resources among the applications.\n\n5. **Monitor GPU Usage**: Use `nvidia-smi` to monitor the GPU usage and ensure that resources are being utilized efficiently.\n\n### Example Code\n\nHere's a simple example of launching multiple CUDA applications under MPS control:\n\n- **Application 1 (app1.cu):**\n  ```cpp\n  #include <cuda_runtime.h>\n  #include <iostream>\n\n  __global__ void kernel1() {\n      // Simple kernel for demonstration\n      printf(\"Kernel 1 running\\n\");\n  }\n\n  int main() {\n      kernel1<<<1, 1>>>();\n      cudaDeviceSynchronize();\n      std::cout << \"Application 1 completed.\" << std::endl;\n      return 0;\n  }\n  ```\n\n- **Application 2 (app2.cu):**\n  ```cpp\n  #include <cuda_runtime.h>\n  #include <iostream>\n\n  __global__ void kernel2() {\n      // Simple kernel for demonstration\n      printf(\"Kernel 2 running\\n\");\n  }\n\n  int main() {\n      kernel2<<<1, 1>>>();\n      cudaDeviceSynchronize();\n      std::cout << \"Application 2 completed.\" << std::endl;\n      return 0;\n  }\n  ```\n\nCompile the applications:\n```bash\nnvcc -o app1 app1.cu\nnvcc -o app2 app2.cu\n```\n\nRun the applications concurrently:\n```bash\n./app1 &\n./app2 &\n```\n\n### Stopping MPS\n\nWhen you want to stop the MPS service, you need to terminate the MPS control daemon:\n```bash\necho quit | nvidia-cuda-mps-control\n```\n\nBy using MPS, you can efficiently share GPU resources among multiple CUDA applications, improving overall utilization. Remember that MPS is most beneficial when individual applications do not fully utilize the GPU, as it allows them to share resources."
    }
]