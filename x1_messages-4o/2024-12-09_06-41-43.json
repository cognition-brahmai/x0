[
    {
        "role": "user",
        "content": "When using Pytorch Lightning's Distributed Data Parallel (DDP), what is the proper way to log metrics? I'm particularly interested in understanding the behavior of the `validation_epoch_end(self, outputs)` function. Does every subprocess receive the data processed from the current GPU or data processed from all GPUs? If the `outputs` is GPU/process specific, what is the proper way to calculate any metric on the entire validation set in `validation_epoch_end` when using DDP? Below is a code snippet of my use case where I would like to report f1, precision and recall on the entire validation dataset using DDP.\n\n```python\n    def _process_epoch_outputs(self,\n                               outputs: List[Dict[str, Any]]\n                               ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Creates and returns tensors containing all labels and predictions\n\n        Goes over the outputs accumulated from every batch, detaches the\n        necessary tensors and stacks them together.\n\n        Args:\n            outputs (List[Dict])\n        \"\"\"\n        all_labels = []\n        all_predictions = []\n\n        for output in outputs:\n            for labels in output['labels'].detach():\n                all_labels.append(labels)\n\n            for predictions in output['predictions'].detach():\n                all_predictions.append(predictions)\n\n        all_labels = torch.stack(all_labels).long().cpu()\n        all_predictions = torch.stack(all_predictions).cpu()\n\n        return all_predictions, all_labels\n\n    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> None:\n        \"\"\"Logs f1, precision and recall on the validation set.\"\"\"\n\n        if self.global_rank == 0:\n            print(f'Validation Epoch: {self.current_epoch}')\n\n        predictions, labels = self._process_epoch_outputs(outputs)\n        for i, name in enumerate(self.label_columns):\n\n            f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],\n                                                             labels[:, i],\n                                                             threshold=None)\n            self.logger.experiment.add_scalar(f'{name}_f1/Val',\n                                              f1,\n                                              self.current_epoch)\n            self.logger.experiment.add_scalar(f'{name}_Precision/Val',\n                                              prec,\n                                              self.current_epoch)\n            self.logger.experiment.add_scalar(f'{name}_Recall/Val',\n                                              recall,\n                                              self.current_epoch)\n\n            if self.global_rank == 0:\n                print((f'F1: {f1}, Precision: {prec}, '\n                       f'Recall: {recall}, Threshold {t}'))\n```"
    },
    {
        "role": "assistant",
        "content": "When using PyTorch Lightning's Distributed Data Parallel (DDP) mode, each subprocess (or GPU) handles a portion of the data. Therefore, the `outputs` list in `validation_epoch_end(self, outputs)` contains data specific to the current GPU or subprocess. This means that each GPU will only have access to the outputs from the batches it has processed.\n\nTo calculate metrics such as F1, precision, and recall on the entire validation set, you'll need to gather the outputs from all GPUs. PyTorch Lightning provides utilities to help with this. One common approach is to use `torch.distributed.all_gather` to collect outputs from all processes.\n\nHere's how you can modify your code to gather outputs across all GPUs before computing metrics:\n\n```python\nimport torch\nfrom torch.distributed import all_gather\nfrom typing import List, Dict, Any, Tuple\n\ndef _process_epoch_outputs(self,\n                           outputs: List[Dict[str, Any]]\n                           ) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Creates and returns tensors containing all labels and predictions.\n\n    Goes over the outputs accumulated from every batch, detaches the\n    necessary tensors and stacks them together.\n\n    Args:\n        outputs (List[Dict])\n    \"\"\"\n    all_labels = []\n    all_predictions = []\n\n    for output in outputs:\n        all_labels.append(output['labels'].detach().cpu())\n        all_predictions.append(output['predictions'].detach().cpu())\n\n    all_labels = torch.cat(all_labels, dim=0)\n    all_predictions = torch.cat(all_predictions, dim=0)\n\n    return all_predictions, all_labels\n\ndef validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> None:\n    \"\"\"Logs F1, precision, and recall on the validation set.\"\"\"\n\n    # Gather outputs from all processes\n    gathered_outputs = [None] * torch.distributed.get_world_size()\n    torch.distributed.all_gather_object(gathered_outputs, outputs)\n\n    # Flatten the gathered outputs\n    all_outputs = []\n    for gathered_output in gathered_outputs:\n        all_outputs.extend(gathered_output)\n\n    # Process the gathered outputs\n    predictions, labels = self._process_epoch_outputs(all_outputs)\n\n    for i, name in enumerate(self.label_columns):\n        f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],\n                                                         labels[:, i],\n                                                         threshold=None)\n        if self.global_rank == 0:\n            self.logger.experiment.add_scalar(f'{name}_f1/Val',\n                                              f1,\n                                              self.current_epoch)\n            self.logger.experiment.add_scalar(f'{name}_Precision/Val',\n                                              prec,\n                                              self.current_epoch)\n            self.logger.experiment.add_scalar(f'{name}_Recall/Val',\n                                              recall,\n                                              self.current_epoch)\n\n            print((f'F1: {f1}, Precision: {prec}, '\n                   f'Recall: {recall}, Threshold {t}'))\n\n```\n\n### Key Points:\n1. **Gathering Outputs:**\n   - Use `torch.distributed.all_gather_object` to gather outputs from all subprocesses.\n   - Note that `all_gather_object` is a utility that allows gathering Python objects, which is suitable here since `outputs` is a list of dictionaries.\n\n2. **Processing Gathered Data:**\n   - After gathering, you need to process the combined outputs to compute your desired metrics.\n\n3. **Logging Metrics:**\n   - Ensure that logging of metrics is done only on the main process (e.g., `global_rank == 0`) to avoid duplicate logs.\n\nBy gathering the outputs from all distributed processes, you can accurately compute metrics across the entire dataset in a DDP setup."
    }
]