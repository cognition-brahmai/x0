[
    {
        "role": "user",
        "content": "How can I output the per-class accuracy in Keras? I'm currently getting the overall accuracy in Keras log as shown in the code below. Caffe has the ability to print not only the overall accuracy, but also per-class accuracy. It's challenging for me to calculate the separate class accuracy. Is there a way to achieve this in Keras?\n\n```python\nEpoch 168/200\n0s - loss: 0.0495 - acc: 0.9818 - val_loss: 0.0519 - val_acc: 0.9796\nEpoch 169/200\n0s - loss: 0.0519 - acc: 0.9796 - val_loss: 0.0496 - val_acc: 0.9815\nEpoch 170/200\n0s - loss: 0.0496 - acc: 0.9815 - val_loss: 0.0514 - val_acc: 0.9801\n```"
    },
    {
        "role": "assistant",
        "content": "To calculate per-class accuracy in Keras, you can use a custom callback to evaluate the model on a validation set at the end of each epoch. Here's a step-by-step approach to achieve this:\n\n1. **Create a Custom Callback**: Define a custom callback that computes the per-class accuracy after each epoch.\n\n2. **Update the Model Training Code**: Add this callback to the list of callbacks when fitting the model.\n\nHere's how you can implement it:\n\n```python\nimport numpy as np\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import confusion_matrix\n\nclass PerClassAccuracy(Callback):\n    def __init__(self, validation_data):\n        super(PerClassAccuracy, self).__init__()\n        self.validation_data = validation_data\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_data, val_labels = self.validation_data\n        val_predictions = np.argmax(self.model.predict(val_data), axis=1)\n        val_true_labels = np.argmax(val_labels, axis=1)\n\n        cm = confusion_matrix(val_true_labels, val_predictions)\n        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n        \n        for i, acc in enumerate(per_class_acc):\n            print(f'Class {i} accuracy: {acc:.4f}')\n\n# Assuming you have your model, train_data, train_labels, val_data, val_labels defined\n# Convert your labels to categorical if they are not\n# from keras.utils import to_categorical\n# train_labels = to_categorical(train_labels, num_classes=number_of_classes)\n# val_labels = to_categorical(val_labels, num_classes=number_of_classes)\n\n# Create the callback instance\nper_class_acc_callback = PerClassAccuracy(validation_data=(val_data, val_labels))\n\n# Fit the model\nmodel.fit(train_data, train_labels,\n          epochs=200,\n          validation_data=(val_data, val_labels),\n          callbacks=[per_class_acc_callback])\n```\n\n### Explanation:\n\n- **Custom Callback**: The `PerClassAccuracy` class inherits from `Callback`. During each epoch end, it predicts the classes for the validation set and calculates the confusion matrix, from which per-class accuracy is derived.\n\n- **Confusion Matrix**: The confusion matrix is used to calculate the per-class accuracy. The diagonal elements of this matrix represent the correctly predicted instances of each class, and the sum along each row gives the total instances for that class.\n\n- **Argmax**: `np.argmax()` is used to get the predicted class indices and true class indices from the one-hot encoded predictions and labels.\n\nMake sure that your validation data and labels are preprocessed correctly (e.g., one-hot encoded if needed) and that the number of classes is consistent across the calculations. This setup will print the per-class accuracies at the end of each epoch during training."
    }
]