[
    {
        "role": "user",
        "content": "I have a complex dataset of approximately 2 million observations that I need to split into training, validation, and test sets in the ratio of 60:20:20. My dataset contains multiple categories which are not evenly balanced and I need to ensure that the training, validation, and test sets all have the same proportions of categories as in the original dataset. For this, I plan on using `StratifiedShuffleSplit` from the `sklearn` library. However, I also need to ensure that the observations from each subject are not split across the training, validation, and test datasets. In other words, all the observations from a given subject need to be in the same bucket. I am unsure about how to achieve a stratified split by category, prevent contamination of `subject_id` across datasets, ensure a 60:20:20 split and ensure that the dataset is shuffled. Additionally, I have learned that `GroupShuffleSplit` function in `sklearn` can achieve grouping by a category and keeping groups together across dataset splits. But I need a combined stratified and grouped shuffle split i.e. `StratifiedGroupShuffleSplit`, which doesn't exist. How can I achieve this?"
    },
    {
        "role": "assistant",
        "content": "To achieve a combined stratified and grouped shuffle split for your dataset, you'll need to perform a custom procedure because `StratifiedGroupShuffleSplit` doesn't exist in `sklearn`. Here's a step-by-step approach to accomplish your goal:\n\n1. **Ensure Data Integrity**: First, make sure your dataset is organized in a way where you have a `subject_id` column and a `category` column (or whatever you are using to stratify).\n\n2. **Group by Subject**: Aggregate your data by `subject_id` to ensure that all observations for a subject are grouped together. This will allow you to handle each subject as a single unit during the split.\n\n3. **Aggregate Category Proportions**: For stratification, calculate the proportion of each category within each subject. This means each subject will have a vector of category proportions.\n\n4. **Split Groups with Stratification**: Perform the actual split in two steps:\n   - Use the `GroupShuffleSplit` from `sklearn.model_selection` to split subjects into training, validation, and test sets, while ensuring all observations for a subject remain intact.\n   - Within each split, verify that the category distribution matches the overall dataset as closely as possible.\n\n5. **Custom Stratified Splitting**:\n   - Use a loop or an iterative process to adjust the splits. If the category proportions in the validation and test sets are not satisfactory, you may need to manually adjust the subjects in these sets until the proportions closely resemble the original data.\n   - This can be achieved by swapping subjects between sets to better match the desired distribution. This step might involve some trial and error or optimization logic.\n\nHere's a rough implementation outline in Python:\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom collections import Counter\nimport numpy as np\n\ndef stratified_group_split(df, stratify_col, group_col, train_size=0.6, valid_size=0.2, test_size=0.2, random_state=None):\n    # Ensure the sizes sum to 1\n    assert train_size + valid_size + test_size == 1\n\n    # Calculate group proportions\n    group_counts = df.groupby(group_col)[stratify_col].apply(lambda x: Counter(x))\n    group_counts_df = pd.DataFrame(list(group_counts), index=group_counts.index).fillna(0)\n\n    # Normalize category counts to proportions\n    group_proportions = group_counts_df.div(group_counts_df.sum(axis=1), axis=0)\n\n    # Get unique groups\n    groups = group_proportions.index.unique()\n\n    # Initial split using GroupShuffleSplit\n    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=random_state)\n    train_idx, temp_idx = next(gss.split(X=groups, groups=groups))\n    \n    # Further split temp into validation and test\n    gss_temp = GroupShuffleSplit(n_splits=1, train_size=valid_size/(valid_size + test_size), random_state=random_state)\n    valid_idx, test_idx = next(gss_temp.split(X=temp_idx, groups=groups[temp_idx]))\n\n    # Get group IDs for each split\n    train_groups = groups[train_idx]\n    valid_groups = groups[temp_idx[valid_idx]]\n    test_groups = groups[temp_idx[test_idx]]\n\n    # Split the original dataframe\n    train_df = df[df[group_col].isin(train_groups)]\n    valid_df = df[df[group_col].isin(valid_groups)]\n    test_df = df[df[group_col].isin(test_groups)]\n\n    return train_df, valid_df, test_df\n\n# Example usage\n# df is your original DataFrame with 'subject_id' and 'category' columns\ntrain_df, valid_df, test_df = stratified_group_split(df, stratify_col='category', group_col='subject_id', random_state=42)\n```\n\nThis method ensures that subjects are kept intact and attempts to maintain category proportions as close to the original dataset as possible. You might need to iterate and adjust if the initial splits don't meet your requirements for category balance."
    }
]