[
    {
        "role": "user",
        "content": "I have recently discovered the doc2vec addition to Gensim and would like to understand how to use pre-trained word vectors, such as those found on the word2vec original website, with doc2vec. Also, I am curious if doc2vec is sourcing the word vectors from the same sentences used for paragraph-vector training?"
    },
    {
        "role": "assistant",
        "content": "Using pre-trained word vectors with doc2vec in Gensim can be a powerful way to enhance the quality of your document embeddings. Here\u2019s a step-by-step guide on how to do this:\n\n### Using Pre-trained Word Vectors with Doc2Vec\n\n1. **Load Pre-trained Word Vectors:**\n   First, you need to load the pre-trained word vectors. Gensim provides convenient functions to load models in Word2Vec format. Here\u2019s how you can load them:\n\n   ```python\n   from gensim.models import KeyedVectors\n\n   # Replace 'path/to/word2vec.bin' with the actual path to the pre-trained model\n   word_vectors = KeyedVectors.load_word2vec_format('path/to/word2vec.bin', binary=True)\n   ```\n\n2. **Initialize Doc2Vec Model:**\n   Once you have the pre-trained word vectors, you can initialize a Doc2Vec model. You can specify the `vector_size` parameter based on the pre-trained model\u2019s vector size:\n\n   ```python\n   from gensim.models import Doc2Vec\n   from gensim.models.doc2vec import TaggedDocument\n\n   # Example documents\n   documents = [TaggedDocument(words=['word1', 'word2', 'word3'], tags=[0]),\n                TaggedDocument(words=['word4', 'word5', 'word6'], tags=[1])]\n\n   # Initialize Doc2Vec model\n   model = Doc2Vec(vector_size=word_vectors.vector_size, min_count=1, epochs=10)\n   ```\n\n3. **Build Vocabulary:**\n   You need to build the vocabulary for the Doc2Vec model. This is where you can integrate the pre-trained word vectors:\n\n   ```python\n   model.build_vocab(documents)\n\n   # Copy the pre-trained word vectors into the Doc2Vec model\n   model.wv.vectors_lockf = np.ones(len(model.wv), dtype=np.float32)\n   for word in word_vectors.key_to_index:\n       if word in model.wv:\n           model.wv[word] = word_vectors[word]\n           model.wv.vectors_lockf[model.wv.key_to_index[word]] = 0.0\n   ```\n\n4. **Train the Doc2Vec Model:**\n   Finally, you can train the Doc2Vec model with your documents. Since the word vectors are already trained, the training will mainly focus on the document vectors:\n\n   ```python\n   model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n   ```\n\n### Doc2Vec Word Vectors Sourcing\n\nRegarding your question about whether Doc2Vec sources word vectors from the same sentences used for paragraph-vector training: Yes, during the training process, Doc2Vec uses the same sentences to update both the word vectors and the paragraph (document) vectors. The model learns to predict words in the context of both the document and its surrounding words, which helps in capturing the semantics of both words and documents.\n\nBy integrating pre-trained word vectors, you can leverage external knowledge to improve the quality of your document embeddings, especially if you have a limited amount of training data."
    }
]