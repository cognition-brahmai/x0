[
    {
        "role": "user",
        "content": "I am currently trying to solve Pendulum-v0 from the openAi gym environment which has a continuous action space. Hence, I am using a Normal Distribution to sample my actions. However, I don't understand the dimension of the log_prob when using it. I was expecting a tensor of size 2 (one log_prob for each action) but it output a tensor of size(2,2). Here is the code I am using:\n\n```python\nimport torch\nfrom torch.distributions import Normal \n\nmeans = torch.tensor([[0.0538],\n        [0.0651]])\nstds = torch.tensor([[0.7865],\n        [0.7792]])\n\ndist = Normal(means, stds)\na = torch.tensor([1.2,3.4])\nd = dist.log_prob(a)\nprint(d.size())\n```\n\nOn the other hand, when using a Categorical distribution for a discrete environment the log_prob has the expected size:\n\n```python\nlogits = torch.tensor([[-0.0657, -0.0949],\n        [-0.0586, -0.1007]])\n\ndist = Categorical(logits = logits)\na = torch.tensor([1, 1])\nprint(dist.log_prob(a).size())\n```\n\nThis gives me a tensor of size(2). Why is the log_prob for Normal distribution of a different size?"
    },
    {
        "role": "assistant",
        "content": "The difference in the size of the `log_prob` results between the `Normal` and `Categorical` distributions is due to how these distributions are structured and how their probabilities are computed.\n\n### Normal Distribution\n\nWhen you use the `Normal` distribution with `means` and `stds` as tensors of shape `[2, 1]`, you're essentially creating two separate normal distributions, each defined by a mean and a standard deviation. These distributions are organized in a batch with the shape `(2, 1)`.\n\nWhen you calculate `log_prob` for a tensor `a` of shape `[2]`, PyTorch implicitly broadcasts it to shape `[2, 1]` to match the distribution's shape, performing the operation element-wise across the batch. This results in a `log_prob` tensor of shape `[2, 2]`, where each entry corresponds to the log probability of the corresponding action under each distribution in the batch.\n\n### Categorical Distribution\n\nFor the `Categorical` distribution, the `logits` tensor of shape `[2, 2]` defines a batch of categorical distributions, each with two possible outcomes. When you calculate `log_prob` for a tensor `a` of shape `[2]`, it straightforwardly maps each action to the corresponding distribution in the batch, resulting in a tensor of shape `[2]`. Each entry in the result corresponds to the log probability of the chosen category for each distribution.\n\n### Summary\n\n- **Normal Distribution**: You are dealing with a batch of distributions where each entry in `means` and `stds` defines a separate normal distribution. The `log_prob` operation is applied element-wise across the batch, which can lead to broadcasting and thus a larger output shape.\n  \n- **Categorical Distribution**: The `log_prob` directly maps each input action to a distribution in the batch, resulting in a straightforward output shape that matches the input shape.\n\nIf you want the `log_prob` for the `Normal` distribution to have a size of `[2]`, you need to ensure that `a` has compatible dimensions, or that the operation is summed or reduced in some way to collapse the extra dimensions. For example, you can perform a reduction like `d.sum(dim=-1)` to achieve the desired size if appropriate for your use case."
    }
]